<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="GAN,Image Translation,Cross Domain Learning," />










<meta name="description" content="1 GAN1.1 IntroductionTo learn the generator’s distribution $p_g$ over data x, we define a prior on input noise variables $p_z(z)$, then represent a mapping to data space as $G(z; θ_g)$, where $G$ is a">
<meta name="keywords" content="GAN,Image Translation,Cross Domain Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="GAN and it&#39;s applications on image translation">
<meta property="og:url" content="http://yoursite.com/2018/03/30/GAN-and-it-s-applications/index.html">
<meta property="og:site_name" content="N4A Space">
<meta property="og:description" content="1 GAN1.1 IntroductionTo learn the generator’s distribution $p_g$ over data x, we define a prior on input noise variables $p_z(z)$, then represent a mapping to data space as $G(z; θ_g)$, where $G$ is a">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/gan.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/cycleGAN.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/cycleGAN-loss.png">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/cycleGAN-experiments.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/cycleGAN-experiments2.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/cycleGAN-experiments3.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/cycleGAN-experiments4.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/cycleGAN-limit.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/diat-transform.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/diat-transform-loss.png">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/diat-enhance.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/diat-enhance-loss.png">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/diat-a-transform-loss.png">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/diat-experiment1.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/diat-experiment2.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/diat-experiment3.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/dtn-model.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/dtn-loss.png">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/dtn-e1.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/dtn-e2.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/dtn-e3.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/stargan-model.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/stargan-loss.png">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/stargan-maskv.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/stargan-model2.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/stargan-model3.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/stargan-e1.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/stargan-e3.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/stargan-e4.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/stargan-e2.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/stargan-e5.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/pix2pix-model1.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/pix2pix-model2.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/pix2pix-loss.png">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/pix2pix-e1.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/pix2pix-e2.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/pix2pix-e3.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/sr-model.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/sr-loss.png">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/sr-e1.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/sr-e2.PNG">
<meta property="og:image" content="http://yoursite.com/img/gan_and_applications/mse-problem.PNG">
<meta property="og:updated_time" content="2018-07-17T08:31:05.797Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GAN and it&#39;s applications on image translation">
<meta name="twitter:description" content="1 GAN1.1 IntroductionTo learn the generator’s distribution $p_g$ over data x, we define a prior on input noise variables $p_z(z)$, then represent a mapping to data space as $G(z; θ_g)$, where $G$ is a">
<meta name="twitter:image" content="http://yoursite.com/img/gan_and_applications/gan.PNG">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/03/30/GAN-and-it-s-applications/"/>





  <title>GAN and it's applications on image translation | N4A Space</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">N4A Space</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">To explore and understand</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/30/GAN-and-it-s-applications/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="N4A">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="N4A Space">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">GAN and it's applications on image translation</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-30T16:12:14+08:00">
                2018-03-30
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/03/30/GAN-and-it-s-applications/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/03/30/GAN-and-it-s-applications/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-GAN"><a href="#1-GAN" class="headerlink" title="1 GAN"></a>1 GAN</h2><h3 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h3><p>To learn the generator’s distribution $p_g$ over data x, we define a prior on input noise variables $p_z(z)$, then represent a mapping to data space as $G(z; θ_g)$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $θ_g$. We also define a second multilayer perceptron $D(x; θ_d)$ that outputs a single scalar. $D(x)$ represents the probability that $x$ came from the data rather than $p_g$. We train $D$ to maximize the probability of assigning the correct label to both training examples and samples from $G$. We simultaneously train $G$ to minimize $log(1 - D(G(z)))$. In other words, $D$ and $G$ play the following two-player mini-max game with value function $V (G; D):$</p>
<p>$$min_G max_DV (D; G) = E_x∼p_{data(x)}[log D(x)] + E_{z∼p_z(z)}[log(1 - D(G(z)))]$$</p>
<a id="more"></a>
<h3 id="1-2-Theory-Analysis"><a href="#1-2-Theory-Analysis" class="headerlink" title="1.2 Theory Analysis"></a>1.2 Theory Analysis</h3><p><img src="/img/gan_and_applications/gan.PNG" alt="gan"></p>
<h2 id="2-Cycle-GAN-ICCV-2017"><a href="#2-Cycle-GAN-ICCV-2017" class="headerlink" title="2 Cycle GAN(ICCV 2017)"></a>2 Cycle GAN(ICCV 2017)</h2><h2 id="2-1-Task-Cross-Domain-Image-Translation"><a href="#2-1-Task-Cross-Domain-Image-Translation" class="headerlink" title="2.1 Task: Cross Domain Image Translation"></a>2.1 Task: Cross Domain Image Translation</h2><p>Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image.</p>
<p>In this paper, we present a method that can learn to do the same: capturing special characteristics of one image collection and figuring out how these characteristics could be translated into the other image collection, all in the absence of any paired training examples. </p>
<h3 id="2-2-Model-Cycle-consistent"><a href="#2-2-Model-Cycle-consistent" class="headerlink" title="2.2 Model: Cycle consistent"></a>2.2 Model: Cycle consistent</h3><p><img src="/img/gan_and_applications/cycleGAN.PNG" alt="Cycle GAN"></p>
<p>Loss:</p>
<p><img src="/img/gan_and_applications/cycleGAN-loss.png" alt="cycle gan loss"></p>
<h3 id="2-3-Experiment"><a href="#2-3-Experiment" class="headerlink" title="2.3 Experiment"></a>2.3 Experiment</h3><p><img src="/img/gan_and_applications/cycleGAN-experiments.PNG" alt="Experiments"></p>
<ol>
<li><p>Dataset: Cityscapes dataset , map and aerial photo on data scraped from Google Maps </p>
</li>
<li><p>Metrics: AMT perceptual studies, FCN score, Semantic segmentation metrics</p>
</li>
<li><p>Result:</p>
<p><img src="/img/gan_and_applications/cycleGAN-experiments2.PNG" alt="Experiments"></p>
<p><img src="/img/gan_and_applications/cycleGAN-experiments3.PNG" alt="Experiments"></p>
<p><img src="/img/gan_and_applications/cycleGAN-experiments4.PNG" alt="Experiments"></p>
</li>
</ol>
<h3 id="2-4-Limitations"><a href="#2-4-Limitations" class="headerlink" title="2.4 Limitations"></a>2.4 Limitations</h3><ol>
<li>On translation tasks that involve color and texture changes, like many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. </li>
<li>Some failure cases are caused by the distribution characteristics of the training datasets.</li>
<li>We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method.  </li>
</ol>
<p><img src="/img/gan_and_applications/cycleGAN-limit.PNG" alt="limitations"></p>
<h2 id="3-DIAT-Deep-Identity-aware-Transfer-of-Facial-Attributes"><a href="#3-DIAT-Deep-Identity-aware-Transfer-of-Facial-Attributes" class="headerlink" title="3 DIAT: Deep Identity-aware Transfer of Facial Attributes"></a>3 DIAT: Deep Identity-aware Transfer of Facial Attributes</h2><h3 id="3-1-Task-Identity-aware-Transfer-of-Facial-Attributes"><a href="#3-1-Task-Identity-aware-Transfer-of-Facial-Attributes" class="headerlink" title="3.1 Task: Identity-aware Transfer of Facial Attributes"></a>3.1 Task: Identity-aware Transfer of Facial Attributes</h3><p>Our DIAT and DIAT-A models can provide a unified solution for several representative facial attribute transfer tasks such as <strong>expression transfer</strong>, <strong>accessory removal</strong>, <strong>age progression</strong>, and <strong>gender</strong> transfer </p>
<h3 id="3-2-Model"><a href="#3-2-Model" class="headerlink" title="3.2 Model"></a>3.2 Model</h3><p>In this section, a two-stage scheme is developed to tackle the identity-aware attribute transfer task. </p>
<ol>
<li><p>Face transform network</p>
<p><img src="/img/gan_and_applications/diat-transform.PNG" alt="diat-transform"></p>
<p>Loss: </p>
<p><img src="/img/gan_and_applications/diat-transform-loss.png" alt="loss"></p>
</li>
<li><p>Face Enhancement Network</p>
<p><img src="/img/gan_and_applications/diat-enhance.PNG" alt="enhance"></p>
<p>Loss:</p>
<p><img src="/img/gan_and_applications/diat-enhance-loss.png" alt="loss"></p>
</li>
</ol>
<h3 id="3-3-DIAT-A"><a href="#3-3-DIAT-A" class="headerlink" title="3.3 DIAT-A"></a>3.3 DIAT-A</h3><p>In DIAT, the perceptual identity loss is defined on the pre-trained VGG-Face. Actually, it may be more effective to define this loss on some CNN trained to attribute transfer. Here we treat identity-preserving and attribute transfer as two related tasks, and define the perceptual identity loss based on the convolutional features of the discriminator. By this way, the network parameters for identity loss will be changed along with the updating of discriminator, and thus we named it as adaptive perceptual identity loss. </p>
<p><img src="/img/gan_and_applications/diat-a-transform-loss.png" alt="loss"></p>
<h3 id="3-4-Experiments"><a href="#3-4-Experiments" class="headerlink" title="3.4 Experiments"></a>3.4 Experiments</h3><p>Dataset: a subset of the aligned CelebA dataset  </p>
<p><img src="/img/gan_and_applications/diat-experiment1.PNG" alt="experiment"></p>
<p><img src="/img/gan_and_applications/diat-experiment2.PNG" alt="experiment"></p>
<p><img src="/img/gan_and_applications/diat-experiment3.PNG" alt="experiment"></p>
<h2 id="4-Unsupervised-Cross-Domain-Image-Generation-ICLR-2017"><a href="#4-Unsupervised-Cross-Domain-Image-Generation-ICLR-2017" class="headerlink" title="4 Unsupervised Cross-Domain Image Generation(ICLR 2017 )"></a>4 Unsupervised Cross-Domain Image Generation(ICLR 2017 )</h2><h3 id="4-1-Task"><a href="#4-1-Task" class="headerlink" title="4.1 Task"></a>4.1 Task</h3><p>Recent achievements replicate some of these capabilities to some degree: Generative Adversarial Networks (GANs) are able to convincingly generate novel samples that match that of a given training set; style transfer methods are able to alter the visual style of images; domain adaptation methods are able to generalize learned functions to new domains even without labeled samples in the target domain and transfer learning is now commonly used to import existing knowledge and to make learning much more efficient.</p>
<p>These capabilities, however, do not address the general analogy synthesis problem that we tackle in this work. Namely, <strong>given separated but otherwise unlabeled samples from domains $S$ and $T$ and a perceptual function $f$, learn a mapping $G : S \to T$ such that $f(x) ∼ f(G(x)$ </strong></p>
<p>As a main application challenge, we tackle the problem of <strong>emoji generation for a given facial image</strong>. Despite a growing interest in emoji and the hurdle of creating such personal emoji manually, no system has been proposed, to our knowledge, that can solve this problem. Our method is able to produce face emoji that are visually appealing and capture much more of the facial characteristics than the emoji created by well-trained human annotators who use the conventional tools.</p>
<h3 id="4-2-Model"><a href="#4-2-Model" class="headerlink" title="4.2 Model"></a>4.2 Model</h3><p><img src="/img/gan_and_applications/dtn-model.PNG" alt="dtn model"></p>
<p>Loss: </p>
<p><img src="/img/gan_and_applications/dtn-loss.png" alt="dtn-loss"></p>
<ol>
<li>$D$ is a ternary classification function from the domain $T$ to 1,2,3, and $D_i(x)$ is the<br>probability it assigns to class $i = 1,2,3$ for an input sample $x$</li>
<li>During optimization, $L_G$ is minimized over $g$ and $L_D$ is minimized over $D$ </li>
<li>$L_{CONST}$ enforces f-constancy for $x \in S$, while $L_{TID}$ enforces that for samples $x \in T$  </li>
<li>$L_{TV}$ is an anisotropic total variation loss, which is added in order to slightly smooth the resulting image</li>
<li>$f$ is trained use other datasets before training this model</li>
</ol>
<h3 id="4-3-Experiments"><a href="#4-3-Experiments" class="headerlink" title="4.3 Experiments"></a>4.3 Experiments</h3><p><img src="/img/gan_and_applications/dtn-e1.PNG" alt="dtn experiment"></p>
<p>Dataset: </p>
<ol>
<li>Street View House Number (SVHN) dataset to the domain of the MNIST dataset</li>
<li>FROM PHOTOS TO EMOJI</li>
</ol>
<p>Metrics: MNIST Accuracy</p>
<p><img src="/img/gan_and_applications/dtn-e2.PNG" alt="e"></p>
<p><img src="/img/gan_and_applications/dtn-e3.PNG" alt="e3"></p>
<h2 id="5-StarGAN-Multi-Domain-Image-to-Image-Translation"><a href="#5-StarGAN-Multi-Domain-Image-to-Image-Translation" class="headerlink" title="5 StarGAN: Multi-Domain Image-to-Image Translation"></a>5 StarGAN: Multi-Domain Image-to-Image Translation</h2><h3 id="5-1-Introduction"><a href="#5-1-Introduction" class="headerlink" title="5.1 Introduction"></a>5.1 Introduction</h3><p>Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. </p>
<p>To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model</p>
<p>We can further extend to training multiple domains from different datasets.</p>
<h3 id="5-2-Model"><a href="#5-2-Model" class="headerlink" title="5.2 Model"></a>5.2 Model</h3><p><img src="/img/gan_and_applications/stargan-model.PNG" alt="model"></p>
<p>Loss:</p>
<p><img src="/img/gan_and_applications/stargan-loss.png" alt="loss"></p>
<ol>
<li>a domain classification loss of real images($L_{cls}^r$) used to optimize D, and a domain classification loss of fake images($L_{cls}^f$) used to optimize G </li>
<li>Use $L_{rec}$ to guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs.</li>
</ol>
<h3 id="5-3-Training-with-Multiple-Datasets"><a href="#5-3-Training-with-Multiple-Datasets" class="headerlink" title="5.3 Training with Multiple Datasets"></a>5.3 Training with Multiple Datasets</h3><h4 id="5-3-1-Mask-Vector"><a href="#5-3-1-Mask-Vector" class="headerlink" title="5.3.1 Mask Vector"></a>5.3.1 Mask Vector</h4><p><img src="/img/gan_and_applications/stargan-maskv.PNG" alt="mask"></p>
<p>In StarGAN, we use an n-dimensional one-hot vector to represent m, with n being the number of datasets.  and $c_i$ represents a vector for the labels of the $i$-th dataset. The vector of the known label $c_i$ can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes</p>
<h4 id="5-3-2-Training-Strategy"><a href="#5-3-2-Training-Strategy" class="headerlink" title="5.3.2 Training Strategy"></a>5.3.2 Training Strategy</h4><p> When training StarGAN with multiple datasets, we use the domain label $\overset{\sim}{c}$ defined at above as input to the generator. By doing so, the generator learns to ignore the unspecified labels, which are zero vectors, and<br>focus on the explicitly given label. The structure of the generator is exactly the same as in training with a single dataset, except for the dimension of the input label $\overset{\sim}{c}$. </p>
<h3 id="5-3-3-CelebA-and-RaFD-dataset-demo"><a href="#5-3-3-CelebA-and-RaFD-dataset-demo" class="headerlink" title="5.3.3 CelebA and RaFD dataset demo"></a>5.3.3 CelebA and RaFD dataset demo</h3><p><img src="/img/gan_and_applications/stargan-model2.PNG" alt="model"></p>
<p><img src="/img/gan_and_applications/stargan-model3.PNG" alt="model"></p>
<h3 id="5-4-Experiments"><a href="#5-4-Experiments" class="headerlink" title="5.4 Experiments"></a>5.4 Experiments</h3><p>Dataset: CelebA, RaFD</p>
<p><img src="/img/gan_and_applications/stargan-e1.PNG" alt="e"></p>
<p><img src="/img/gan_and_applications/stargan-e3.PNG" alt="e"></p>
<p><img src="/img/gan_and_applications/stargan-e4.PNG" alt="e"></p>
<p>Metrics: AMT(human evaluation)</p>
<p><img src="/img/gan_and_applications/stargan-e2.PNG" alt="e"></p>
<p>Dataset: RaFD dataset (90%/10% splitting for training and test sets) </p>
<p>Metrics: compute the classification error of a facial expression on synthesized images</p>
<p><img src="/img/gan_and_applications/stargan-e5.PNG" alt="e"></p>
<h2 id="6-Pix2Pix-Image-to-Image-Translation-with-Conditional-Adversarial-Networks-use-paired-data-CVPR-2017"><a href="#6-Pix2Pix-Image-to-Image-Translation-with-Conditional-Adversarial-Networks-use-paired-data-CVPR-2017" class="headerlink" title="6 Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks(use paired data)(CVPR 2017)"></a>6 Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks(use paired data)(CVPR 2017)</h2><h3 id="6-1-Introduction"><a href="#6-1-Introduction" class="headerlink" title="6.1 Introduction"></a>6.1 Introduction</h3><p>We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. </p>
<p>we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either </p>
<p>(One architecture to different works)</p>
<h3 id="6-2-Model"><a href="#6-2-Model" class="headerlink" title="6.2 Model"></a>6.2 Model</h3><h4 id="6-2-1-Generator-with-skips"><a href="#6-2-1-Generator-with-skips" class="headerlink" title="6.2.1 Generator with skips"></a>6.2.1 Generator with skips</h4><p><img src="/img/gan_and_applications/pix2pix-model1.PNG" alt="model"></p>
<h4 id="6-2-2-Conditional-GANs"><a href="#6-2-2-Conditional-GANs" class="headerlink" title="6.2.2 Conditional GANs"></a>6.2.2 Conditional GANs</h4><p><img src="/img/gan_and_applications/pix2pix-model2.PNG" alt="model"></p>
<h4 id="6-2-3-PatchGAN"><a href="#6-2-3-PatchGAN" class="headerlink" title="6.2.3 PatchGAN"></a>6.2.3 PatchGAN</h4><p>It is well known that the L2 loss and L1produce blurry results on image generation problems . Although these losses fail to encourage high-frequency crispness, in many cases they nonetheless accurately capture the low frequencies .</p>
<p>In order to model high-frequencies, it is sufficient to restrict our attention to the structure in local image patches. Therefore, we design a discriminator architecture – which we term a PatchGAN – that only penalizes structure at the scale of patches. This discriminator tries to classify if each N × N patch in an image is real or fake. We run this discriminator convolutionally across the image, averaging all responses to provide the ultimate output of D </p>
<h3 id="6-2-4-Loss"><a href="#6-2-4-Loss" class="headerlink" title="6.2.4 Loss"></a>6.2.4 Loss</h3><p><img src="/img/gan_and_applications/pix2pix-loss.png" alt="loss"></p>
<h3 id="6-3-Experiments"><a href="#6-3-Experiments" class="headerlink" title="6.3 Experiments"></a>6.3 Experiments</h3><p>Dataset:</p>
<ol>
<li>Semantic labels$photo, trained on the Cityscapes dataset.</li>
<li>Architectural labels!photo, trained on CMP Facades</li>
<li>Map to aerial photo, trained on data scraped from Google Maps.</li>
<li>BW to color photos, trained on [50 Imagenet large scale visual recognition challenge].</li>
<li>Edges to photo, trained on data from [64 Generative visual manipulation on the natural image manifold] and [59 Fine-Grained Visual Comparisons with Local Learning ]; binary edges generated using the HED edge detector [57 Holistically-nested edge detection ]  plus post processing.</li>
<li>Sketch to photo: tests edges to photo models on human drawn sketches from [18 How do humans sketch objects].</li>
<li>Day to night, trained on [32 Transient attributes for high-level understanding and editing of outdoor<br>scenes ].</li>
<li>Thermal to color photos, trained on data from [26 Multispectral pedestrian detection: Benchmark dataset and baseline].</li>
<li>Photo with missing pixels to inpainted photo, trained on Paris StreetView from [13 What makes paris look like paris] </li>
</ol>
<p>Metrics: AMT, FCN-scores</p>
<p><img src="/img/gan_and_applications/pix2pix-e1.PNG" alt="loss"></p>
<p><img src="/img/gan_and_applications/pix2pix-e2.PNG" alt="loss"></p>
<p><img src="/img/gan_and_applications/pix2pix-e3.PNG" alt="loss"></p>
<h2 id="7-Photo-Realistic-Single-Image-Super-Resolution-Using-a-GAN-use-paired-data-to-train-CVPR-2017"><a href="#7-Photo-Realistic-Single-Image-Super-Resolution-Using-a-GAN-use-paired-data-to-train-CVPR-2017" class="headerlink" title="7 Photo-Realistic Single Image Super-Resolution Using a GAN(use paired data to train)(CVPR 2017)"></a>7 Photo-Realistic Single Image Super-Resolution Using a GAN(use paired data to train)(CVPR 2017)</h2><h3 id="7-1-Task"><a href="#7-1-Task" class="headerlink" title="7.1 Task"></a>7.1 Task</h3><p>Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? </p>
<p>Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution.</p>
<p>To our knowledge, it is the first framework capable of inferring photo-realistic natural images for <strong>4× upscaling</strong> factors. To achieve this, we propose a perceptual loss function which consists of an <strong>adversarial loss</strong> and a <strong>content loss</strong></p>
<h3 id="7-2-Model"><a href="#7-2-Model" class="headerlink" title="7.2 Model"></a>7.2 Model</h3><p><img src="/img/gan_and_applications/sr-model.PNG" alt="model"></p>
<p>Loss:</p>
<p><img src="/img/gan_and_applications/sr-loss.png" alt="model"></p>
<ol>
<li>$φ<em>{i,j}$ in $l</em>{VGG/i,j}^{SR}$ , we indicate the feature map obtained by the j-th convolution (after activation) before the i-th max pooling layer within the VGG19 network </li>
<li>D network is optimized by the min-max game</li>
<li>G network is optimized by the loss $l^{SR}$</li>
</ol>
<h3 id="7-3-Experiments"><a href="#7-3-Experiments" class="headerlink" title="7.3 Experiments"></a>7.3 Experiments</h3><p>Dataset: </p>
<ol>
<li>Set5 [Low-complexity single-image super-resolution based on nonnegative neighbor embedding ],</li>
<li>Set14 [On single image scale-up using sparse-representations ]</li>
<li>BSD100</li>
<li>the testing set of BSD300  </li>
</ol>
<p>Metrics: Mean opinion score (MOS) testing(human evaluation)</p>
<p><img src="/img/gan_and_applications/sr-e1.PNG" alt="experiment"></p>
<p><img src="/img/gan_and_applications/sr-e2.PNG" alt="e"></p>
<h2 id="8-Conclusion"><a href="#8-Conclusion" class="headerlink" title="8 Conclusion"></a>8 Conclusion</h2><h3 id="8-1-Reason-for-using-GAN"><a href="#8-1-Reason-for-using-GAN" class="headerlink" title="8.1 Reason for using GAN"></a>8.1 Reason for using GAN</h3><ul>
<li><p>Difficulties of traditional methods</p>
<ol>
<li>How to design effective loss</li>
<li>How to use unpaired data</li>
</ol>
</li>
<li><p>GAN’s advantages</p>
<ol>
<li>No need of the specific loss, but a high level goal</li>
<li>Able to handle unpaired data</li>
</ol>
</li>
<li><p>GAN’s disadvantages</p>
<ol>
<li>The Generator network often produce insensitive results</li>
<li>Mode collapse: all inputs are mapped to the same output</li>
</ol>
<p><img src="/img/gan_and_applications/mse-problem.PNG" alt="mse problem"></p>
<p>​</p>
</li>
</ul>
<h3 id="8-2-Good-ideas"><a href="#8-2-Good-ideas" class="headerlink" title="8.2 Good ideas"></a>8.2 Good ideas</h3><ul>
<li>GAN Loss: keep high level domain feature</li>
<li>Keep specific entity feature<ul>
<li>Given separated but otherwise unlabeled samples from domains $S$ and $T$ and a perceptual function $f$, learn a mapping $G : S \to T$ such that $f(x) ∼ f(G(x))$ <ol>
<li>Perceptual Loss</li>
<li>pre-trained f</li>
</ol>
</li>
<li>Cycle consistency</li>
<li>Enhancement network</li>
</ul>
</li>
<li>Translations for multiple domains using only a single model</li>
</ul>
<h3 id="8-3-Metrics"><a href="#8-3-Metrics" class="headerlink" title="8.3 Metrics"></a>8.3 Metrics</h3><ul>
<li>Human evaluation: AMT, MOS</li>
<li>Visualizing the generated  results</li>
<li>Use a model in the target domain to evaluate: FCN Scores(MNIST classifiers, VGG face classifier)</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/GAN/" rel="tag"># GAN</a>
          
            <a href="/tags/Image-Translation/" rel="tag"># Image Translation</a>
          
            <a href="/tags/Cross-Domain-Learning/" rel="tag"># Cross Domain Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/11/09/Capsule-net/" rel="next" title="Capsule net">
                <i class="fa fa-chevron-left"></i> Capsule net
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/16/ICML-18-GAN-理论文章总结/" rel="prev" title="ICML'18 GAN 理论文章总结">
                ICML'18 GAN 理论文章总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">N4A</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-GAN"><span class="nav-number">1.</span> <span class="nav-text">1 GAN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Introduction"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Theory-Analysis"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 Theory Analysis</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Cycle-GAN-ICCV-2017"><span class="nav-number">2.</span> <span class="nav-text">2 Cycle GAN(ICCV 2017)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-Task-Cross-Domain-Image-Translation"><span class="nav-number">3.</span> <span class="nav-text">2.1 Task: Cross Domain Image Translation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Model-Cycle-consistent"><span class="nav-number">3.1.</span> <span class="nav-text">2.2 Model: Cycle consistent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Experiment"><span class="nav-number">3.2.</span> <span class="nav-text">2.3 Experiment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Limitations"><span class="nav-number">3.3.</span> <span class="nav-text">2.4 Limitations</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-DIAT-Deep-Identity-aware-Transfer-of-Facial-Attributes"><span class="nav-number">4.</span> <span class="nav-text">3 DIAT: Deep Identity-aware Transfer of Facial Attributes</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Task-Identity-aware-Transfer-of-Facial-Attributes"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 Task: Identity-aware Transfer of Facial Attributes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Model"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-DIAT-A"><span class="nav-number">4.3.</span> <span class="nav-text">3.3 DIAT-A</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Experiments"><span class="nav-number">4.4.</span> <span class="nav-text">3.4 Experiments</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Unsupervised-Cross-Domain-Image-Generation-ICLR-2017"><span class="nav-number">5.</span> <span class="nav-text">4 Unsupervised Cross-Domain Image Generation(ICLR 2017 )</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Task"><span class="nav-number">5.1.</span> <span class="nav-text">4.1 Task</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Model"><span class="nav-number">5.2.</span> <span class="nav-text">4.2 Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Experiments"><span class="nav-number">5.3.</span> <span class="nav-text">4.3 Experiments</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-StarGAN-Multi-Domain-Image-to-Image-Translation"><span class="nav-number">6.</span> <span class="nav-text">5 StarGAN: Multi-Domain Image-to-Image Translation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-Introduction"><span class="nav-number">6.1.</span> <span class="nav-text">5.1 Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Model"><span class="nav-number">6.2.</span> <span class="nav-text">5.2 Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-Training-with-Multiple-Datasets"><span class="nav-number">6.3.</span> <span class="nav-text">5.3 Training with Multiple Datasets</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-1-Mask-Vector"><span class="nav-number">6.3.1.</span> <span class="nav-text">5.3.1 Mask Vector</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-2-Training-Strategy"><span class="nav-number">6.3.2.</span> <span class="nav-text">5.3.2 Training Strategy</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-3-CelebA-and-RaFD-dataset-demo"><span class="nav-number">6.4.</span> <span class="nav-text">5.3.3 CelebA and RaFD dataset demo</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-Experiments"><span class="nav-number">6.5.</span> <span class="nav-text">5.4 Experiments</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Pix2Pix-Image-to-Image-Translation-with-Conditional-Adversarial-Networks-use-paired-data-CVPR-2017"><span class="nav-number">7.</span> <span class="nav-text">6 Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks(use paired data)(CVPR 2017)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-Introduction"><span class="nav-number">7.1.</span> <span class="nav-text">6.1 Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-Model"><span class="nav-number">7.2.</span> <span class="nav-text">6.2 Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-Generator-with-skips"><span class="nav-number">7.2.1.</span> <span class="nav-text">6.2.1 Generator with skips</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-2-Conditional-GANs"><span class="nav-number">7.2.2.</span> <span class="nav-text">6.2.2 Conditional GANs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-3-PatchGAN"><span class="nav-number">7.2.3.</span> <span class="nav-text">6.2.3 PatchGAN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-4-Loss"><span class="nav-number">7.3.</span> <span class="nav-text">6.2.4 Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-Experiments"><span class="nav-number">7.4.</span> <span class="nav-text">6.3 Experiments</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Photo-Realistic-Single-Image-Super-Resolution-Using-a-GAN-use-paired-data-to-train-CVPR-2017"><span class="nav-number">8.</span> <span class="nav-text">7 Photo-Realistic Single Image Super-Resolution Using a GAN(use paired data to train)(CVPR 2017)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-Task"><span class="nav-number">8.1.</span> <span class="nav-text">7.1 Task</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-Model"><span class="nav-number">8.2.</span> <span class="nav-text">7.2 Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-Experiments"><span class="nav-number">8.3.</span> <span class="nav-text">7.3 Experiments</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-Conclusion"><span class="nav-number">9.</span> <span class="nav-text">8 Conclusion</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-Reason-for-using-GAN"><span class="nav-number">9.1.</span> <span class="nav-text">8.1 Reason for using GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-Good-ideas"><span class="nav-number">9.2.</span> <span class="nav-text">8.2 Good ideas</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-Metrics"><span class="nav-number">9.3.</span> <span class="nav-text">8.3 Metrics</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">N4A</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://n4a-github-io.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/03/30/GAN-and-it-s-applications/';
          this.page.identifier = '2018/03/30/GAN-and-it-s-applications/';
          this.page.title = 'GAN and it\'s applications on image translation';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://n4a-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
