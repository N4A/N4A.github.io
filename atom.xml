<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>N4A Space</title>
  
  <subtitle>To understand and be understood</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://n4a.github.io/"/>
  <updated>2018-07-29T12:41:53.058Z</updated>
  <id>https://n4a.github.io/</id>
  
  <author>
    <name>N4A</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Meta Learning</title>
    <link href="https://n4a.github.io/2018/07/29/Meta-Learning/"/>
    <id>https://n4a.github.io/2018/07/29/Meta-Learning/</id>
    <published>2018-07-29T12:38:31.000Z</published>
    <updated>2018-07-29T12:41:53.058Z</updated>
    
    <content type="html"><![CDATA[<p>​    Meta Learning 最早源于上世纪八九十年代 [6], 最近成为研究的热点，这是一个很好的 可以用来解决 Learn to learn 问题的框架。 17 年 NIPS 有一个 Workshop on Meta Learning 。与迁移学习相比， Meta Learning 可以视为一个更泛化的概念。 </p><p>​    传统的机器学习方法为解决某一个特定的任务总是需要大量的训练数据，有一个很直 观的原因是因为传统的机器学习方法在训练一个模型时，总是从零开始学习。但是人类 的学习过程并不是这样，显然人的学习是一个连续的过程，当一个人想要解决某一个问题 时，他会使用之前跟这个任务相关的知识。以图像分类任务为例，传统的机器学习方法， 例如普通的 CNN 模型，或者 AlexNet， VGG， ResNet 这些模型都需要大量的训练数据。 为了解决数据依赖的问题，现在的一个研究热点就是“one shot learning” (few shot learning)[4]。许多解决这个问题的方法 [9, 8] 就是基于 Meta Learning。 </p><a id="more"></a><div class="row">    <embed src="/pdf/meta-learning.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    Meta Learning 最早源于上世纪八九十年代 [6], 最近成为研究的热点，这是一个很好的 可以用来解决 Learn to learn 问题的框架。 17 年 NIPS 有一个 Workshop on Meta Learning 。与迁移学习相比， Meta Learning 可以视为一个更泛化的概念。 &lt;/p&gt;
&lt;p&gt;​    传统的机器学习方法为解决某一个特定的任务总是需要大量的训练数据，有一个很直 观的原因是因为传统的机器学习方法在训练一个模型时，总是从零开始学习。但是人类 的学习过程并不是这样，显然人的学习是一个连续的过程，当一个人想要解决某一个问题 时，他会使用之前跟这个任务相关的知识。以图像分类任务为例，传统的机器学习方法， 例如普通的 CNN 模型，或者 AlexNet， VGG， ResNet 这些模型都需要大量的训练数据。 为了解决数据依赖的问题，现在的一个研究热点就是“one shot learning” (few shot learning)[4]。许多解决这个问题的方法 [9, 8] 就是基于 Meta Learning。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="Meta Learning" scheme="https://n4a.github.io/tags/Meta-Learning/"/>
    
      <category term="AI" scheme="https://n4a.github.io/tags/AI/"/>
    
      <category term="Machine Learning" scheme="https://n4a.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>ICML&#39;18 GAN 理论文章总结</title>
    <link href="https://n4a.github.io/2018/07/16/ICML-18-GAN-%E7%90%86%E8%AE%BA%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93/"/>
    <id>https://n4a.github.io/2018/07/16/ICML-18-GAN-理论文章总结/</id>
    <published>2018-07-16T15:17:07.000Z</published>
    <updated>2018-07-17T14:44:58.856Z</updated>
    
    <content type="html"><![CDATA[<p>这个部分有五篇文章，其中：</p><ol><li>两篇文是通过改变GAN的结构以解决GAN训练困难和模式消失（Mode collapse）的问题。</li><li>一篇文章从新的数学角度推导GAN的更新过程，该更新过程更一般化，原有的GAN参数更新过程可视为其某种条件下的特例。文中也简要说明了该更新过程是 stable 的。</li><li>一篇文章探究了GAN中生成器的 Jacobian 矩阵的奇异值分布和 GAN 性能的关系。这篇文章很有趣，它根据生成器的 Jacobian 矩阵定义了一个<strong>condition number</strong>，然后在训练过程中发现该值与常用的GAN评估方法 <strong>Inception Score</strong> 和 <strong>Frechet Inception Distance</strong> 的评估值十分相关，最后文中提出一种方法通过控制 <strong>condition number</strong> 来改进 GAN 的训练过程。</li><li>一篇文章提出一种新的方法计算WGAN中 Wasserstein distance，同时做了许多相关的理论推导。 这篇文章理论知识很多，我看起来很费劲，也很困惑。文章中虽然做了很多的工作，但是相比于WGAN没有太大的创新。</li></ol><a id="more"></a> <div class="row">    <embed src="/pdf/memo-icml18-gan.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这个部分有五篇文章，其中：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;两篇文是通过改变GAN的结构以解决GAN训练困难和模式消失（Mode collapse）的问题。&lt;/li&gt;
&lt;li&gt;一篇文章从新的数学角度推导GAN的更新过程，该更新过程更一般化，原有的GAN参数更新过程可视为其某种条件下的特例。文中也简要说明了该更新过程是 stable 的。&lt;/li&gt;
&lt;li&gt;一篇文章探究了GAN中生成器的 Jacobian 矩阵的奇异值分布和 GAN 性能的关系。这篇文章很有趣，它根据生成器的 Jacobian 矩阵定义了一个&lt;strong&gt;condition number&lt;/strong&gt;，然后在训练过程中发现该值与常用的GAN评估方法 &lt;strong&gt;Inception Score&lt;/strong&gt; 和 &lt;strong&gt;Frechet Inception Distance&lt;/strong&gt; 的评估值十分相关，最后文中提出一种方法通过控制 &lt;strong&gt;condition number&lt;/strong&gt; 来改进 GAN 的训练过程。&lt;/li&gt;
&lt;li&gt;一篇文章提出一种新的方法计算WGAN中 Wasserstein distance，同时做了许多相关的理论推导。 这篇文章理论知识很多，我看起来很费劲，也很困惑。文章中虽然做了很多的工作，但是相比于WGAN没有太大的创新。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="GAN" scheme="https://n4a.github.io/tags/GAN/"/>
    
      <category term="ICML" scheme="https://n4a.github.io/tags/ICML/"/>
    
  </entry>
  
  <entry>
    <title>GAN and it&#39;s applications on image translation</title>
    <link href="https://n4a.github.io/2018/03/30/GAN-and-it-s-applications/"/>
    <id>https://n4a.github.io/2018/03/30/GAN-and-it-s-applications/</id>
    <published>2018-03-30T08:12:14.000Z</published>
    <updated>2018-07-17T08:31:05.797Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-GAN"><a href="#1-GAN" class="headerlink" title="1 GAN"></a>1 GAN</h2><h3 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h3><p>To learn the generator’s distribution $p_g$ over data x, we define a prior on input noise variables $p_z(z)$, then represent a mapping to data space as $G(z; θ_g)$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $θ_g$. We also define a second multilayer perceptron $D(x; θ_d)$ that outputs a single scalar. $D(x)$ represents the probability that $x$ came from the data rather than $p_g$. We train $D$ to maximize the probability of assigning the correct label to both training examples and samples from $G$. We simultaneously train $G$ to minimize $log(1 - D(G(z)))$. In other words, $D$ and $G$ play the following two-player mini-max game with value function $V (G; D):$</p><p>$$min_G max_DV (D; G) = E_x∼p_{data(x)}[log D(x)] + E_{z∼p_z(z)}[log(1 - D(G(z)))]$$</p><a id="more"></a><h3 id="1-2-Theory-Analysis"><a href="#1-2-Theory-Analysis" class="headerlink" title="1.2 Theory Analysis"></a>1.2 Theory Analysis</h3><p><img src="/img/gan_and_applications/gan.PNG" alt="gan"></p><h2 id="2-Cycle-GAN-ICCV-2017"><a href="#2-Cycle-GAN-ICCV-2017" class="headerlink" title="2 Cycle GAN(ICCV 2017)"></a>2 Cycle GAN(ICCV 2017)</h2><h2 id="2-1-Task-Cross-Domain-Image-Translation"><a href="#2-1-Task-Cross-Domain-Image-Translation" class="headerlink" title="2.1 Task: Cross Domain Image Translation"></a>2.1 Task: Cross Domain Image Translation</h2><p>Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image.</p><p>In this paper, we present a method that can learn to do the same: capturing special characteristics of one image collection and figuring out how these characteristics could be translated into the other image collection, all in the absence of any paired training examples. </p><h3 id="2-2-Model-Cycle-consistent"><a href="#2-2-Model-Cycle-consistent" class="headerlink" title="2.2 Model: Cycle consistent"></a>2.2 Model: Cycle consistent</h3><p><img src="/img/gan_and_applications/cycleGAN.PNG" alt="Cycle GAN"></p><p>Loss:</p><p><img src="/img/gan_and_applications/cycleGAN-loss.png" alt="cycle gan loss"></p><h3 id="2-3-Experiment"><a href="#2-3-Experiment" class="headerlink" title="2.3 Experiment"></a>2.3 Experiment</h3><p><img src="/img/gan_and_applications/cycleGAN-experiments.PNG" alt="Experiments"></p><ol><li><p>Dataset: Cityscapes dataset , map and aerial photo on data scraped from Google Maps </p></li><li><p>Metrics: AMT perceptual studies, FCN score, Semantic segmentation metrics</p></li><li><p>Result:</p><p><img src="/img/gan_and_applications/cycleGAN-experiments2.PNG" alt="Experiments"></p><p><img src="/img/gan_and_applications/cycleGAN-experiments3.PNG" alt="Experiments"></p><p><img src="/img/gan_and_applications/cycleGAN-experiments4.PNG" alt="Experiments"></p></li></ol><h3 id="2-4-Limitations"><a href="#2-4-Limitations" class="headerlink" title="2.4 Limitations"></a>2.4 Limitations</h3><ol><li>On translation tasks that involve color and texture changes, like many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. </li><li>Some failure cases are caused by the distribution characteristics of the training datasets.</li><li>We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method.  </li></ol><p><img src="/img/gan_and_applications/cycleGAN-limit.PNG" alt="limitations"></p><h2 id="3-DIAT-Deep-Identity-aware-Transfer-of-Facial-Attributes"><a href="#3-DIAT-Deep-Identity-aware-Transfer-of-Facial-Attributes" class="headerlink" title="3 DIAT: Deep Identity-aware Transfer of Facial Attributes"></a>3 DIAT: Deep Identity-aware Transfer of Facial Attributes</h2><h3 id="3-1-Task-Identity-aware-Transfer-of-Facial-Attributes"><a href="#3-1-Task-Identity-aware-Transfer-of-Facial-Attributes" class="headerlink" title="3.1 Task: Identity-aware Transfer of Facial Attributes"></a>3.1 Task: Identity-aware Transfer of Facial Attributes</h3><p>Our DIAT and DIAT-A models can provide a unified solution for several representative facial attribute transfer tasks such as <strong>expression transfer</strong>, <strong>accessory removal</strong>, <strong>age progression</strong>, and <strong>gender</strong> transfer </p><h3 id="3-2-Model"><a href="#3-2-Model" class="headerlink" title="3.2 Model"></a>3.2 Model</h3><p>In this section, a two-stage scheme is developed to tackle the identity-aware attribute transfer task. </p><ol><li><p>Face transform network</p><p><img src="/img/gan_and_applications/diat-transform.PNG" alt="diat-transform"></p><p>Loss: </p><p><img src="/img/gan_and_applications/diat-transform-loss.png" alt="loss"></p></li><li><p>Face Enhancement Network</p><p><img src="/img/gan_and_applications/diat-enhance.PNG" alt="enhance"></p><p>Loss:</p><p><img src="/img/gan_and_applications/diat-enhance-loss.png" alt="loss"></p></li></ol><h3 id="3-3-DIAT-A"><a href="#3-3-DIAT-A" class="headerlink" title="3.3 DIAT-A"></a>3.3 DIAT-A</h3><p>In DIAT, the perceptual identity loss is defined on the pre-trained VGG-Face. Actually, it may be more effective to define this loss on some CNN trained to attribute transfer. Here we treat identity-preserving and attribute transfer as two related tasks, and define the perceptual identity loss based on the convolutional features of the discriminator. By this way, the network parameters for identity loss will be changed along with the updating of discriminator, and thus we named it as adaptive perceptual identity loss. </p><p><img src="/img/gan_and_applications/diat-a-transform-loss.png" alt="loss"></p><h3 id="3-4-Experiments"><a href="#3-4-Experiments" class="headerlink" title="3.4 Experiments"></a>3.4 Experiments</h3><p>Dataset: a subset of the aligned CelebA dataset  </p><p><img src="/img/gan_and_applications/diat-experiment1.PNG" alt="experiment"></p><p><img src="/img/gan_and_applications/diat-experiment2.PNG" alt="experiment"></p><p><img src="/img/gan_and_applications/diat-experiment3.PNG" alt="experiment"></p><h2 id="4-Unsupervised-Cross-Domain-Image-Generation-ICLR-2017"><a href="#4-Unsupervised-Cross-Domain-Image-Generation-ICLR-2017" class="headerlink" title="4 Unsupervised Cross-Domain Image Generation(ICLR 2017 )"></a>4 Unsupervised Cross-Domain Image Generation(ICLR 2017 )</h2><h3 id="4-1-Task"><a href="#4-1-Task" class="headerlink" title="4.1 Task"></a>4.1 Task</h3><p>Recent achievements replicate some of these capabilities to some degree: Generative Adversarial Networks (GANs) are able to convincingly generate novel samples that match that of a given training set; style transfer methods are able to alter the visual style of images; domain adaptation methods are able to generalize learned functions to new domains even without labeled samples in the target domain and transfer learning is now commonly used to import existing knowledge and to make learning much more efficient.</p><p>These capabilities, however, do not address the general analogy synthesis problem that we tackle in this work. Namely, <strong>given separated but otherwise unlabeled samples from domains $S$ and $T$ and a perceptual function $f$, learn a mapping $G : S \to T$ such that $f(x) ∼ f(G(x)$ </strong></p><p>As a main application challenge, we tackle the problem of <strong>emoji generation for a given facial image</strong>. Despite a growing interest in emoji and the hurdle of creating such personal emoji manually, no system has been proposed, to our knowledge, that can solve this problem. Our method is able to produce face emoji that are visually appealing and capture much more of the facial characteristics than the emoji created by well-trained human annotators who use the conventional tools.</p><h3 id="4-2-Model"><a href="#4-2-Model" class="headerlink" title="4.2 Model"></a>4.2 Model</h3><p><img src="/img/gan_and_applications/dtn-model.PNG" alt="dtn model"></p><p>Loss: </p><p><img src="/img/gan_and_applications/dtn-loss.png" alt="dtn-loss"></p><ol><li>$D$ is a ternary classification function from the domain $T$ to 1,2,3, and $D_i(x)$ is the<br>probability it assigns to class $i = 1,2,3$ for an input sample $x$</li><li>During optimization, $L_G$ is minimized over $g$ and $L_D$ is minimized over $D$ </li><li>$L_{CONST}$ enforces f-constancy for $x \in S$, while $L_{TID}$ enforces that for samples $x \in T$  </li><li>$L_{TV}$ is an anisotropic total variation loss, which is added in order to slightly smooth the resulting image</li><li>$f$ is trained use other datasets before training this model</li></ol><h3 id="4-3-Experiments"><a href="#4-3-Experiments" class="headerlink" title="4.3 Experiments"></a>4.3 Experiments</h3><p><img src="/img/gan_and_applications/dtn-e1.PNG" alt="dtn experiment"></p><p>Dataset: </p><ol><li>Street View House Number (SVHN) dataset to the domain of the MNIST dataset</li><li>FROM PHOTOS TO EMOJI</li></ol><p>Metrics: MNIST Accuracy</p><p><img src="/img/gan_and_applications/dtn-e2.PNG" alt="e"></p><p><img src="/img/gan_and_applications/dtn-e3.PNG" alt="e3"></p><h2 id="5-StarGAN-Multi-Domain-Image-to-Image-Translation"><a href="#5-StarGAN-Multi-Domain-Image-to-Image-Translation" class="headerlink" title="5 StarGAN: Multi-Domain Image-to-Image Translation"></a>5 StarGAN: Multi-Domain Image-to-Image Translation</h2><h3 id="5-1-Introduction"><a href="#5-1-Introduction" class="headerlink" title="5.1 Introduction"></a>5.1 Introduction</h3><p>Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. </p><p>To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model</p><p>We can further extend to training multiple domains from different datasets.</p><h3 id="5-2-Model"><a href="#5-2-Model" class="headerlink" title="5.2 Model"></a>5.2 Model</h3><p><img src="/img/gan_and_applications/stargan-model.PNG" alt="model"></p><p>Loss:</p><p><img src="/img/gan_and_applications/stargan-loss.png" alt="loss"></p><ol><li>a domain classification loss of real images($L_{cls}^r$) used to optimize D, and a domain classification loss of fake images($L_{cls}^f$) used to optimize G </li><li>Use $L_{rec}$ to guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs.</li></ol><h3 id="5-3-Training-with-Multiple-Datasets"><a href="#5-3-Training-with-Multiple-Datasets" class="headerlink" title="5.3 Training with Multiple Datasets"></a>5.3 Training with Multiple Datasets</h3><h4 id="5-3-1-Mask-Vector"><a href="#5-3-1-Mask-Vector" class="headerlink" title="5.3.1 Mask Vector"></a>5.3.1 Mask Vector</h4><p><img src="/img/gan_and_applications/stargan-maskv.PNG" alt="mask"></p><p>In StarGAN, we use an n-dimensional one-hot vector to represent m, with n being the number of datasets.  and $c_i$ represents a vector for the labels of the $i$-th dataset. The vector of the known label $c_i$ can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes</p><h4 id="5-3-2-Training-Strategy"><a href="#5-3-2-Training-Strategy" class="headerlink" title="5.3.2 Training Strategy"></a>5.3.2 Training Strategy</h4><p> When training StarGAN with multiple datasets, we use the domain label $\overset{\sim}{c}$ defined at above as input to the generator. By doing so, the generator learns to ignore the unspecified labels, which are zero vectors, and<br>focus on the explicitly given label. The structure of the generator is exactly the same as in training with a single dataset, except for the dimension of the input label $\overset{\sim}{c}$. </p><h3 id="5-3-3-CelebA-and-RaFD-dataset-demo"><a href="#5-3-3-CelebA-and-RaFD-dataset-demo" class="headerlink" title="5.3.3 CelebA and RaFD dataset demo"></a>5.3.3 CelebA and RaFD dataset demo</h3><p><img src="/img/gan_and_applications/stargan-model2.PNG" alt="model"></p><p><img src="/img/gan_and_applications/stargan-model3.PNG" alt="model"></p><h3 id="5-4-Experiments"><a href="#5-4-Experiments" class="headerlink" title="5.4 Experiments"></a>5.4 Experiments</h3><p>Dataset: CelebA, RaFD</p><p><img src="/img/gan_and_applications/stargan-e1.PNG" alt="e"></p><p><img src="/img/gan_and_applications/stargan-e3.PNG" alt="e"></p><p><img src="/img/gan_and_applications/stargan-e4.PNG" alt="e"></p><p>Metrics: AMT(human evaluation)</p><p><img src="/img/gan_and_applications/stargan-e2.PNG" alt="e"></p><p>Dataset: RaFD dataset (90%/10% splitting for training and test sets) </p><p>Metrics: compute the classification error of a facial expression on synthesized images</p><p><img src="/img/gan_and_applications/stargan-e5.PNG" alt="e"></p><h2 id="6-Pix2Pix-Image-to-Image-Translation-with-Conditional-Adversarial-Networks-use-paired-data-CVPR-2017"><a href="#6-Pix2Pix-Image-to-Image-Translation-with-Conditional-Adversarial-Networks-use-paired-data-CVPR-2017" class="headerlink" title="6 Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks(use paired data)(CVPR 2017)"></a>6 Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks(use paired data)(CVPR 2017)</h2><h3 id="6-1-Introduction"><a href="#6-1-Introduction" class="headerlink" title="6.1 Introduction"></a>6.1 Introduction</h3><p>We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. </p><p>we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either </p><p>(One architecture to different works)</p><h3 id="6-2-Model"><a href="#6-2-Model" class="headerlink" title="6.2 Model"></a>6.2 Model</h3><h4 id="6-2-1-Generator-with-skips"><a href="#6-2-1-Generator-with-skips" class="headerlink" title="6.2.1 Generator with skips"></a>6.2.1 Generator with skips</h4><p><img src="/img/gan_and_applications/pix2pix-model1.PNG" alt="model"></p><h4 id="6-2-2-Conditional-GANs"><a href="#6-2-2-Conditional-GANs" class="headerlink" title="6.2.2 Conditional GANs"></a>6.2.2 Conditional GANs</h4><p><img src="/img/gan_and_applications/pix2pix-model2.PNG" alt="model"></p><h4 id="6-2-3-PatchGAN"><a href="#6-2-3-PatchGAN" class="headerlink" title="6.2.3 PatchGAN"></a>6.2.3 PatchGAN</h4><p>It is well known that the L2 loss and L1produce blurry results on image generation problems . Although these losses fail to encourage high-frequency crispness, in many cases they nonetheless accurately capture the low frequencies .</p><p>In order to model high-frequencies, it is sufficient to restrict our attention to the structure in local image patches. Therefore, we design a discriminator architecture – which we term a PatchGAN – that only penalizes structure at the scale of patches. This discriminator tries to classify if each N × N patch in an image is real or fake. We run this discriminator convolutionally across the image, averaging all responses to provide the ultimate output of D </p><h3 id="6-2-4-Loss"><a href="#6-2-4-Loss" class="headerlink" title="6.2.4 Loss"></a>6.2.4 Loss</h3><p><img src="/img/gan_and_applications/pix2pix-loss.png" alt="loss"></p><h3 id="6-3-Experiments"><a href="#6-3-Experiments" class="headerlink" title="6.3 Experiments"></a>6.3 Experiments</h3><p>Dataset:</p><ol><li>Semantic labels$photo, trained on the Cityscapes dataset.</li><li>Architectural labels!photo, trained on CMP Facades</li><li>Map to aerial photo, trained on data scraped from Google Maps.</li><li>BW to color photos, trained on [50 Imagenet large scale visual recognition challenge].</li><li>Edges to photo, trained on data from [64 Generative visual manipulation on the natural image manifold] and [59 Fine-Grained Visual Comparisons with Local Learning ]; binary edges generated using the HED edge detector [57 Holistically-nested edge detection ]  plus post processing.</li><li>Sketch to photo: tests edges to photo models on human drawn sketches from [18 How do humans sketch objects].</li><li>Day to night, trained on [32 Transient attributes for high-level understanding and editing of outdoor<br>scenes ].</li><li>Thermal to color photos, trained on data from [26 Multispectral pedestrian detection: Benchmark dataset and baseline].</li><li>Photo with missing pixels to inpainted photo, trained on Paris StreetView from [13 What makes paris look like paris] </li></ol><p>Metrics: AMT, FCN-scores</p><p><img src="/img/gan_and_applications/pix2pix-e1.PNG" alt="loss"></p><p><img src="/img/gan_and_applications/pix2pix-e2.PNG" alt="loss"></p><p><img src="/img/gan_and_applications/pix2pix-e3.PNG" alt="loss"></p><h2 id="7-Photo-Realistic-Single-Image-Super-Resolution-Using-a-GAN-use-paired-data-to-train-CVPR-2017"><a href="#7-Photo-Realistic-Single-Image-Super-Resolution-Using-a-GAN-use-paired-data-to-train-CVPR-2017" class="headerlink" title="7 Photo-Realistic Single Image Super-Resolution Using a GAN(use paired data to train)(CVPR 2017)"></a>7 Photo-Realistic Single Image Super-Resolution Using a GAN(use paired data to train)(CVPR 2017)</h2><h3 id="7-1-Task"><a href="#7-1-Task" class="headerlink" title="7.1 Task"></a>7.1 Task</h3><p>Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? </p><p>Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution.</p><p>To our knowledge, it is the first framework capable of inferring photo-realistic natural images for <strong>4× upscaling</strong> factors. To achieve this, we propose a perceptual loss function which consists of an <strong>adversarial loss</strong> and a <strong>content loss</strong></p><h3 id="7-2-Model"><a href="#7-2-Model" class="headerlink" title="7.2 Model"></a>7.2 Model</h3><p><img src="/img/gan_and_applications/sr-model.PNG" alt="model"></p><p>Loss:</p><p><img src="/img/gan_and_applications/sr-loss.png" alt="model"></p><ol><li>$φ<em>{i,j}$ in $l</em>{VGG/i,j}^{SR}$ , we indicate the feature map obtained by the j-th convolution (after activation) before the i-th max pooling layer within the VGG19 network </li><li>D network is optimized by the min-max game</li><li>G network is optimized by the loss $l^{SR}$</li></ol><h3 id="7-3-Experiments"><a href="#7-3-Experiments" class="headerlink" title="7.3 Experiments"></a>7.3 Experiments</h3><p>Dataset: </p><ol><li>Set5 [Low-complexity single-image super-resolution based on nonnegative neighbor embedding ],</li><li>Set14 [On single image scale-up using sparse-representations ]</li><li>BSD100</li><li>the testing set of BSD300  </li></ol><p>Metrics: Mean opinion score (MOS) testing(human evaluation)</p><p><img src="/img/gan_and_applications/sr-e1.PNG" alt="experiment"></p><p><img src="/img/gan_and_applications/sr-e2.PNG" alt="e"></p><h2 id="8-Conclusion"><a href="#8-Conclusion" class="headerlink" title="8 Conclusion"></a>8 Conclusion</h2><h3 id="8-1-Reason-for-using-GAN"><a href="#8-1-Reason-for-using-GAN" class="headerlink" title="8.1 Reason for using GAN"></a>8.1 Reason for using GAN</h3><ul><li><p>Difficulties of traditional methods</p><ol><li>How to design effective loss</li><li>How to use unpaired data</li></ol></li><li><p>GAN’s advantages</p><ol><li>No need of the specific loss, but a high level goal</li><li>Able to handle unpaired data</li></ol></li><li><p>GAN’s disadvantages</p><ol><li>The Generator network often produce insensitive results</li><li>Mode collapse: all inputs are mapped to the same output</li></ol><p><img src="/img/gan_and_applications/mse-problem.PNG" alt="mse problem"></p><p>​</p></li></ul><h3 id="8-2-Good-ideas"><a href="#8-2-Good-ideas" class="headerlink" title="8.2 Good ideas"></a>8.2 Good ideas</h3><ul><li>GAN Loss: keep high level domain feature</li><li>Keep specific entity feature<ul><li>Given separated but otherwise unlabeled samples from domains $S$ and $T$ and a perceptual function $f$, learn a mapping $G : S \to T$ such that $f(x) ∼ f(G(x))$ <ol><li>Perceptual Loss</li><li>pre-trained f</li></ol></li><li>Cycle consistency</li><li>Enhancement network</li></ul></li><li>Translations for multiple domains using only a single model</li></ul><h3 id="8-3-Metrics"><a href="#8-3-Metrics" class="headerlink" title="8.3 Metrics"></a>8.3 Metrics</h3><ul><li>Human evaluation: AMT, MOS</li><li>Visualizing the generated  results</li><li>Use a model in the target domain to evaluate: FCN Scores(MNIST classifiers, VGG face classifier)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-GAN&quot;&gt;&lt;a href=&quot;#1-GAN&quot; class=&quot;headerlink&quot; title=&quot;1 GAN&quot;&gt;&lt;/a&gt;1 GAN&lt;/h2&gt;&lt;h3 id=&quot;1-1-Introduction&quot;&gt;&lt;a href=&quot;#1-1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1.1 Introduction&quot;&gt;&lt;/a&gt;1.1 Introduction&lt;/h3&gt;&lt;p&gt;To learn the generator’s distribution $p_g$ over data x, we define a prior on input noise variables $p_z(z)$, then represent a mapping to data space as $G(z; θ_g)$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $θ_g$. We also define a second multilayer perceptron $D(x; θ_d)$ that outputs a single scalar. $D(x)$ represents the probability that $x$ came from the data rather than $p_g$. We train $D$ to maximize the probability of assigning the correct label to both training examples and samples from $G$. We simultaneously train $G$ to minimize $log(1 - D(G(z)))$. In other words, $D$ and $G$ play the following two-player mini-max game with value function $V (G; D):$&lt;/p&gt;
&lt;p&gt;$$min_G max_DV (D; G) = E_x∼p_{data(x)}[log D(x)] + E_{z∼p_z(z)}[log(1 - D(G(z)))]$$&lt;/p&gt;
    
    </summary>
    
    
      <category term="GAN" scheme="https://n4a.github.io/tags/GAN/"/>
    
      <category term="Image Translation" scheme="https://n4a.github.io/tags/Image-Translation/"/>
    
      <category term="Cross Domain Learning" scheme="https://n4a.github.io/tags/Cross-Domain-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Capsule net</title>
    <link href="https://n4a.github.io/2017/11/09/Capsule-net/"/>
    <id>https://n4a.github.io/2017/11/09/Capsule-net/</id>
    <published>2017-11-09T08:08:07.000Z</published>
    <updated>2018-07-17T12:05:22.950Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Transforming-Auto-encoder-ICANN-2011"><a href="#1-Transforming-Auto-encoder-ICANN-2011" class="headerlink" title="1 Transforming Auto-encoder(ICANN 2011 )"></a>1 Transforming Auto-encoder(ICANN 2011 )</h2><h3 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h3><blockquote><p>Artificial neural networks should use local “capsules” that perform some quite complicated internal computations on their inputs and then encapsulate the results of these computations into a small vector of highly informative outputs. Each capsule learns to recognize an implicitly defined visual entity over a limited domain of viewing conditions and deformations and it outputs both the probability that the entity is present within its limited domain and a set of “instantiation parameters” that may include the precise pose, lighting and deformation of the visual entity relative to an implicitly defined canonical version of that entity. When the capsule is working properly, the probability of the visual entity being present is locally invariant – it does not change as the entity moves over the manifold of possible appearances within the limited domain covered by the capsule. The instantiation parameters, however, are “equivariant” – as the viewing conditions change and the entity moves over the appearance manifold, the instantiation parameters change by a corresponding amount because they are representing the intrinsic coordinates of the entity on the appearance manifold. </p></blockquote><a id="more"></a><h3 id="1-2-Details-about-a-image-shift-example"><a href="#1-2-Details-about-a-image-shift-example" class="headerlink" title="1.2 Details about a image shift example"></a>1.2 Details about a image shift example</h3><ol><li><p>Model graph</p><p><img src="/img/tae.PNG" alt="tae"></p></li><li><p>Core codes (use tensorflow)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, X_in, extra_in)</span>:</span></span><br><span class="line">  rec = tf.sigmoid(self.fc_layer(X_in, self.in_dim, self.r_dim,</span><br><span class="line">                                 <span class="string">'recog_layer_pre_act'</span>), <span class="string">'recog_layer'</span>)</span><br><span class="line">   </span><br><span class="line">  xy_vec = self.fc_layer(rec, self.r_dim, <span class="number">2</span>, <span class="string">'xy_prediction'</span>)</span><br><span class="line">  pro = tf.sigmoid(self.fc_layer(rec, self.r_dim, <span class="number">1</span>,</span><br><span class="line">                                 <span class="string">'probability_lin'</span>),<span class="string">'probability_prediction'</span>)</span><br><span class="line">  probability_vec = tf.tile(pro, (<span class="number">1</span>, self.in_dim))</span><br><span class="line">   </span><br><span class="line">  xy_extend = tf.add(xy_vec, extra_in)</span><br><span class="line">  gen = tf.sigmoid(self.fc_layer(xy_extend, <span class="number">2</span>, self.g_dim, <span class="string">'gen_pre_act'</span>), </span><br><span class="line">                   <span class="string">'gen_layer'</span>)</span><br><span class="line">   </span><br><span class="line">  out = self.fc_layer(gen, self.g_dim, self.in_dim, <span class="string">'out_prediction'</span>)</span><br><span class="line">   </span><br><span class="line">  <span class="keyword">return</span> tf.multiply(out, probability_vec)</span><br></pre></td></tr></table></figure></li><li><p>Experiment Result</p><p><img src="/img/experiment.PNG" alt="experiment"></p></li></ol><h3 id="1-3-Work-to-do"><a href="#1-3-Work-to-do" class="headerlink" title="1.3 Work to do"></a>1.3 Work to do</h3><p>​    Once pixel intensities have been converted into the outputs of a set of active, first-level capsules each of which produces an explicit representation of the pose of its visual entity, it is relatively easy to see how larger and more complex visual entities can be recognized by using <strong>agreements of the poses</strong> predicted by active, lower-level capsules. </p><p>​    <strong>Agreements of the poses example</strong>： If a capsule can learn to output the pose of its visual entity in a vector that is linearly related to the “natural” representations of pose used in computer graphics, there is a simple and highly selective test for whether the visual entities represented by two active capsules, A and B, have the right spatial relationship to activate a higher-level capsule, C. Suppose that the pose outputs of capsule A are represented by a matrix, TA, that specifies the coordinate transform between the canonical visual entity of A and the actual instantiation of that entity found by capsule A. If we multiply TA by the part-whole coordinate transform TAC that relates the canonical visual entity of A to the canonical visual entity of C, we get a prediction for TC. Similarly, we can use TB and TBC to get another prediction. If these predictions are a good match, the instantiations found by capsules A and B are in the right spatial relationship to activate capsule C and the average of the predictions tells us how the larger visual entity represented by C is transformed relative to the canonical visual entity of C. If, for example, A represents a mouth and B represents a nose, they can each make a prediction for the pose of the face. If these predictions agree, the mouth and nose must be in the right spatial relationship to form a face. An interesting property of this way of performing shape recognition is that the knowledge of part-whole relationships is viewpoint-invariant and is represented by weight matrices whereas the knowledge of the instantiation parameters of currently observed objects and their parts is viewpoint-equivariant and is represented by neural activities  </p><h2 id="2-Dynamic-Routing-Between-Capsules-NIPS-2017"><a href="#2-Dynamic-Routing-Between-Capsules-NIPS-2017" class="headerlink" title="2 Dynamic Routing Between Capsules(NIPS 2017)"></a>2 Dynamic Routing Between Capsules(NIPS 2017)</h2><h3 id="2-1-Introduction"><a href="#2-1-Introduction" class="headerlink" title="2.1 Introduction"></a>2.1 Introduction</h3><p>​    The previous paper said that: “Once pixel intensities have been converted into the outputs of a set of active, first-level capsules each of which produces an explicit representation of the pose of its visual entity, it is relatively easy to see how larger and more complex visual entities can be recognized by using <strong>agreements of the poses</strong> predicted by active, lower-level capsules. “ But it didn’t do the work to build a multilayer capsules network.</p><p>​    This paper give a implementation of the multilayer capsules network that do classification work on <strong>MNIST</strong> dataset. And it give a routing algorithm comparing to just the word <strong>agreements</strong>. It give a detail formulation to measure <strong>agreements</strong>. Then it use <strong>Margin Loss</strong> and <strong>Reconstruction</strong> as a regularization method </p><p>​    Besides, instead of  using common full connection layer to initialize the first-level capsules, it use convolutional neural network to do this work. The paper says: “Convolutional neural networks (CNNs) use translated replicas of learned feature detectors and this allows them to translate knowledge about good weight values acquired at one position in an image to other positions. This has proven extremely helpful in image interpretation. Even though we are replacing the scalar-output feature detectors of CNNs with vector-output capsules and max-pooling with routing-by-agreement, we would still like to replicate learned knowledge across space, so we make all but the last layer of capsules be convolutional. “ </p><h3 id="2-2-Model-detail"><a href="#2-2-Model-detail" class="headerlink" title="2.2 Model detail"></a>2.2 Model detail</h3><ol><li><p>Model graph</p><p><img src="/img/capsnet.PNG" alt="capsule net"></p></li><li><p>Core codes and algorithms</p><ol><li><p>First layer is a common convolutional layer</p></li><li><p>Second layer is PrimaryCaps layer.  But One can see ist as a Convolution layer with $$v_j = \frac{||s_j||^2}{1 +||s_j||^2 }\frac{s_j}{||s_j||}$$  as its block non-linearity.  Just as the following codes.(In the following codes, squash is a function to implement the former eq. )</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PrimaryCap</span><span class="params">(inputs, dim_vector, n_channels, kernel_size, strides, padding)</span>:</span></span><br><span class="line">    outputs = []</span><br><span class="line">    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size,</span><br><span class="line">                           strides=strides, padding=padding)(inputs)</span><br><span class="line">    outputs = layers.Reshape(target_shape=[<span class="number">-1</span>, dim_vector])(output)</span><br><span class="line">    <span class="keyword">return</span> layers.Lambda(squash)(outputs)</span><br></pre></td></tr></table></figure></li><li><p>In total PrimaryCapsules has [32; 6; 6] capsule outputs (each output is an 8D vector) . (shape is [32*6*6, 8]). The third layer is Digit layer. Between these two layers, the author use the routing algorithm as bellow.</p><p><img src="/img/routing.PNG" alt="routing"></p></li><li><p>Margin loss.</p><p><img src="/img/margin_loss.PNG" alt="margin losss"></p></li><li><p>Reconstruction as a regularization method.</p><p><img src="/img/reg.PNG" alt="regularization"></p><p>Fig. 3:</p><p> <img src="/img/reconstruction.PNG" alt="reconstruction"></p></li></ol></li><li><p>Experiment Result</p><ol><li>error rate on MNIST</li></ol><p><img src="/img/capsnet_exp.PNG" alt="experiment"></p><ol start="2"><li><p>What the individual dimensions of a capsule represent.</p><blockquote><p>Since we are passing the encoding of only one digit and zeroing out other digits, the dimensions of a digit capsule should learn to span the space of variations in the way digits of that class are instantiated. These variations include stroke thickness, skew and width. They also include digit-specific variations such as the length of the tail of a 2. We can see what the individual dimensions represent by making use of the decoder network. After computing the activity vector for the correct digit capsule, we can feed a perturbed version of this activity vector to the decoder network and see how the perturbation affects the reconstruction. Examples of these perturbations are shown in Fig. 4. We found that one dimension (out of 16) of the capsule almost always represents the width of the digit. While some dimensions represent combinations of global variations, there are other dimensions that represent variation in a localized part of the digit. For example, different dimensions are used for the length of the ascender of a 6 and the size of the loop </p></blockquote><p>Fig 4:</p><p><img src="/img/digit_d.PNG" alt="What the individual dimensions of a capsule represent"></p></li><li><p>Robustness to Affine Transformations </p><blockquote><p>To test the robustness of CapsNet to affine transformations we trained a CapsNet and a traditional convolutional network (with MaxPooling and DropOut) on a padded and translated MNIST training set, in which each example is an MNIST digit placed randomly on a black background of 40 × 40 pixels. We then tested this network on the affNIST4 data set, in which each example is an MNIST digit with a random small affine transformation.</p></blockquote><p>| Net\Accuracy | Expanded MNIST Test Set | Affined Test Set |<br>| :———-: | :———————: | :————–: |<br>|   CapsNet    |         99.23%          |        79        |<br>|     CNN      |          99.22          |        66        |</p></li><li><p>Segmenting highly overlapping digits</p><p><img src="/img/overlap.PNG" alt="overlap"></p><blockquote><p>Figure 5: Sample reconstructions of a CapsNet with 3 routing iterations on MultiMNIST test dataset. <strong>The two reconstructed digits are overlayed in green and red as the lower image</strong>. The upper image shows the input image. L:(l1; l2) represents the label for the two digits in the image and R:(r1; r2) represents the two digits used for reconstruction. The two right most columns show two examples with wrong classification reconstructed from the label and from the prediction (P). In (2; 8) example the model confuses 8 with a 7 and in (4; 9) it confuses 9 with 0. The other columns have correct classifications and show that model accounts for all the pixels while being able to assign one pixel to two digits in extremely difficult scenarios (column 1 - 4). Note that in dataset generation the pixel values are clipped at 1. The two columns with <strong>the (*) mark</strong> show reconstructions from a digit that is neither the label nor the prediction. </p></blockquote></li></ol></li></ol><h3 id="2-3-Drawbacks"><a href="#2-3-Drawbacks" class="headerlink" title="2.3 Drawbacks"></a>2.3 Drawbacks</h3><blockquote><p>One drawback of Capsules which it shares with generative models is that it likes to account for<br>everything in the image so it does better when it can model the clutter than when it just uses an<br>additional “orphan” category in the dynamic routing. In CIFAR-10, the backgrounds are much too<br>varied to model in a reasonable sized net which helps to account for the poorer performance. </p></blockquote><h2 id="3-Matrix-capsules-with-EM-routing-ICLR-2018-under-review"><a href="#3-Matrix-capsules-with-EM-routing-ICLR-2018-under-review" class="headerlink" title="3 Matrix capsules with EM routing  (ICLR 2018, under review)"></a>3 Matrix capsules with EM routing  (ICLR 2018, under review)</h2><h3 id="3-1-Introduction"><a href="#3-1-Introduction" class="headerlink" title="3.1 Introduction"></a>3.1 Introduction</h3><p>The system in previous paper has several deficiencies  as arguing in this paper:</p><ol><li>It uses the length of the pose vector to represent the probability that the entity represented by<br>a capsule is present. To keep the length less than 1 requires an unprincipled non-linearity<br>that prevents there from being any sensible objective function that is minimized by the<br>iterative routing procedure. </li><li>It uses the cosine of the angle between two pose vectors to measure their agreement. Unlike<br>the log variance of a Gaussian cluster, the cosine is not good at distinguishing between quite<br>good agreement and very good agreement </li><li>It uses a vector of length n rather than a matrix with n elements to represent a pose, so its<br>transformation matrices have n2 parameters rather than just n. </li></ol><h3 id="3-2-Model-detail"><a href="#3-2-Model-detail" class="headerlink" title="3.2 Model detail"></a>3.2 Model detail</h3><ol><li><p>Model Graph</p><p><img src="/img/em_capsnet.PNG" alt="model"></p></li><li><p>Gaussian mixture</p><p><img src="/img/EM_old_faithful.gif" alt="gaussian mixture"></p></li><li><p>EM routing</p><p><img src="/img/em_routing.PNG" alt="em routing"></p></li><li><p>Experiment Result: ADVERSARIAL ROBUSTNESS </p><p><img src="/img/ad_re.PNG" alt="adversarial"></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Transforming-Auto-encoder-ICANN-2011&quot;&gt;&lt;a href=&quot;#1-Transforming-Auto-encoder-ICANN-2011&quot; class=&quot;headerlink&quot; title=&quot;1 Transforming Auto-encoder(ICANN 2011 )&quot;&gt;&lt;/a&gt;1 Transforming Auto-encoder(ICANN 2011 )&lt;/h2&gt;&lt;h3 id=&quot;1-1-Introduction&quot;&gt;&lt;a href=&quot;#1-1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1.1 Introduction&quot;&gt;&lt;/a&gt;1.1 Introduction&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;Artificial neural networks should use local “capsules” that perform some quite complicated internal computations on their inputs and then encapsulate the results of these computations into a small vector of highly informative outputs. Each capsule learns to recognize an implicitly defined visual entity over a limited domain of viewing conditions and deformations and it outputs both the probability that the entity is present within its limited domain and a set of “instantiation parameters” that may include the precise pose, lighting and deformation of the visual entity relative to an implicitly defined canonical version of that entity. When the capsule is working properly, the probability of the visual entity being present is locally invariant – it does not change as the entity moves over the manifold of possible appearances within the limited domain covered by the capsule. The instantiation parameters, however, are “equivariant” – as the viewing conditions change and the entity moves over the appearance manifold, the instantiation parameters change by a corresponding amount because they are representing the intrinsic coordinates of the entity on the appearance manifold. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Capsule Net" scheme="https://n4a.github.io/tags/Capsule-Net/"/>
    
      <category term="CNN" scheme="https://n4a.github.io/tags/CNN/"/>
    
      <category term="Deep Learning" scheme="https://n4a.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>李航_统计学习方法</title>
    <link href="https://n4a.github.io/2017/10/21/%E6%9D%8E%E8%88%AA-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    <id>https://n4a.github.io/2017/10/21/李航-统计学习方法/</id>
    <published>2017-10-21T08:21:54.000Z</published>
    <updated>2018-07-17T12:11:32.996Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-统计学习概述"><a href="#1-统计学习概述" class="headerlink" title="1  统计学习概述"></a>1  统计学习概述</h3><p>​    统计学习三要素：模型、策略、算法。</p><h4 id="1-1-模型"><a href="#1-1-模型" class="headerlink" title="1.1 模型"></a>1.1 模型</h4><p>​    模型就是所要学习的条件概率分布或者决策函数</p><h4 id="1-2-策略"><a href="#1-2-策略" class="headerlink" title="1.2 策略"></a>1.2 策略</h4><p>​    策略即是决定用什么样的准则学习或选择最优的模型。 </p><ol><li><p>损失函数（loss function）</p><p><img src="/img/lihang/loss%20function.PNG" alt="loss function"></p></li><li><p>经验风险最小化和结构风险最小化</p><ol><li>empirical risk minimization，ERM：其理论依据是大数定理。但是通常情况下训练数据较少并不满足大数定理的要求，容易发生过拟合现象。</li><li>structural risk minimization，SRM：为了防止过拟合现象，SRM增加正则化项，对模型的复杂度进行约束，要求模型复杂度较小。</li></ol></li></ol><h4 id="1-3-算法"><a href="#1-3-算法" class="headerlink" title="1.3 算法"></a>1.3 算法</h4><p>​    算法是指学习模型的具体算法，例如BP算法、EM算法等。</p><h4 id="1-4-模型评估与模型选择"><a href="#1-4-模型评估与模型选择" class="headerlink" title="1.4 模型评估与模型选择"></a>1.4 模型评估与模型选择</h4><p>​    训练误差、测试误差、交叉验证。</p><p>​    生成模型、判别模型。</p><a id="more"></a><h3 id="2-感知机模型"><a href="#2-感知机模型" class="headerlink" title="2  感知机模型"></a>2  感知机模型</h3><h4 id="2-1-模型"><a href="#2-1-模型" class="headerlink" title="2.1 模型"></a>2.1 模型</h4><p>$$<br>f(x) = sign(wx + b) \tag{1.1}<br>$$</p><p>$$<br>sign(x) = \begin{cases}<br>        1,  &amp; \text{if x &gt;= 0} \<br>        -1, &amp; \text{if x &lt; 0}<br>        \end{cases} \tag{1.2}<br>$$</p><p>​    其中所要训练的参数为w和b，感知机模型是一种简单的线性分类模型，属于判别模型。</p><h4 id="2-2-策略"><a href="#2-2-策略" class="headerlink" title="2.2 策略"></a>2.2 策略</h4><p>定义 loss function：</p><p><img src="/img/lihang/perceptron.PNG" alt="感知机模型亏损函数"></p><h4 id="2-3-算法"><a href="#2-3-算法" class="headerlink" title="2.3 算法"></a>2.3 算法</h4><p>SGD算法：</p><p><img src="/img/lihang/perceptron%20sgd.PNG" alt="随机梯度下降算法"></p><p>3  KNN模型</p><h4 id="3-1-模型"><a href="#3-1-模型" class="headerlink" title="3.1 模型"></a>3.1 模型</h4><p><img src="/img/lihang/knn.PNG" alt="KNN模型"></p><h4 id="3-2-策略"><a href="#3-2-策略" class="headerlink" title="3.2 策略"></a>3.2 策略</h4><p>KNN模型是一个只需正向统计的过程，没有待训练参数，也不需要定义 loss function 。但是在统计前要决定策略三要素：距离度量方法、k值选择和分类决策方法。</p><ol><li>距离度量方法： 欧式距离、$L_p$距离、曼哈顿距离等。</li><li>k值选择：k值越小对临近数据点越敏感，模型越复杂，越容易发生过拟合；k值越大，模型越简单，不易发生过拟合，但是模型能力若，预测能力差。</li><li>分类决策方法：多数表决，平均值方法等。</li></ol><h4 id="3-3-算法"><a href="#3-3-算法" class="headerlink" title="3.3 算法"></a>3.3 算法</h4><p>最简单的算法就是线性搜索所有数据集，找出K个最近邻。其搜索复杂度为O(n)</p><p>优化的算法如<strong>kd树算法</strong>，搜索复杂度为 O(log n)</p><ol><li><p>构造kd树</p><p><img src="/img/lihang/kdt1.PNG" alt="构造kd树"></p><p><img src="/img/lihang/kdt2.PNG" alt="构造kd树"></p></li><li><p>利用kd树搜索最近邻</p><p><img src="/img/lihang/kdt3.png" alt="kd树搜索"></p><p><img src="/img/lihang/kdt4.png" alt="利用kd树搜索"></p></li></ol><h3 id="4-朴素贝叶斯方法"><a href="#4-朴素贝叶斯方法" class="headerlink" title="4 朴素贝叶斯方法"></a>4 朴素贝叶斯方法</h3><h4 id="4-1-模型"><a href="#4-1-模型" class="headerlink" title="4.1 模型"></a>4.1 模型</h4><p>朴素贝叶斯法通过训练数据来学习联合概率分布P(X,Y)。根据$P(X,Y) = P(X|Y)P(Y)$，所以如下图所示，我们可以通过学习Y的先验概率分布和X的条件概率分布来确定X与Y的联合分布。</p><p><img src="/img/lihang/b1.PNG" alt="Bayesian"></p><p>之后根据前面所说的大数定理，训练数据满足这种分布，之后的新数据也当满足这种分布。至于训练数据不足的问题，也可套用结构风险最小的理论。</p><p>但是求解P(X|Y)的发杂度非常高，设X为n维向量，$x_j$有$S_j$个不同的取值，Y有K个不同的取值。那么统计P(X|Y)的复杂度为$K\Pi_{j=1}^{n}Sj$.</p><p>为降低复杂度，朴素贝叶斯法做了X的每个维度相互独立的假设，那么条件概率分布变为：<br>$$<br>P(X=x|Y=y) = P(X^1=x^1,…,X^n = x^n | Y=c_k)  \<br>= \Pi_{j=1}^nP(X^j = x^j|Y=c_k) \<br>k = 1,2,…,K \tag{4.3}<br>$$<br>这样使得模型变得简单，可计算。但是其效果就要差一点。</p><h4 id="4-2-策略"><a href="#4-2-策略" class="headerlink" title="4.2 策略"></a>4.2 策略</h4><p>朴素贝叶斯的策略被称为后验概率最大化策略，后面会说明这一策略和经验风险最小化策略是等价的。</p><ol><li><p>后验概率最大化策略</p><p><img src="/img/lihang/posterior.PNG" alt="posterior"></p><p>​</p></li><li><p>后验概率最大等价于经验风险最小</p><p>为了证明这个问题，首先定义0-1损失函数如下：<br>$$<br>L(Y,F(X)) = \begin{cases}<br>1, &amp; Y \neq f(X) \<br>-1, &amp; Y = f(X)<br>\end{cases}<br>$$<br>式中f(X)式分类决策函数. 这时，期望风险函数为<br>$$<br>R_{exp} = E[L(Y,f(X))]<br>$$<br>然后利用条件期望全期望公式得：<br>$$<br>R_{exp}(f) = E_x\sum_{k=1}^K[L(c_k, f(X))]P(c_k|X)<br>$$<br>因为P(X=x)是确定的，所以只需对X=x的情况下逐个取最小化，如下：<br>$$<br>\begin{align}<br>f(x) &amp; = arg {min}<em>{y \in Y}\sum</em>{k=1}^KL(c_k,y)P(c_kX=x) \\<br> &amp; = arg {min}<em>{y \in Y} \sum</em>{k=1}^KP(y \neq c_k|X=x) \\<br> &amp; = arg{min}(1-P(y=c_k|X=x)) \\<br> &amp; = arg {max}_{y \in Y}P(y=c_k|X=x)<br>\end{align}<br>$$<br>这样经验风险最小化就和前面的后验概率最大化策略目标相同了</p></li></ol><h4 id="4-3-算法"><a href="#4-3-算法" class="headerlink" title="4.3 算法"></a>4.3 算法</h4><p><img src="/img/lihang/bayesian%20algorithm.PNG" alt="bayes algorithm"></p><h3 id="5-决策树模型"><a href="#5-决策树模型" class="headerlink" title="5 决策树模型"></a>5 决策树模型</h3><h4 id="5-1-模型"><a href="#5-1-模型" class="headerlink" title="5.1 模型"></a>5.1 模型</h4><p><img src="/img/lihang/decision_tree_model.PNG" alt="decision_tree"></p><h4 id="5-2-策略"><a href="#5-2-策略" class="headerlink" title="5.2 策略"></a>5.2 策略</h4><p>决策树的学习策略是损失函数最小的策略，但是这一策略在学习算法中不会明显的体现出来。具体的学习算法只会要求对于训练数据，分类尽可能正确。这也就蕴含了损失函数最小的思想。</p><p>即是损失函数已经最小了，决策树学习还要求树的结构最优，即树的层数要尽量少。</p><h4 id="5-3-算法"><a href="#5-3-算法" class="headerlink" title="5.3 算法"></a>5.3 算法</h4><p>从所有的二叉树中找出结构最优的的树是NP难度的问题。所以具体的算法都是启发式的算法，从根节点开始先找到分类最优的分类特征，然后一次递归地执行下去。</p><p>常用的算法有ID3，C4.5 与 CART。这些算法基本都基于信息熵和信息增益的理论。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-统计学习概述&quot;&gt;&lt;a href=&quot;#1-统计学习概述&quot; class=&quot;headerlink&quot; title=&quot;1  统计学习概述&quot;&gt;&lt;/a&gt;1  统计学习概述&lt;/h3&gt;&lt;p&gt;​    统计学习三要素：模型、策略、算法。&lt;/p&gt;
&lt;h4 id=&quot;1-1-模型&quot;&gt;&lt;a href=&quot;#1-1-模型&quot; class=&quot;headerlink&quot; title=&quot;1.1 模型&quot;&gt;&lt;/a&gt;1.1 模型&lt;/h4&gt;&lt;p&gt;​    模型就是所要学习的条件概率分布或者决策函数&lt;/p&gt;
&lt;h4 id=&quot;1-2-策略&quot;&gt;&lt;a href=&quot;#1-2-策略&quot; class=&quot;headerlink&quot; title=&quot;1.2 策略&quot;&gt;&lt;/a&gt;1.2 策略&lt;/h4&gt;&lt;p&gt;​    策略即是决定用什么样的准则学习或选择最优的模型。 &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;损失函数（loss function）&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/lihang/loss%20function.PNG&quot; alt=&quot;loss function&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;经验风险最小化和结构风险最小化&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;empirical risk minimization，ERM：其理论依据是大数定理。但是通常情况下训练数据较少并不满足大数定理的要求，容易发生过拟合现象。&lt;/li&gt;
&lt;li&gt;structural risk minimization，SRM：为了防止过拟合现象，SRM增加正则化项，对模型的复杂度进行约束，要求模型复杂度较小。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&quot;1-3-算法&quot;&gt;&lt;a href=&quot;#1-3-算法&quot; class=&quot;headerlink&quot; title=&quot;1.3 算法&quot;&gt;&lt;/a&gt;1.3 算法&lt;/h4&gt;&lt;p&gt;​    算法是指学习模型的具体算法，例如BP算法、EM算法等。&lt;/p&gt;
&lt;h4 id=&quot;1-4-模型评估与模型选择&quot;&gt;&lt;a href=&quot;#1-4-模型评估与模型选择&quot; class=&quot;headerlink&quot; title=&quot;1.4 模型评估与模型选择&quot;&gt;&lt;/a&gt;1.4 模型评估与模型选择&lt;/h4&gt;&lt;p&gt;​    训练误差、测试误差、交叉验证。&lt;/p&gt;
&lt;p&gt;​    生成模型、判别模型。&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学习" scheme="https://n4a.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
