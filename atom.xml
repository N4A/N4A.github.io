<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>N4A Space</title>
  
  <subtitle>To understand and be understood</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://n4a.github.io/"/>
  <updated>2019-09-10T07:57:53.679Z</updated>
  <id>https://n4a.github.io/</id>
  
  <author>
    <name>N4A</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Paper_Notes_About_Recommendation_in_AAAI19</title>
    <link href="https://n4a.github.io/2019/09/10/Paper-Notes-About-Recommendation-in-AAAI19/"/>
    <id>https://n4a.github.io/2019/09/10/Paper-Notes-About-Recommendation-in-AAAI19/</id>
    <published>2019-09-10T07:53:58.000Z</published>
    <updated>2019-09-10T07:57:53.679Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><a id="more"></a><h1 id="Session-based-Recommendation-with-Graph-Neural-Networks"><a href="#Session-based-Recommendation-with-Graph-Neural-Networks" class="headerlink" title="Session-based Recommendation with Graph Neural Networks"></a>Session-based Recommendation with Graph Neural Networks</h1><p>Problem: Session Recommendation</p><p>Gap: Transitions among distant items are often overlooked by previous methods. </p><p>Method: Use Graph to model item relation in sessions and utilize Graph Neural Networks to capturing transitions of items and generate accurate item embedding vectors.</p><p><img src="./img/aaai19/session-graph.PNG" alt="Session graph"></p><h1 id="CAMO-A-Collaborative-Ranking-Method-for-Content-Based-Recommendation"><a href="#CAMO-A-Collaborative-Ranking-Method-for-Content-Based-Recommendation" class="headerlink" title="CAMO: A Collaborative Ranking Method for Content Based Recommendation"></a>CAMO: A Collaborative Ranking Method for Content Based Recommendation</h1><p>Problem: A traditional line that uses auxiliary information to enrich item embedding based on MF. The general score function for user $i$, item $j$ and auxiliary info $doc_j$ is as followings:<br>$$<br>f(i,j,doc_j) = p_i^T(q_j + g(doc_j))<br>$$<br>where $g(.)$ is the encoding function.</p><p>Gap: Some methods ignore the order of words, while others fail to identify the high leveled topic info.</p><p>Method: An encoding function based on GRU(capture order) and multi-head attention mechanism(model topic info).</p><p><img src="./img/aaai19/camo.PNG" alt="CAMO"></p><p>A Nice Attention Introduction: <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html</a></p><h1 id="Explainable-Reasoning-over-Knowledge-Graphs-for-Recommendation"><a href="#Explainable-Reasoning-over-Knowledge-Graphs-for-Recommendation" class="headerlink" title="Explainable Reasoning over Knowledge Graphs for Recommendation"></a>Explainable Reasoning over Knowledge Graphs for Recommendation</h1><p>Motivation: The connectivity information in KG can help to endow recommender systems the ability of reasoning and explainability. </p><p>Gap: Some traditional methods(use mata-paths) need domain knowledge to predefine meta-paths, while others(embedding based, like using TransE) are less explainable.</p><p>Method:</p><ol><li><p>Model user-item interaction $(u, i)$ as KG triplet relation $(u, interact, i)$.</p></li><li><p>For each $(u,i)$, select all paths from user node $u$ to item node $i$, denoted as $P(u,i) = {p_1, p_2, …, p_K}$</p></li><li><p>The score function from user $u$ to item $i$ is model as:<br>$$<br>\hat{y}<em>{ui} = f</em>{\Theta}(u,i|P(u,i))<br>$$</p></li><li><p>Use LSTM to model the preference of each path and use a weighted pooling layer(not attention) to calculate the final preference and estimate the importance of each path.</p><p><img src="./img/aaai19/kg-lstm.PNG" alt="kg-lstm"></p></li></ol><p>Weighted Pooling Layer:</p><p>For each preference $s_k$ outputted from the LSTM layer, the weighted pooling layer is defined as follows:<br>$$<br>g(s_1, s_2, …, s_K) = log[\sum_{k=1}^K \exp (\frac{s_K}{\gamma})]<br>$$<br>where $\gamma$ is a hyper-parameter to control each exponential weight and the path importance is modeled as the gradient on each path preference:<br>$$<br>\frac{\partial g}{\partial s_k} = \frac{\exp (s_k/ \gamma)}{\gamma \sum_{k^{‘}} \exp (s_{k^{‘}}/\gamma)}<br>$$<br>Finally,  the score function is<br>$$<br>y_{ui} = \sigma(g(s_1,s_2,…,s_K))<br>$$</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h1&gt;
    
    </summary>
    
    
      <category term="Recommender Systems" scheme="https://n4a.github.io/tags/Recommender-Systems/"/>
    
      <category term="Knowledge graph" scheme="https://n4a.github.io/tags/Knowledge-graph/"/>
    
      <category term="Attention Mechanism" scheme="https://n4a.github.io/tags/Attention-Mechanism/"/>
    
  </entry>
  
  <entry>
    <title>Paper_Notes_About_Recommendation_in_SIGIR18</title>
    <link href="https://n4a.github.io/2019/05/14/Paper-Notes-About-Recommendation-in-SIGIR18/"/>
    <id>https://n4a.github.io/2019/05/14/Paper-Notes-About-Recommendation-in-SIGIR18/</id>
    <published>2019-05-14T08:08:15.000Z</published>
    <updated>2019-09-10T08:14:01.035Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>•Online recommendation(1)</p><p>•Recommendation with Social Networks(2+1)</p><p>•Group representation, community detection, sequence-aware Rec</p><p>•Recommendation with Knowledge Base(1)</p><p>•Improve traditional methods(3)</p><p>•APR, CMN, Bandit problem</p><p>•Some specific tasks(5)</p><p>•Recommend email, mention, citation, Wikipedia article section</p><p>•Conversational recommender system</p><p>•User modeling: Geo-social based(1)</p><a id="more"></a><h1 id="PPT-1"><a href="#PPT-1" class="headerlink" title="PPT 1"></a>PPT 1</h1><p>&lt;% pdf /pdf/SIGIR18_Papers_about_Recommendation.pdf %&gt;</p><h1 id="PPT-2"><a href="#PPT-2" class="headerlink" title="PPT 2"></a>PPT 2</h1><p>&lt;% pdf /pdf/SIGIR18_Papers_about_Recommendation2.pdf %&gt;</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h1&gt;&lt;p&gt;•Online recommendation(1)&lt;/p&gt;
&lt;p&gt;•Recommendation with Social Networks(2+1)&lt;/p&gt;
&lt;p&gt;•Group representation, community detection, sequence-aware Rec&lt;/p&gt;
&lt;p&gt;•Recommendation with Knowledge Base(1)&lt;/p&gt;
&lt;p&gt;•Improve traditional methods(3)&lt;/p&gt;
&lt;p&gt;•APR, CMN, Bandit problem&lt;/p&gt;
&lt;p&gt;•Some specific tasks(5)&lt;/p&gt;
&lt;p&gt;•Recommend email, mention, citation, Wikipedia article section&lt;/p&gt;
&lt;p&gt;•Conversational recommender system&lt;/p&gt;
&lt;p&gt;•User modeling: Geo-social based(1)&lt;/p&gt;
    
    </summary>
    
    
      <category term="Recommender System" scheme="https://n4a.github.io/tags/Recommender-System/"/>
    
      <category term="Social Netowrk" scheme="https://n4a.github.io/tags/Social-Netowrk/"/>
    
      <category term="Attention" scheme="https://n4a.github.io/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>Paper summary_work based on meta learning</title>
    <link href="https://n4a.github.io/2018/12/30/Paper-summary-work-based-on-meta-learning/"/>
    <id>https://n4a.github.io/2018/12/30/Paper-summary-work-based-on-meta-learning/</id>
    <published>2018-12-30T13:55:17.000Z</published>
    <updated>2018-12-30T14:06:07.178Z</updated>
    
    <content type="html"><![CDATA[<ol><li>Meta Learning Framework</li><li>ProtoNet: 一个和 Matching Net 十分相似的处理 few shot learning 任务的<br>模型</li><li>MAML: MAML 模型通过 Meta learning 的方法，尝试为所有的子任务学<br>习一个初始参数，使得各个子任务能够在该参数基础上快速收敛。</li><li>Meta learning for unsupervised learning: 一种利用 meta learning 框架来使<br>用无标签数据的方法。</li><li>Meta learning for item cold-start recommendation: Meta learning 在物品冷<br>启动任务中的一个应用。 </li></ol><a id="more"></a><div class="row">    <embed src="/pdf/Meta_learning_Paper_summary.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;Meta Learning Framework&lt;/li&gt;
&lt;li&gt;ProtoNet: 一个和 Matching Net 十分相似的处理 few shot learning 任务的&lt;br&gt;模型&lt;/li&gt;
&lt;li&gt;MAML: MAML 模型通过 Meta learning 的方法，尝试为所有的子任务学&lt;br&gt;习一个初始参数，使得各个子任务能够在该参数基础上快速收敛。&lt;/li&gt;
&lt;li&gt;Meta learning for unsupervised learning: 一种利用 meta learning 框架来使&lt;br&gt;用无标签数据的方法。&lt;/li&gt;
&lt;li&gt;Meta learning for item cold-start recommendation: Meta learning 在物品冷&lt;br&gt;启动任务中的一个应用。 &lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="Machine learning" scheme="https://n4a.github.io/tags/Machine-learning/"/>
    
      <category term="Meta learning" scheme="https://n4a.github.io/tags/Meta-learning/"/>
    
      <category term="ML" scheme="https://n4a.github.io/tags/ML/"/>
    
      <category term="few-shot learning" scheme="https://n4a.github.io/tags/few-shot-learning/"/>
    
  </entry>
  
  <entry>
    <title>BP derivation for MLP and CNN</title>
    <link href="https://n4a.github.io/2018/12/14/BP-derivation-for-MLP-and-CNN/"/>
    <id>https://n4a.github.io/2018/12/14/BP-derivation-for-MLP-and-CNN/</id>
    <published>2018-12-14T07:26:08.000Z</published>
    <updated>2018-12-14T07:29:10.385Z</updated>
    
    <content type="html"><![CDATA[<p>\section{Task description}<br>Please derive a backpropagation process</p><p>(1)   for the multi-layer neural network with one hidden layer, where data are in a m-dimensional feature space with n classes. Loss functions can use L2 distance or cross entropy.</p><p>(2)   for the LeNet-5 CNN.  </p><a id="more"></a><div class="row">    <embed src="/pdf/bp-derivation-mlp.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;\section{Task description}&lt;br&gt;Please derive a backpropagation process&lt;/p&gt;
&lt;p&gt;(1)   for the multi-layer neural network with one hidden layer, where data are in a m-dimensional feature space with n classes. Loss functions can use L2 distance or cross entropy.&lt;/p&gt;
&lt;p&gt;(2)   for the LeNet-5 CNN.  &lt;/p&gt;
    
    </summary>
    
    
      <category term="BP" scheme="https://n4a.github.io/tags/BP/"/>
    
      <category term="MLP" scheme="https://n4a.github.io/tags/MLP/"/>
    
      <category term="CNN" scheme="https://n4a.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>ml basic knowledge practice_regression problem</title>
    <link href="https://n4a.github.io/2018/10/27/ml-basic-knowledge-practice-regression-problem/"/>
    <id>https://n4a.github.io/2018/10/27/ml-basic-knowledge-practice-regression-problem/</id>
    <published>2018-10-27T12:36:48.000Z</published>
    <updated>2018-10-27T12:43:42.586Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Task-Description"><a href="#1-Task-Description" class="headerlink" title="1 Task Description"></a>1 Task Description</h2><p>Use the following dataset to do house price predicting work.</p><ol><li><a href="https://www.kaggle.com/vikrishnan/boston-house-prices" target="_blank" rel="noopener">https://www.kaggle.com/vikrishnan/boston-house-prices</a></li><li><a href="https://github.com/datasets/house-prices-uk" target="_blank" rel="noopener">https://github.com/datasets/house-prices-uk</a></li></ol><p>Details: design a model to do house price predicting work. Linear Regression models including basic linear model based on polynomial, Ridge Regression, Lasso Regression and regression model based Decision Tree must be implemented. Regression models based on SVM and Deep Learning is optional.</p><a id="more"></a><div class="row">    <embed src="/pdf/regression.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Task-Description&quot;&gt;&lt;a href=&quot;#1-Task-Description&quot; class=&quot;headerlink&quot; title=&quot;1 Task Description&quot;&gt;&lt;/a&gt;1 Task Description&lt;/h2&gt;&lt;p&gt;Use the following dataset to do house price predicting work.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/vikrishnan/boston-house-prices&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.kaggle.com/vikrishnan/boston-house-prices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/datasets/house-prices-uk&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/datasets/house-prices-uk&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Details: design a model to do house price predicting work. Linear Regression models including basic linear model based on polynomial, Ridge Regression, Lasso Regression and regression model based Decision Tree must be implemented. Regression models based on SVM and Deep Learning is optional.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Mechine Learning" scheme="https://n4a.github.io/tags/Mechine-Learning/"/>
    
      <category term="Regression Probelm" scheme="https://n4a.github.io/tags/Regression-Probelm/"/>
    
  </entry>
  
  <entry>
    <title>To the Moon_拐骗少女的小混混</title>
    <link href="https://n4a.github.io/2018/09/27/%E6%8B%90%E9%AA%97%E5%B0%91%E5%A5%B3%E7%9A%84%E5%B0%8F%E6%B7%B7%E6%B7%B7/"/>
    <id>https://n4a.github.io/2018/09/27/拐骗少女的小混混/</id>
    <published>2018-09-27T01:36:10.000Z</published>
    <updated>2018-09-28T16:58:59.021Z</updated>
    
    <content type="html"><![CDATA[<p>​    这篇文章是在之前的一篇文章的基础上续写的。没有读过的读者可以看一下这篇之前的介绍文。<a href="https://www.bilibili.com/read/cv1128664" target="_blank" rel="noopener">To the Moon 游戏设定介绍</a></p><p>​    To the Moon 这款游戏的剧情设计非常精彩。在游戏中，我们会一点点回溯主人公 John 的记忆。随着对故事的一点一点地深入了解，我们会困惑，会误解，会略带嘲弄，也会略有所感。可是当我们最后看清故事始末，只剩下感动与泪水。</p><p>​    如果想要完整的体验游戏所展现的故事，可以观看下面的视频。</p><iframe src="//player.bilibili.com/player.html?aid=31555731&cid=55173221&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="500"> </iframe><a id="more"></a><iframe src="//player.bilibili.com/player.html?aid=31555731&cid=55173303&page=2" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="500"> </iframe><iframe src="//player.bilibili.com/player.html?aid=31555731&cid=55172705&page=3" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="500"> </iframe><iframe src="//player.bilibili.com/player.html?aid=31555731&cid=55173120&page=4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="500"> </iframe><p>​    在回溯 John 记忆地过程中，我们发现女主 River 是个奇怪的人，她是一种特殊疾病患者，身患这种疾病的人既希望交流又不回避交流或者有交流障碍，可以理解为交流障碍患者。John 与其交往总是产生各种各样难以交流的问题。但是难能可贵，John 总是包容 River 奇怪的想法，并且想尽一切办法想要治好 River 的病，让她能够过上正常人的生活。这时候我们就会认为这个游戏讲了一个因爱而包容的故事，John 的形象非常饱满正面，值得学习。</p><p>​    但是，当我们来到男主中学时代的时候，就会惊讶的发现男主当年表白的动机并不单纯。Dr. Watts 看到这里之后，感概：“这么说这个家伙是个拐骗少女的小混混？”。</p><p><img src="http://i0.hdslb.com/bfs/article/941c7067c974c99290346fee00135bbf59ae4993.png" alt="img"></p><p>​    John 当初表白的动机和理由都很没有说服力，让他的朋友觉得这简直就是一段时间的骚扰罢了。John 认为 River 是一个特别的人，她总是一个人，那么跟她在一起一定是一件很酷的事。确实这个理由不怎么像是有责任心的人说的话，听上去 John 只是一时兴起，并且这样做可以让他觉得很酷。但是这时候，加上我们前面对 John 和 River 后来发展的了解。这个故事就变成了这样。虽然 John 的动机狗血，却在后来在交往过程中爱上了 River，并且毫不在乎 River 的病症给她带来的困扰。甚至许多年之后，John 认为自己不该对 River 有任何隐瞒，于是他就将自己当年表的动机告诉了 River。自此之后，River 开始以她一贯的特异的表达方式来表达她的感情。她剪了头发，并且开始疯狂地折纸兔子。这里我们机智的 Dr. Watts 又发表了个人感想：“ 然后她就发狂了，并开始做那些诡异的兔子，是吗？”。这确实像一个交流障碍症患者。</p><p><img src="http://i0.hdslb.com/bfs/article/2fa2d54e7f4aadca202379e860630644b6c3239c.png" alt="img"></p><p>​    现在，我们对 John 有了更深入的了解。John 的形象跌了一点，可也还不错。当他后来在交往过程中爱上了 River 之后，无怨无悔地为 River 付出。他对 River 的作为并不能理解，可面对 River 的坚持，他依然照办。至于 River，她的幸福此时也有了一点点不幸。</p><p>​    之后，我们再继续回溯 John 的记忆，发现 John 之前的记忆呈现黑暗和紊乱的状态。这种情况导致我们无法再继续探索。 Dr. Watts 和 Dr. Rosalence 只好基于现有的信息诱导修改 John 的记忆，帮他成去月球的愿望。如果对这个修改记忆的操作不理解可以参考这篇文章<a href="https://www.bilibili.com/read/cv1128664" target="_blank" rel="noopener">To the Moon 游戏设定介绍</a>。然而无论怎么诱导修改，John 之后的记忆演变都没有发生任何改变。最后，Dr. Rosalence 分析是因为我们还不了解 John 为什么想要到月球上去，这使得我们的诱导修改都没有对症下药。面对这种困境，Dr. Watts 和 Dr. Rosalence 想尽了办法终于回溯到了 John 的童年时期。</p><p>接下来就是游戏的高潮部分，所有的感动和泪水也都献给了这个部分。当我们回到 John 童年的时候，发现John 还有一个弟弟 Joey，这一点前面的故事也有很多伏笔。可不幸的是弟弟在院子里玩的时候，妈妈开车时没注意撞到了他。随着弟弟去世，妈妈精神有点失常，并且她给 John 服用了一些消除记忆的药物，好让他忘了这个悲惨的事情。这一切导致 John 的童年记忆难以到达。 在药物的作用下，John 忘记了许多东西，可那个重要的相遇和约定一直存在于 John 脑海的深层之处，只是受药物阻碍无法完整浮现出来。</p><p>随着记忆继续向前回溯，一次邂逅在我们面前展开。这是一个游乐园的场景，John 不想和妈妈、弟弟一起玩，就自己一个人在游乐园寻找有趣的地方，他找到了游乐园山上的一个平常没人去的角落。John在这里欣赏天上的星星。就在这时候，有一个小女孩也来到了这个地方。他们在非常惊讶，同时也很开心有一个人像自己一样来到了这个角落。小女孩的名字叫 River。也就是这一天，John 和 River 第一次相遇。John 是普通的 John，世界上有一大票人都叫 John；River 是特殊的 River，世界上没有几个人叫 River。普通的 John 和 特殊的 River 都喜欢这个角落，在这里他们一起看天上的星星，星星闪闪发光，星星就如同灯塔，向远方的灯塔传达交流的信息。无论是普通的 John 和 特殊的 River，他们一直以来只是以不同的形式保持孤独，但是，此时此刻他们都交到了朋友。</p><p><img src="http://i0.hdslb.com/bfs/article/603476bb36c2a11a7231c698a6e85316a9a6f5fc.png" alt="img"></p><p>​    他们发现了自定义的星座，一个兔子星座，而月亮就是这个兔子圆鼓鼓的肚子。</p><p><img src="http://i0.hdslb.com/bfs/article/f007c801ff21259e0d758da96f185d6850aeee17.png" alt="img"></p><p>​    在分别的时候，他们约定之后再见。“但是如果你忘记了或者走丢了呢？”，“那么我们一定会在月亮上相遇的，傻瓜！”</p><p><img src="http://i0.hdslb.com/bfs/article/330e4365f6d6d442905eee612cb189310cf417dc.png" alt="img"></p><p>​    发展至此，我们了解了故事的始末。一次简单的相遇，却是一生的缘分。再回首过往，所有的谜题都被解开。</p><p>​    John 之所以想去月球，是因为他们曾做了这样的约定，可惜 John 因为药物的影响，对这件事印象模糊了，可他记住了这个约定。“但是如果你忘记了或者走丢了呢？”，“那么我们一定会在月亮上相遇的，傻瓜！”。年老之后，River 因病先他而去。River不在身边之后，这个愿望自发地又浮现在脑海。</p><p>​    John 到中学之后，遇到了一个女孩，他爱上了这个女孩，却不知道为什么。然而他只是忘记了，就好像年老之时他想要去月球，却不知道为什么，现在他爱上了一个女孩，也不知道为什么。潜藏在记忆深处的东西自发地推动着他，让他爱上了这个女孩。“我不知道为什么爱上你，但是我一直有一种感觉：我爱你。” 看似狗血的表白理由，只是因为外人无法了解其内心的感受。</p><p>​    River 也不是若有若无的配角存在，她也不是稀里糊涂地接受了 John 的邀请，她一直在等，在等，等了许多年。感情之事，当局者清，旁观者迷。</p><p>​    John 之后一直深爱着 River，尝试所有办法来治好 River。可惜这个可怜的失忆的人不曾意识到自己的问题。“我倾尽一生想要治好你，却没有发现病人原来是自己”。</p><p>​    特殊的 River 更是一直深爱着 John，以她独特的方式。当 John 向她抛白中学时代的愧疚之后。River 剪了头发，因为他们第一次相见的时候，她是短发；她不断的折着纸兔子，因为那是他们的兔子星座；她宁愿放弃自己的治疗，也要在那个废弃的灯塔旁边盖起他们自己的房子，因为那灯塔就是他们相遇时的天上的星星。她所作的一切怪异的事情只是为了唤起 John 的记忆。</p><p>​    命运为幸福的人带来不幸，不幸的人却幸福地活着。</p><p>​    浅薄的言语永远无法表达游戏带来的感受，我推荐这款游戏，希望将这份感动送给你。<a href="https://www.bilibili.com/video/av31555731/?p=1" target="_blank" rel="noopener">To the Moon 游玩记录</a></p><p>​    游戏至此依然没有结束，之后的故事之后再说吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    这篇文章是在之前的一篇文章的基础上续写的。没有读过的读者可以看一下这篇之前的介绍文。&lt;a href=&quot;https://www.bilibili.com/read/cv1128664&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;To the Moon 游戏设定介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;​    To the Moon 这款游戏的剧情设计非常精彩。在游戏中，我们会一点点回溯主人公 John 的记忆。随着对故事的一点一点地深入了解，我们会困惑，会误解，会略带嘲弄，也会略有所感。可是当我们最后看清故事始末，只剩下感动与泪水。&lt;/p&gt;
&lt;p&gt;​    如果想要完整的体验游戏所展现的故事，可以观看下面的视频。&lt;/p&gt;
&lt;iframe src=&quot;//player.bilibili.com/player.html?aid=31555731&amp;cid=55173221&amp;page=1&quot; scrolling=&quot;no&quot; border=&quot;0&quot; frameborder=&quot;no&quot; framespacing=&quot;0&quot; allowfullscreen=&quot;true&quot; width=&quot;100%&quot; height=&quot;500&quot;&gt; &lt;/iframe&gt;
    
    </summary>
    
    
      <category term="个人" scheme="https://n4a.github.io/tags/%E4%B8%AA%E4%BA%BA/"/>
    
      <category term="生活" scheme="https://n4a.github.io/tags/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="To the Moon" scheme="https://n4a.github.io/tags/To-the-Moon/"/>
    
  </entry>
  
  <entry>
    <title>Meta Learning</title>
    <link href="https://n4a.github.io/2018/07/29/Meta-Learning/"/>
    <id>https://n4a.github.io/2018/07/29/Meta-Learning/</id>
    <published>2018-07-29T12:38:31.000Z</published>
    <updated>2018-10-27T12:49:40.229Z</updated>
    
    <content type="html"><![CDATA[<p>​    Meta Learning 最早源于上世纪八九十年代 [6], 最近成为研究的热点，这是一个很好的 可以用来解决 Learn to learn 问题的框架。 17 年 NIPS 有一个 Workshop on Meta Learning 。与迁移学习相比， Meta Learning 可以视为一个更泛化的概念。 </p><p>​    传统的机器学习方法为解决某一个特定的任务总是需要大量的训练数据，有一个很直 观的原因是因为传统的机器学习方法在训练一个模型时，总是从零开始学习。但是人类 的学习过程并不是这样，显然人的学习是一个连续的过程，当一个人想要解决某一个问题 时，他会使用之前跟这个任务相关的知识。以图像分类任务为例，传统的机器学习方法， 例如普通的 CNN 模型，或者 AlexNet， VGG， ResNet 这些模型都需要大量的训练数据。 为了解决数据依赖的问题，现在的一个研究热点就是“one shot learning” (few shot learning)[4]。许多解决这个问题的方法 [9, 8] 就是基于 Meta Learning。 </p><a id="more"></a><div class="row">    <embed src="/pdf/meta-learning.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    Meta Learning 最早源于上世纪八九十年代 [6], 最近成为研究的热点，这是一个很好的 可以用来解决 Learn to learn 问题的框架。 17 年 NIPS 有一个 Workshop on Meta Learning 。与迁移学习相比， Meta Learning 可以视为一个更泛化的概念。 &lt;/p&gt;
&lt;p&gt;​    传统的机器学习方法为解决某一个特定的任务总是需要大量的训练数据，有一个很直 观的原因是因为传统的机器学习方法在训练一个模型时，总是从零开始学习。但是人类 的学习过程并不是这样，显然人的学习是一个连续的过程，当一个人想要解决某一个问题 时，他会使用之前跟这个任务相关的知识。以图像分类任务为例，传统的机器学习方法， 例如普通的 CNN 模型，或者 AlexNet， VGG， ResNet 这些模型都需要大量的训练数据。 为了解决数据依赖的问题，现在的一个研究热点就是“one shot learning” (few shot learning)[4]。许多解决这个问题的方法 [9, 8] 就是基于 Meta Learning。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="Meta Learning" scheme="https://n4a.github.io/tags/Meta-Learning/"/>
    
      <category term="AI" scheme="https://n4a.github.io/tags/AI/"/>
    
      <category term="Machine Learning" scheme="https://n4a.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>ICML&#39;18 GAN 理论文章总结</title>
    <link href="https://n4a.github.io/2018/07/16/ICML-18-GAN-%E7%90%86%E8%AE%BA%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93/"/>
    <id>https://n4a.github.io/2018/07/16/ICML-18-GAN-理论文章总结/</id>
    <published>2018-07-16T15:17:07.000Z</published>
    <updated>2018-07-17T14:44:58.856Z</updated>
    
    <content type="html"><![CDATA[<p>这个部分有五篇文章，其中：</p><ol><li>两篇文是通过改变GAN的结构以解决GAN训练困难和模式消失（Mode collapse）的问题。</li><li>一篇文章从新的数学角度推导GAN的更新过程，该更新过程更一般化，原有的GAN参数更新过程可视为其某种条件下的特例。文中也简要说明了该更新过程是 stable 的。</li><li>一篇文章探究了GAN中生成器的 Jacobian 矩阵的奇异值分布和 GAN 性能的关系。这篇文章很有趣，它根据生成器的 Jacobian 矩阵定义了一个<strong>condition number</strong>，然后在训练过程中发现该值与常用的GAN评估方法 <strong>Inception Score</strong> 和 <strong>Frechet Inception Distance</strong> 的评估值十分相关，最后文中提出一种方法通过控制 <strong>condition number</strong> 来改进 GAN 的训练过程。</li><li>一篇文章提出一种新的方法计算WGAN中 Wasserstein distance，同时做了许多相关的理论推导。 这篇文章理论知识很多，我看起来很费劲，也很困惑。文章中虽然做了很多的工作，但是相比于WGAN没有太大的创新。</li></ol><a id="more"></a> <div class="row">    <embed src="/pdf/memo-icml18-gan.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这个部分有五篇文章，其中：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;两篇文是通过改变GAN的结构以解决GAN训练困难和模式消失（Mode collapse）的问题。&lt;/li&gt;
&lt;li&gt;一篇文章从新的数学角度推导GAN的更新过程，该更新过程更一般化，原有的GAN参数更新过程可视为其某种条件下的特例。文中也简要说明了该更新过程是 stable 的。&lt;/li&gt;
&lt;li&gt;一篇文章探究了GAN中生成器的 Jacobian 矩阵的奇异值分布和 GAN 性能的关系。这篇文章很有趣，它根据生成器的 Jacobian 矩阵定义了一个&lt;strong&gt;condition number&lt;/strong&gt;，然后在训练过程中发现该值与常用的GAN评估方法 &lt;strong&gt;Inception Score&lt;/strong&gt; 和 &lt;strong&gt;Frechet Inception Distance&lt;/strong&gt; 的评估值十分相关，最后文中提出一种方法通过控制 &lt;strong&gt;condition number&lt;/strong&gt; 来改进 GAN 的训练过程。&lt;/li&gt;
&lt;li&gt;一篇文章提出一种新的方法计算WGAN中 Wasserstein distance，同时做了许多相关的理论推导。 这篇文章理论知识很多，我看起来很费劲，也很困惑。文章中虽然做了很多的工作，但是相比于WGAN没有太大的创新。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="GAN" scheme="https://n4a.github.io/tags/GAN/"/>
    
      <category term="ICML" scheme="https://n4a.github.io/tags/ICML/"/>
    
  </entry>
  
  <entry>
    <title>GAN and it&#39;s applications on image translation</title>
    <link href="https://n4a.github.io/2018/03/30/GAN-and-it-s-applications/"/>
    <id>https://n4a.github.io/2018/03/30/GAN-and-it-s-applications/</id>
    <published>2018-03-30T08:12:14.000Z</published>
    <updated>2018-07-17T08:31:05.797Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-GAN"><a href="#1-GAN" class="headerlink" title="1 GAN"></a>1 GAN</h2><h3 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h3><p>To learn the generator’s distribution $p_g$ over data x, we define a prior on input noise variables $p_z(z)$, then represent a mapping to data space as $G(z; θ_g)$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $θ_g$. We also define a second multilayer perceptron $D(x; θ_d)$ that outputs a single scalar. $D(x)$ represents the probability that $x$ came from the data rather than $p_g$. We train $D$ to maximize the probability of assigning the correct label to both training examples and samples from $G$. We simultaneously train $G$ to minimize $log(1 - D(G(z)))$. In other words, $D$ and $G$ play the following two-player mini-max game with value function $V (G; D):$</p><p>$$min_G max_DV (D; G) = E_x∼p_{data(x)}[log D(x)] + E_{z∼p_z(z)}[log(1 - D(G(z)))]$$</p><a id="more"></a><h3 id="1-2-Theory-Analysis"><a href="#1-2-Theory-Analysis" class="headerlink" title="1.2 Theory Analysis"></a>1.2 Theory Analysis</h3><p><img src="/img/gan_and_applications/gan.PNG" alt="gan"></p><h2 id="2-Cycle-GAN-ICCV-2017"><a href="#2-Cycle-GAN-ICCV-2017" class="headerlink" title="2 Cycle GAN(ICCV 2017)"></a>2 Cycle GAN(ICCV 2017)</h2><h2 id="2-1-Task-Cross-Domain-Image-Translation"><a href="#2-1-Task-Cross-Domain-Image-Translation" class="headerlink" title="2.1 Task: Cross Domain Image Translation"></a>2.1 Task: Cross Domain Image Translation</h2><p>Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image.</p><p>In this paper, we present a method that can learn to do the same: capturing special characteristics of one image collection and figuring out how these characteristics could be translated into the other image collection, all in the absence of any paired training examples. </p><h3 id="2-2-Model-Cycle-consistent"><a href="#2-2-Model-Cycle-consistent" class="headerlink" title="2.2 Model: Cycle consistent"></a>2.2 Model: Cycle consistent</h3><p><img src="/img/gan_and_applications/cycleGAN.PNG" alt="Cycle GAN"></p><p>Loss:</p><p><img src="/img/gan_and_applications/cycleGAN-loss.png" alt="cycle gan loss"></p><h3 id="2-3-Experiment"><a href="#2-3-Experiment" class="headerlink" title="2.3 Experiment"></a>2.3 Experiment</h3><p><img src="/img/gan_and_applications/cycleGAN-experiments.PNG" alt="Experiments"></p><ol><li><p>Dataset: Cityscapes dataset , map and aerial photo on data scraped from Google Maps </p></li><li><p>Metrics: AMT perceptual studies, FCN score, Semantic segmentation metrics</p></li><li><p>Result:</p><p><img src="/img/gan_and_applications/cycleGAN-experiments2.PNG" alt="Experiments"></p><p><img src="/img/gan_and_applications/cycleGAN-experiments3.PNG" alt="Experiments"></p><p><img src="/img/gan_and_applications/cycleGAN-experiments4.PNG" alt="Experiments"></p></li></ol><h3 id="2-4-Limitations"><a href="#2-4-Limitations" class="headerlink" title="2.4 Limitations"></a>2.4 Limitations</h3><ol><li>On translation tasks that involve color and texture changes, like many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. </li><li>Some failure cases are caused by the distribution characteristics of the training datasets.</li><li>We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method.  </li></ol><p><img src="/img/gan_and_applications/cycleGAN-limit.PNG" alt="limitations"></p><h2 id="3-DIAT-Deep-Identity-aware-Transfer-of-Facial-Attributes"><a href="#3-DIAT-Deep-Identity-aware-Transfer-of-Facial-Attributes" class="headerlink" title="3 DIAT: Deep Identity-aware Transfer of Facial Attributes"></a>3 DIAT: Deep Identity-aware Transfer of Facial Attributes</h2><h3 id="3-1-Task-Identity-aware-Transfer-of-Facial-Attributes"><a href="#3-1-Task-Identity-aware-Transfer-of-Facial-Attributes" class="headerlink" title="3.1 Task: Identity-aware Transfer of Facial Attributes"></a>3.1 Task: Identity-aware Transfer of Facial Attributes</h3><p>Our DIAT and DIAT-A models can provide a unified solution for several representative facial attribute transfer tasks such as <strong>expression transfer</strong>, <strong>accessory removal</strong>, <strong>age progression</strong>, and <strong>gender</strong> transfer </p><h3 id="3-2-Model"><a href="#3-2-Model" class="headerlink" title="3.2 Model"></a>3.2 Model</h3><p>In this section, a two-stage scheme is developed to tackle the identity-aware attribute transfer task. </p><ol><li><p>Face transform network</p><p><img src="/img/gan_and_applications/diat-transform.PNG" alt="diat-transform"></p><p>Loss: </p><p><img src="/img/gan_and_applications/diat-transform-loss.png" alt="loss"></p></li><li><p>Face Enhancement Network</p><p><img src="/img/gan_and_applications/diat-enhance.PNG" alt="enhance"></p><p>Loss:</p><p><img src="/img/gan_and_applications/diat-enhance-loss.png" alt="loss"></p></li></ol><h3 id="3-3-DIAT-A"><a href="#3-3-DIAT-A" class="headerlink" title="3.3 DIAT-A"></a>3.3 DIAT-A</h3><p>In DIAT, the perceptual identity loss is defined on the pre-trained VGG-Face. Actually, it may be more effective to define this loss on some CNN trained to attribute transfer. Here we treat identity-preserving and attribute transfer as two related tasks, and define the perceptual identity loss based on the convolutional features of the discriminator. By this way, the network parameters for identity loss will be changed along with the updating of discriminator, and thus we named it as adaptive perceptual identity loss. </p><p><img src="/img/gan_and_applications/diat-a-transform-loss.png" alt="loss"></p><h3 id="3-4-Experiments"><a href="#3-4-Experiments" class="headerlink" title="3.4 Experiments"></a>3.4 Experiments</h3><p>Dataset: a subset of the aligned CelebA dataset  </p><p><img src="/img/gan_and_applications/diat-experiment1.PNG" alt="experiment"></p><p><img src="/img/gan_and_applications/diat-experiment2.PNG" alt="experiment"></p><p><img src="/img/gan_and_applications/diat-experiment3.PNG" alt="experiment"></p><h2 id="4-Unsupervised-Cross-Domain-Image-Generation-ICLR-2017"><a href="#4-Unsupervised-Cross-Domain-Image-Generation-ICLR-2017" class="headerlink" title="4 Unsupervised Cross-Domain Image Generation(ICLR 2017 )"></a>4 Unsupervised Cross-Domain Image Generation(ICLR 2017 )</h2><h3 id="4-1-Task"><a href="#4-1-Task" class="headerlink" title="4.1 Task"></a>4.1 Task</h3><p>Recent achievements replicate some of these capabilities to some degree: Generative Adversarial Networks (GANs) are able to convincingly generate novel samples that match that of a given training set; style transfer methods are able to alter the visual style of images; domain adaptation methods are able to generalize learned functions to new domains even without labeled samples in the target domain and transfer learning is now commonly used to import existing knowledge and to make learning much more efficient.</p><p>These capabilities, however, do not address the general analogy synthesis problem that we tackle in this work. Namely, <strong>given separated but otherwise unlabeled samples from domains $S$ and $T$ and a perceptual function $f$, learn a mapping $G : S \to T$ such that $f(x) ∼ f(G(x)$ </strong></p><p>As a main application challenge, we tackle the problem of <strong>emoji generation for a given facial image</strong>. Despite a growing interest in emoji and the hurdle of creating such personal emoji manually, no system has been proposed, to our knowledge, that can solve this problem. Our method is able to produce face emoji that are visually appealing and capture much more of the facial characteristics than the emoji created by well-trained human annotators who use the conventional tools.</p><h3 id="4-2-Model"><a href="#4-2-Model" class="headerlink" title="4.2 Model"></a>4.2 Model</h3><p><img src="/img/gan_and_applications/dtn-model.PNG" alt="dtn model"></p><p>Loss: </p><p><img src="/img/gan_and_applications/dtn-loss.png" alt="dtn-loss"></p><ol><li>$D$ is a ternary classification function from the domain $T$ to 1,2,3, and $D_i(x)$ is the<br>probability it assigns to class $i = 1,2,3$ for an input sample $x$</li><li>During optimization, $L_G$ is minimized over $g$ and $L_D$ is minimized over $D$ </li><li>$L_{CONST}$ enforces f-constancy for $x \in S$, while $L_{TID}$ enforces that for samples $x \in T$  </li><li>$L_{TV}$ is an anisotropic total variation loss, which is added in order to slightly smooth the resulting image</li><li>$f$ is trained use other datasets before training this model</li></ol><h3 id="4-3-Experiments"><a href="#4-3-Experiments" class="headerlink" title="4.3 Experiments"></a>4.3 Experiments</h3><p><img src="/img/gan_and_applications/dtn-e1.PNG" alt="dtn experiment"></p><p>Dataset: </p><ol><li>Street View House Number (SVHN) dataset to the domain of the MNIST dataset</li><li>FROM PHOTOS TO EMOJI</li></ol><p>Metrics: MNIST Accuracy</p><p><img src="/img/gan_and_applications/dtn-e2.PNG" alt="e"></p><p><img src="/img/gan_and_applications/dtn-e3.PNG" alt="e3"></p><h2 id="5-StarGAN-Multi-Domain-Image-to-Image-Translation"><a href="#5-StarGAN-Multi-Domain-Image-to-Image-Translation" class="headerlink" title="5 StarGAN: Multi-Domain Image-to-Image Translation"></a>5 StarGAN: Multi-Domain Image-to-Image Translation</h2><h3 id="5-1-Introduction"><a href="#5-1-Introduction" class="headerlink" title="5.1 Introduction"></a>5.1 Introduction</h3><p>Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. </p><p>To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model</p><p>We can further extend to training multiple domains from different datasets.</p><h3 id="5-2-Model"><a href="#5-2-Model" class="headerlink" title="5.2 Model"></a>5.2 Model</h3><p><img src="/img/gan_and_applications/stargan-model.PNG" alt="model"></p><p>Loss:</p><p><img src="/img/gan_and_applications/stargan-loss.png" alt="loss"></p><ol><li>a domain classification loss of real images($L_{cls}^r$) used to optimize D, and a domain classification loss of fake images($L_{cls}^f$) used to optimize G </li><li>Use $L_{rec}$ to guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs.</li></ol><h3 id="5-3-Training-with-Multiple-Datasets"><a href="#5-3-Training-with-Multiple-Datasets" class="headerlink" title="5.3 Training with Multiple Datasets"></a>5.3 Training with Multiple Datasets</h3><h4 id="5-3-1-Mask-Vector"><a href="#5-3-1-Mask-Vector" class="headerlink" title="5.3.1 Mask Vector"></a>5.3.1 Mask Vector</h4><p><img src="/img/gan_and_applications/stargan-maskv.PNG" alt="mask"></p><p>In StarGAN, we use an n-dimensional one-hot vector to represent m, with n being the number of datasets.  and $c_i$ represents a vector for the labels of the $i$-th dataset. The vector of the known label $c_i$ can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes</p><h4 id="5-3-2-Training-Strategy"><a href="#5-3-2-Training-Strategy" class="headerlink" title="5.3.2 Training Strategy"></a>5.3.2 Training Strategy</h4><p> When training StarGAN with multiple datasets, we use the domain label $\overset{\sim}{c}$ defined at above as input to the generator. By doing so, the generator learns to ignore the unspecified labels, which are zero vectors, and<br>focus on the explicitly given label. The structure of the generator is exactly the same as in training with a single dataset, except for the dimension of the input label $\overset{\sim}{c}$. </p><h3 id="5-3-3-CelebA-and-RaFD-dataset-demo"><a href="#5-3-3-CelebA-and-RaFD-dataset-demo" class="headerlink" title="5.3.3 CelebA and RaFD dataset demo"></a>5.3.3 CelebA and RaFD dataset demo</h3><p><img src="/img/gan_and_applications/stargan-model2.PNG" alt="model"></p><p><img src="/img/gan_and_applications/stargan-model3.PNG" alt="model"></p><h3 id="5-4-Experiments"><a href="#5-4-Experiments" class="headerlink" title="5.4 Experiments"></a>5.4 Experiments</h3><p>Dataset: CelebA, RaFD</p><p><img src="/img/gan_and_applications/stargan-e1.PNG" alt="e"></p><p><img src="/img/gan_and_applications/stargan-e3.PNG" alt="e"></p><p><img src="/img/gan_and_applications/stargan-e4.PNG" alt="e"></p><p>Metrics: AMT(human evaluation)</p><p><img src="/img/gan_and_applications/stargan-e2.PNG" alt="e"></p><p>Dataset: RaFD dataset (90%/10% splitting for training and test sets) </p><p>Metrics: compute the classification error of a facial expression on synthesized images</p><p><img src="/img/gan_and_applications/stargan-e5.PNG" alt="e"></p><h2 id="6-Pix2Pix-Image-to-Image-Translation-with-Conditional-Adversarial-Networks-use-paired-data-CVPR-2017"><a href="#6-Pix2Pix-Image-to-Image-Translation-with-Conditional-Adversarial-Networks-use-paired-data-CVPR-2017" class="headerlink" title="6 Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks(use paired data)(CVPR 2017)"></a>6 Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks(use paired data)(CVPR 2017)</h2><h3 id="6-1-Introduction"><a href="#6-1-Introduction" class="headerlink" title="6.1 Introduction"></a>6.1 Introduction</h3><p>We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. </p><p>we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either </p><p>(One architecture to different works)</p><h3 id="6-2-Model"><a href="#6-2-Model" class="headerlink" title="6.2 Model"></a>6.2 Model</h3><h4 id="6-2-1-Generator-with-skips"><a href="#6-2-1-Generator-with-skips" class="headerlink" title="6.2.1 Generator with skips"></a>6.2.1 Generator with skips</h4><p><img src="/img/gan_and_applications/pix2pix-model1.PNG" alt="model"></p><h4 id="6-2-2-Conditional-GANs"><a href="#6-2-2-Conditional-GANs" class="headerlink" title="6.2.2 Conditional GANs"></a>6.2.2 Conditional GANs</h4><p><img src="/img/gan_and_applications/pix2pix-model2.PNG" alt="model"></p><h4 id="6-2-3-PatchGAN"><a href="#6-2-3-PatchGAN" class="headerlink" title="6.2.3 PatchGAN"></a>6.2.3 PatchGAN</h4><p>It is well known that the L2 loss and L1produce blurry results on image generation problems . Although these losses fail to encourage high-frequency crispness, in many cases they nonetheless accurately capture the low frequencies .</p><p>In order to model high-frequencies, it is sufficient to restrict our attention to the structure in local image patches. Therefore, we design a discriminator architecture – which we term a PatchGAN – that only penalizes structure at the scale of patches. This discriminator tries to classify if each N × N patch in an image is real or fake. We run this discriminator convolutionally across the image, averaging all responses to provide the ultimate output of D </p><h3 id="6-2-4-Loss"><a href="#6-2-4-Loss" class="headerlink" title="6.2.4 Loss"></a>6.2.4 Loss</h3><p><img src="/img/gan_and_applications/pix2pix-loss.png" alt="loss"></p><h3 id="6-3-Experiments"><a href="#6-3-Experiments" class="headerlink" title="6.3 Experiments"></a>6.3 Experiments</h3><p>Dataset:</p><ol><li>Semantic labels$photo, trained on the Cityscapes dataset.</li><li>Architectural labels!photo, trained on CMP Facades</li><li>Map to aerial photo, trained on data scraped from Google Maps.</li><li>BW to color photos, trained on [50 Imagenet large scale visual recognition challenge].</li><li>Edges to photo, trained on data from [64 Generative visual manipulation on the natural image manifold] and [59 Fine-Grained Visual Comparisons with Local Learning ]; binary edges generated using the HED edge detector [57 Holistically-nested edge detection ]  plus post processing.</li><li>Sketch to photo: tests edges to photo models on human drawn sketches from [18 How do humans sketch objects].</li><li>Day to night, trained on [32 Transient attributes for high-level understanding and editing of outdoor<br>scenes ].</li><li>Thermal to color photos, trained on data from [26 Multispectral pedestrian detection: Benchmark dataset and baseline].</li><li>Photo with missing pixels to inpainted photo, trained on Paris StreetView from [13 What makes paris look like paris] </li></ol><p>Metrics: AMT, FCN-scores</p><p><img src="/img/gan_and_applications/pix2pix-e1.PNG" alt="loss"></p><p><img src="/img/gan_and_applications/pix2pix-e2.PNG" alt="loss"></p><p><img src="/img/gan_and_applications/pix2pix-e3.PNG" alt="loss"></p><h2 id="7-Photo-Realistic-Single-Image-Super-Resolution-Using-a-GAN-use-paired-data-to-train-CVPR-2017"><a href="#7-Photo-Realistic-Single-Image-Super-Resolution-Using-a-GAN-use-paired-data-to-train-CVPR-2017" class="headerlink" title="7 Photo-Realistic Single Image Super-Resolution Using a GAN(use paired data to train)(CVPR 2017)"></a>7 Photo-Realistic Single Image Super-Resolution Using a GAN(use paired data to train)(CVPR 2017)</h2><h3 id="7-1-Task"><a href="#7-1-Task" class="headerlink" title="7.1 Task"></a>7.1 Task</h3><p>Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? </p><p>Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution.</p><p>To our knowledge, it is the first framework capable of inferring photo-realistic natural images for <strong>4× upscaling</strong> factors. To achieve this, we propose a perceptual loss function which consists of an <strong>adversarial loss</strong> and a <strong>content loss</strong></p><h3 id="7-2-Model"><a href="#7-2-Model" class="headerlink" title="7.2 Model"></a>7.2 Model</h3><p><img src="/img/gan_and_applications/sr-model.PNG" alt="model"></p><p>Loss:</p><p><img src="/img/gan_and_applications/sr-loss.png" alt="model"></p><ol><li>$φ<em>{i,j}$ in $l</em>{VGG/i,j}^{SR}$ , we indicate the feature map obtained by the j-th convolution (after activation) before the i-th max pooling layer within the VGG19 network </li><li>D network is optimized by the min-max game</li><li>G network is optimized by the loss $l^{SR}$</li></ol><h3 id="7-3-Experiments"><a href="#7-3-Experiments" class="headerlink" title="7.3 Experiments"></a>7.3 Experiments</h3><p>Dataset: </p><ol><li>Set5 [Low-complexity single-image super-resolution based on nonnegative neighbor embedding ],</li><li>Set14 [On single image scale-up using sparse-representations ]</li><li>BSD100</li><li>the testing set of BSD300  </li></ol><p>Metrics: Mean opinion score (MOS) testing(human evaluation)</p><p><img src="/img/gan_and_applications/sr-e1.PNG" alt="experiment"></p><p><img src="/img/gan_and_applications/sr-e2.PNG" alt="e"></p><h2 id="8-Conclusion"><a href="#8-Conclusion" class="headerlink" title="8 Conclusion"></a>8 Conclusion</h2><h3 id="8-1-Reason-for-using-GAN"><a href="#8-1-Reason-for-using-GAN" class="headerlink" title="8.1 Reason for using GAN"></a>8.1 Reason for using GAN</h3><ul><li><p>Difficulties of traditional methods</p><ol><li>How to design effective loss</li><li>How to use unpaired data</li></ol></li><li><p>GAN’s advantages</p><ol><li>No need of the specific loss, but a high level goal</li><li>Able to handle unpaired data</li></ol></li><li><p>GAN’s disadvantages</p><ol><li>The Generator network often produce insensitive results</li><li>Mode collapse: all inputs are mapped to the same output</li></ol><p><img src="/img/gan_and_applications/mse-problem.PNG" alt="mse problem"></p><p>​</p></li></ul><h3 id="8-2-Good-ideas"><a href="#8-2-Good-ideas" class="headerlink" title="8.2 Good ideas"></a>8.2 Good ideas</h3><ul><li>GAN Loss: keep high level domain feature</li><li>Keep specific entity feature<ul><li>Given separated but otherwise unlabeled samples from domains $S$ and $T$ and a perceptual function $f$, learn a mapping $G : S \to T$ such that $f(x) ∼ f(G(x))$ <ol><li>Perceptual Loss</li><li>pre-trained f</li></ol></li><li>Cycle consistency</li><li>Enhancement network</li></ul></li><li>Translations for multiple domains using only a single model</li></ul><h3 id="8-3-Metrics"><a href="#8-3-Metrics" class="headerlink" title="8.3 Metrics"></a>8.3 Metrics</h3><ul><li>Human evaluation: AMT, MOS</li><li>Visualizing the generated  results</li><li>Use a model in the target domain to evaluate: FCN Scores(MNIST classifiers, VGG face classifier)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-GAN&quot;&gt;&lt;a href=&quot;#1-GAN&quot; class=&quot;headerlink&quot; title=&quot;1 GAN&quot;&gt;&lt;/a&gt;1 GAN&lt;/h2&gt;&lt;h3 id=&quot;1-1-Introduction&quot;&gt;&lt;a href=&quot;#1-1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1.1 Introduction&quot;&gt;&lt;/a&gt;1.1 Introduction&lt;/h3&gt;&lt;p&gt;To learn the generator’s distribution $p_g$ over data x, we define a prior on input noise variables $p_z(z)$, then represent a mapping to data space as $G(z; θ_g)$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $θ_g$. We also define a second multilayer perceptron $D(x; θ_d)$ that outputs a single scalar. $D(x)$ represents the probability that $x$ came from the data rather than $p_g$. We train $D$ to maximize the probability of assigning the correct label to both training examples and samples from $G$. We simultaneously train $G$ to minimize $log(1 - D(G(z)))$. In other words, $D$ and $G$ play the following two-player mini-max game with value function $V (G; D):$&lt;/p&gt;
&lt;p&gt;$$min_G max_DV (D; G) = E_x∼p_{data(x)}[log D(x)] + E_{z∼p_z(z)}[log(1 - D(G(z)))]$$&lt;/p&gt;
    
    </summary>
    
    
      <category term="GAN" scheme="https://n4a.github.io/tags/GAN/"/>
    
      <category term="Image Translation" scheme="https://n4a.github.io/tags/Image-Translation/"/>
    
      <category term="Cross Domain Learning" scheme="https://n4a.github.io/tags/Cross-Domain-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Capsule net</title>
    <link href="https://n4a.github.io/2017/11/09/Capsule-net/"/>
    <id>https://n4a.github.io/2017/11/09/Capsule-net/</id>
    <published>2017-11-09T08:08:07.000Z</published>
    <updated>2018-07-17T12:05:22.950Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Transforming-Auto-encoder-ICANN-2011"><a href="#1-Transforming-Auto-encoder-ICANN-2011" class="headerlink" title="1 Transforming Auto-encoder(ICANN 2011 )"></a>1 Transforming Auto-encoder(ICANN 2011 )</h2><h3 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h3><blockquote><p>Artificial neural networks should use local “capsules” that perform some quite complicated internal computations on their inputs and then encapsulate the results of these computations into a small vector of highly informative outputs. Each capsule learns to recognize an implicitly defined visual entity over a limited domain of viewing conditions and deformations and it outputs both the probability that the entity is present within its limited domain and a set of “instantiation parameters” that may include the precise pose, lighting and deformation of the visual entity relative to an implicitly defined canonical version of that entity. When the capsule is working properly, the probability of the visual entity being present is locally invariant – it does not change as the entity moves over the manifold of possible appearances within the limited domain covered by the capsule. The instantiation parameters, however, are “equivariant” – as the viewing conditions change and the entity moves over the appearance manifold, the instantiation parameters change by a corresponding amount because they are representing the intrinsic coordinates of the entity on the appearance manifold. </p></blockquote><a id="more"></a><h3 id="1-2-Details-about-a-image-shift-example"><a href="#1-2-Details-about-a-image-shift-example" class="headerlink" title="1.2 Details about a image shift example"></a>1.2 Details about a image shift example</h3><ol><li><p>Model graph</p><p><img src="/img/tae.PNG" alt="tae"></p></li><li><p>Core codes (use tensorflow)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, X_in, extra_in)</span>:</span></span><br><span class="line">  rec = tf.sigmoid(self.fc_layer(X_in, self.in_dim, self.r_dim,</span><br><span class="line">                                 <span class="string">'recog_layer_pre_act'</span>), <span class="string">'recog_layer'</span>)</span><br><span class="line">   </span><br><span class="line">  xy_vec = self.fc_layer(rec, self.r_dim, <span class="number">2</span>, <span class="string">'xy_prediction'</span>)</span><br><span class="line">  pro = tf.sigmoid(self.fc_layer(rec, self.r_dim, <span class="number">1</span>,</span><br><span class="line">                                 <span class="string">'probability_lin'</span>),<span class="string">'probability_prediction'</span>)</span><br><span class="line">  probability_vec = tf.tile(pro, (<span class="number">1</span>, self.in_dim))</span><br><span class="line">   </span><br><span class="line">  xy_extend = tf.add(xy_vec, extra_in)</span><br><span class="line">  gen = tf.sigmoid(self.fc_layer(xy_extend, <span class="number">2</span>, self.g_dim, <span class="string">'gen_pre_act'</span>), </span><br><span class="line">                   <span class="string">'gen_layer'</span>)</span><br><span class="line">   </span><br><span class="line">  out = self.fc_layer(gen, self.g_dim, self.in_dim, <span class="string">'out_prediction'</span>)</span><br><span class="line">   </span><br><span class="line">  <span class="keyword">return</span> tf.multiply(out, probability_vec)</span><br></pre></td></tr></table></figure></li><li><p>Experiment Result</p><p><img src="/img/experiment.PNG" alt="experiment"></p></li></ol><h3 id="1-3-Work-to-do"><a href="#1-3-Work-to-do" class="headerlink" title="1.3 Work to do"></a>1.3 Work to do</h3><p>​    Once pixel intensities have been converted into the outputs of a set of active, first-level capsules each of which produces an explicit representation of the pose of its visual entity, it is relatively easy to see how larger and more complex visual entities can be recognized by using <strong>agreements of the poses</strong> predicted by active, lower-level capsules. </p><p>​    <strong>Agreements of the poses example</strong>： If a capsule can learn to output the pose of its visual entity in a vector that is linearly related to the “natural” representations of pose used in computer graphics, there is a simple and highly selective test for whether the visual entities represented by two active capsules, A and B, have the right spatial relationship to activate a higher-level capsule, C. Suppose that the pose outputs of capsule A are represented by a matrix, TA, that specifies the coordinate transform between the canonical visual entity of A and the actual instantiation of that entity found by capsule A. If we multiply TA by the part-whole coordinate transform TAC that relates the canonical visual entity of A to the canonical visual entity of C, we get a prediction for TC. Similarly, we can use TB and TBC to get another prediction. If these predictions are a good match, the instantiations found by capsules A and B are in the right spatial relationship to activate capsule C and the average of the predictions tells us how the larger visual entity represented by C is transformed relative to the canonical visual entity of C. If, for example, A represents a mouth and B represents a nose, they can each make a prediction for the pose of the face. If these predictions agree, the mouth and nose must be in the right spatial relationship to form a face. An interesting property of this way of performing shape recognition is that the knowledge of part-whole relationships is viewpoint-invariant and is represented by weight matrices whereas the knowledge of the instantiation parameters of currently observed objects and their parts is viewpoint-equivariant and is represented by neural activities  </p><h2 id="2-Dynamic-Routing-Between-Capsules-NIPS-2017"><a href="#2-Dynamic-Routing-Between-Capsules-NIPS-2017" class="headerlink" title="2 Dynamic Routing Between Capsules(NIPS 2017)"></a>2 Dynamic Routing Between Capsules(NIPS 2017)</h2><h3 id="2-1-Introduction"><a href="#2-1-Introduction" class="headerlink" title="2.1 Introduction"></a>2.1 Introduction</h3><p>​    The previous paper said that: “Once pixel intensities have been converted into the outputs of a set of active, first-level capsules each of which produces an explicit representation of the pose of its visual entity, it is relatively easy to see how larger and more complex visual entities can be recognized by using <strong>agreements of the poses</strong> predicted by active, lower-level capsules. “ But it didn’t do the work to build a multilayer capsules network.</p><p>​    This paper give a implementation of the multilayer capsules network that do classification work on <strong>MNIST</strong> dataset. And it give a routing algorithm comparing to just the word <strong>agreements</strong>. It give a detail formulation to measure <strong>agreements</strong>. Then it use <strong>Margin Loss</strong> and <strong>Reconstruction</strong> as a regularization method </p><p>​    Besides, instead of  using common full connection layer to initialize the first-level capsules, it use convolutional neural network to do this work. The paper says: “Convolutional neural networks (CNNs) use translated replicas of learned feature detectors and this allows them to translate knowledge about good weight values acquired at one position in an image to other positions. This has proven extremely helpful in image interpretation. Even though we are replacing the scalar-output feature detectors of CNNs with vector-output capsules and max-pooling with routing-by-agreement, we would still like to replicate learned knowledge across space, so we make all but the last layer of capsules be convolutional. “ </p><h3 id="2-2-Model-detail"><a href="#2-2-Model-detail" class="headerlink" title="2.2 Model detail"></a>2.2 Model detail</h3><ol><li><p>Model graph</p><p><img src="/img/capsnet.PNG" alt="capsule net"></p></li><li><p>Core codes and algorithms</p><ol><li><p>First layer is a common convolutional layer</p></li><li><p>Second layer is PrimaryCaps layer.  But One can see ist as a Convolution layer with $$v_j = \frac{||s_j||^2}{1 +||s_j||^2 }\frac{s_j}{||s_j||}$$  as its block non-linearity.  Just as the following codes.(In the following codes, squash is a function to implement the former eq. )</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PrimaryCap</span><span class="params">(inputs, dim_vector, n_channels, kernel_size, strides, padding)</span>:</span></span><br><span class="line">    outputs = []</span><br><span class="line">    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size,</span><br><span class="line">                           strides=strides, padding=padding)(inputs)</span><br><span class="line">    outputs = layers.Reshape(target_shape=[<span class="number">-1</span>, dim_vector])(output)</span><br><span class="line">    <span class="keyword">return</span> layers.Lambda(squash)(outputs)</span><br></pre></td></tr></table></figure></li><li><p>In total PrimaryCapsules has [32; 6; 6] capsule outputs (each output is an 8D vector) . (shape is [32*6*6, 8]). The third layer is Digit layer. Between these two layers, the author use the routing algorithm as bellow.</p><p><img src="/img/routing.PNG" alt="routing"></p></li><li><p>Margin loss.</p><p><img src="/img/margin_loss.PNG" alt="margin losss"></p></li><li><p>Reconstruction as a regularization method.</p><p><img src="/img/reg.PNG" alt="regularization"></p><p>Fig. 3:</p><p> <img src="/img/reconstruction.PNG" alt="reconstruction"></p></li></ol></li><li><p>Experiment Result</p><ol><li>error rate on MNIST</li></ol><p><img src="/img/capsnet_exp.PNG" alt="experiment"></p><ol start="2"><li><p>What the individual dimensions of a capsule represent.</p><blockquote><p>Since we are passing the encoding of only one digit and zeroing out other digits, the dimensions of a digit capsule should learn to span the space of variations in the way digits of that class are instantiated. These variations include stroke thickness, skew and width. They also include digit-specific variations such as the length of the tail of a 2. We can see what the individual dimensions represent by making use of the decoder network. After computing the activity vector for the correct digit capsule, we can feed a perturbed version of this activity vector to the decoder network and see how the perturbation affects the reconstruction. Examples of these perturbations are shown in Fig. 4. We found that one dimension (out of 16) of the capsule almost always represents the width of the digit. While some dimensions represent combinations of global variations, there are other dimensions that represent variation in a localized part of the digit. For example, different dimensions are used for the length of the ascender of a 6 and the size of the loop </p></blockquote><p>Fig 4:</p><p><img src="/img/digit_d.PNG" alt="What the individual dimensions of a capsule represent"></p></li><li><p>Robustness to Affine Transformations </p><blockquote><p>To test the robustness of CapsNet to affine transformations we trained a CapsNet and a traditional convolutional network (with MaxPooling and DropOut) on a padded and translated MNIST training set, in which each example is an MNIST digit placed randomly on a black background of 40 × 40 pixels. We then tested this network on the affNIST4 data set, in which each example is an MNIST digit with a random small affine transformation.</p></blockquote><p>| Net\Accuracy | Expanded MNIST Test Set | Affined Test Set |<br>| :———-: | :———————: | :————–: |<br>|   CapsNet    |         99.23%          |        79        |<br>|     CNN      |          99.22          |        66        |</p></li><li><p>Segmenting highly overlapping digits</p><p><img src="/img/overlap.PNG" alt="overlap"></p><blockquote><p>Figure 5: Sample reconstructions of a CapsNet with 3 routing iterations on MultiMNIST test dataset. <strong>The two reconstructed digits are overlayed in green and red as the lower image</strong>. The upper image shows the input image. L:(l1; l2) represents the label for the two digits in the image and R:(r1; r2) represents the two digits used for reconstruction. The two right most columns show two examples with wrong classification reconstructed from the label and from the prediction (P). In (2; 8) example the model confuses 8 with a 7 and in (4; 9) it confuses 9 with 0. The other columns have correct classifications and show that model accounts for all the pixels while being able to assign one pixel to two digits in extremely difficult scenarios (column 1 - 4). Note that in dataset generation the pixel values are clipped at 1. The two columns with <strong>the (*) mark</strong> show reconstructions from a digit that is neither the label nor the prediction. </p></blockquote></li></ol></li></ol><h3 id="2-3-Drawbacks"><a href="#2-3-Drawbacks" class="headerlink" title="2.3 Drawbacks"></a>2.3 Drawbacks</h3><blockquote><p>One drawback of Capsules which it shares with generative models is that it likes to account for<br>everything in the image so it does better when it can model the clutter than when it just uses an<br>additional “orphan” category in the dynamic routing. In CIFAR-10, the backgrounds are much too<br>varied to model in a reasonable sized net which helps to account for the poorer performance. </p></blockquote><h2 id="3-Matrix-capsules-with-EM-routing-ICLR-2018-under-review"><a href="#3-Matrix-capsules-with-EM-routing-ICLR-2018-under-review" class="headerlink" title="3 Matrix capsules with EM routing  (ICLR 2018, under review)"></a>3 Matrix capsules with EM routing  (ICLR 2018, under review)</h2><h3 id="3-1-Introduction"><a href="#3-1-Introduction" class="headerlink" title="3.1 Introduction"></a>3.1 Introduction</h3><p>The system in previous paper has several deficiencies  as arguing in this paper:</p><ol><li>It uses the length of the pose vector to represent the probability that the entity represented by<br>a capsule is present. To keep the length less than 1 requires an unprincipled non-linearity<br>that prevents there from being any sensible objective function that is minimized by the<br>iterative routing procedure. </li><li>It uses the cosine of the angle between two pose vectors to measure their agreement. Unlike<br>the log variance of a Gaussian cluster, the cosine is not good at distinguishing between quite<br>good agreement and very good agreement </li><li>It uses a vector of length n rather than a matrix with n elements to represent a pose, so its<br>transformation matrices have n2 parameters rather than just n. </li></ol><h3 id="3-2-Model-detail"><a href="#3-2-Model-detail" class="headerlink" title="3.2 Model detail"></a>3.2 Model detail</h3><ol><li><p>Model Graph</p><p><img src="/img/em_capsnet.PNG" alt="model"></p></li><li><p>Gaussian mixture</p><p><img src="/img/EM_old_faithful.gif" alt="gaussian mixture"></p></li><li><p>EM routing</p><p><img src="/img/em_routing.PNG" alt="em routing"></p></li><li><p>Experiment Result: ADVERSARIAL ROBUSTNESS </p><p><img src="/img/ad_re.PNG" alt="adversarial"></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Transforming-Auto-encoder-ICANN-2011&quot;&gt;&lt;a href=&quot;#1-Transforming-Auto-encoder-ICANN-2011&quot; class=&quot;headerlink&quot; title=&quot;1 Transforming Auto-encoder(ICANN 2011 )&quot;&gt;&lt;/a&gt;1 Transforming Auto-encoder(ICANN 2011 )&lt;/h2&gt;&lt;h3 id=&quot;1-1-Introduction&quot;&gt;&lt;a href=&quot;#1-1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1.1 Introduction&quot;&gt;&lt;/a&gt;1.1 Introduction&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;Artificial neural networks should use local “capsules” that perform some quite complicated internal computations on their inputs and then encapsulate the results of these computations into a small vector of highly informative outputs. Each capsule learns to recognize an implicitly defined visual entity over a limited domain of viewing conditions and deformations and it outputs both the probability that the entity is present within its limited domain and a set of “instantiation parameters” that may include the precise pose, lighting and deformation of the visual entity relative to an implicitly defined canonical version of that entity. When the capsule is working properly, the probability of the visual entity being present is locally invariant – it does not change as the entity moves over the manifold of possible appearances within the limited domain covered by the capsule. The instantiation parameters, however, are “equivariant” – as the viewing conditions change and the entity moves over the appearance manifold, the instantiation parameters change by a corresponding amount because they are representing the intrinsic coordinates of the entity on the appearance manifold. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="CNN" scheme="https://n4a.github.io/tags/CNN/"/>
    
      <category term="Capsule Net" scheme="https://n4a.github.io/tags/Capsule-Net/"/>
    
      <category term="Deep Learning" scheme="https://n4a.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
</feed>
