<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>N4A Space</title>
  
  <subtitle>To understand and be understood</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://n4a.github.io/"/>
  <updated>2018-12-14T07:29:10.385Z</updated>
  <id>https://n4a.github.io/</id>
  
  <author>
    <name>N4A</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>BP derivation for MLP and CNN</title>
    <link href="https://n4a.github.io/2018/12/14/BP-derivation-for-MLP-and-CNN/"/>
    <id>https://n4a.github.io/2018/12/14/BP-derivation-for-MLP-and-CNN/</id>
    <published>2018-12-14T07:26:08.000Z</published>
    <updated>2018-12-14T07:29:10.385Z</updated>
    
    <content type="html"><![CDATA[<p>\section{Task description}<br>Please derive a backpropagation process</p><p>(1)   for the multi-layer neural network with one hidden layer, where data are in a m-dimensional feature space with n classes. Loss functions can use L2 distance or cross entropy.</p><p>(2)   for the LeNet-5 CNN.  </p><a id="more"></a><div class="row">    <embed src="/pdf/bp-derivation-mlp.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;\section{Task description}&lt;br&gt;Please derive a backpropagation process&lt;/p&gt;
&lt;p&gt;(1)   for the multi-layer neural network with one hidden layer, where data are in a m-dimensional feature space with n classes. Loss functions can use L2 distance or cross entropy.&lt;/p&gt;
&lt;p&gt;(2)   for the LeNet-5 CNN.  &lt;/p&gt;
    
    </summary>
    
    
      <category term="BP" scheme="https://n4a.github.io/tags/BP/"/>
    
      <category term="MLP" scheme="https://n4a.github.io/tags/MLP/"/>
    
      <category term="CNN" scheme="https://n4a.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>ml basic knowledge practice_regression problem</title>
    <link href="https://n4a.github.io/2018/10/27/ml-basic-knowledge-practice-regression-problem/"/>
    <id>https://n4a.github.io/2018/10/27/ml-basic-knowledge-practice-regression-problem/</id>
    <published>2018-10-27T12:36:48.000Z</published>
    <updated>2018-10-27T12:43:42.586Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Task-Description"><a href="#1-Task-Description" class="headerlink" title="1 Task Description"></a>1 Task Description</h2><p>Use the following dataset to do house price predicting work.</p><ol><li><a href="https://www.kaggle.com/vikrishnan/boston-house-prices" target="_blank" rel="noopener">https://www.kaggle.com/vikrishnan/boston-house-prices</a></li><li><a href="https://github.com/datasets/house-prices-uk" target="_blank" rel="noopener">https://github.com/datasets/house-prices-uk</a></li></ol><p>Details: design a model to do house price predicting work. Linear Regression models including basic linear model based on polynomial, Ridge Regression, Lasso Regression and regression model based Decision Tree must be implemented. Regression models based on SVM and Deep Learning is optional.</p><a id="more"></a><div class="row">    <embed src="/pdf/regression.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Task-Description&quot;&gt;&lt;a href=&quot;#1-Task-Description&quot; class=&quot;headerlink&quot; title=&quot;1 Task Description&quot;&gt;&lt;/a&gt;1 Task Description&lt;/h2&gt;&lt;p&gt;Use the following dataset to do house price predicting work.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/vikrishnan/boston-house-prices&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.kaggle.com/vikrishnan/boston-house-prices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/datasets/house-prices-uk&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/datasets/house-prices-uk&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Details: design a model to do house price predicting work. Linear Regression models including basic linear model based on polynomial, Ridge Regression, Lasso Regression and regression model based Decision Tree must be implemented. Regression models based on SVM and Deep Learning is optional.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Mechine Learning" scheme="https://n4a.github.io/tags/Mechine-Learning/"/>
    
      <category term="Regression Probelm" scheme="https://n4a.github.io/tags/Regression-Probelm/"/>
    
  </entry>
  
  <entry>
    <title>To the Moon_拐骗少女的小混混</title>
    <link href="https://n4a.github.io/2018/09/27/%E6%8B%90%E9%AA%97%E5%B0%91%E5%A5%B3%E7%9A%84%E5%B0%8F%E6%B7%B7%E6%B7%B7/"/>
    <id>https://n4a.github.io/2018/09/27/拐骗少女的小混混/</id>
    <published>2018-09-27T01:36:10.000Z</published>
    <updated>2018-09-28T16:58:59.021Z</updated>
    
    <content type="html"><![CDATA[<p>​    这篇文章是在之前的一篇文章的基础上续写的。没有读过的读者可以看一下这篇之前的介绍文。<a href="https://www.bilibili.com/read/cv1128664" target="_blank" rel="noopener">To the Moon 游戏设定介绍</a></p><p>​    To the Moon 这款游戏的剧情设计非常精彩。在游戏中，我们会一点点回溯主人公 John 的记忆。随着对故事的一点一点地深入了解，我们会困惑，会误解，会略带嘲弄，也会略有所感。可是当我们最后看清故事始末，只剩下感动与泪水。</p><p>​    如果想要完整的体验游戏所展现的故事，可以观看下面的视频。</p><iframe src="//player.bilibili.com/player.html?aid=31555731&cid=55173221&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="500"> </iframe><a id="more"></a><iframe src="//player.bilibili.com/player.html?aid=31555731&cid=55173303&page=2" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="500"> </iframe><iframe src="//player.bilibili.com/player.html?aid=31555731&cid=55172705&page=3" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="500"> </iframe><iframe src="//player.bilibili.com/player.html?aid=31555731&cid=55173120&page=4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="500"> </iframe><p>​    在回溯 John 记忆地过程中，我们发现女主 River 是个奇怪的人，她是一种特殊疾病患者，身患这种疾病的人既希望交流又不回避交流或者有交流障碍，可以理解为交流障碍患者。John 与其交往总是产生各种各样难以交流的问题。但是难能可贵，John 总是包容 River 奇怪的想法，并且想尽一切办法想要治好 River 的病，让她能够过上正常人的生活。这时候我们就会认为这个游戏讲了一个因爱而包容的故事，John 的形象非常饱满正面，值得学习。</p><p>​    但是，当我们来到男主中学时代的时候，就会惊讶的发现男主当年表白的动机并不单纯。Dr. Watts 看到这里之后，感概：“这么说这个家伙是个拐骗少女的小混混？”。</p><p><img src="http://i0.hdslb.com/bfs/article/941c7067c974c99290346fee00135bbf59ae4993.png" alt="img"></p><p>​    John 当初表白的动机和理由都很没有说服力，让他的朋友觉得这简直就是一段时间的骚扰罢了。John 认为 River 是一个特别的人，她总是一个人，那么跟她在一起一定是一件很酷的事。确实这个理由不怎么像是有责任心的人说的话，听上去 John 只是一时兴起，并且这样做可以让他觉得很酷。但是这时候，加上我们前面对 John 和 River 后来发展的了解。这个故事就变成了这样。虽然 John 的动机狗血，却在后来在交往过程中爱上了 River，并且毫不在乎 River 的病症给她带来的困扰。甚至许多年之后，John 认为自己不该对 River 有任何隐瞒，于是他就将自己当年表的动机告诉了 River。自此之后，River 开始以她一贯的特异的表达方式来表达她的感情。她剪了头发，并且开始疯狂地折纸兔子。这里我们机智的 Dr. Watts 又发表了个人感想：“ 然后她就发狂了，并开始做那些诡异的兔子，是吗？”。这确实像一个交流障碍症患者。</p><p><img src="http://i0.hdslb.com/bfs/article/2fa2d54e7f4aadca202379e860630644b6c3239c.png" alt="img"></p><p>​    现在，我们对 John 有了更深入的了解。John 的形象跌了一点，可也还不错。当他后来在交往过程中爱上了 River 之后，无怨无悔地为 River 付出。他对 River 的作为并不能理解，可面对 River 的坚持，他依然照办。至于 River，她的幸福此时也有了一点点不幸。</p><p>​    之后，我们再继续回溯 John 的记忆，发现 John 之前的记忆呈现黑暗和紊乱的状态。这种情况导致我们无法再继续探索。 Dr. Watts 和 Dr. Rosalence 只好基于现有的信息诱导修改 John 的记忆，帮他成去月球的愿望。如果对这个修改记忆的操作不理解可以参考这篇文章<a href="https://www.bilibili.com/read/cv1128664" target="_blank" rel="noopener">To the Moon 游戏设定介绍</a>。然而无论怎么诱导修改，John 之后的记忆演变都没有发生任何改变。最后，Dr. Rosalence 分析是因为我们还不了解 John 为什么想要到月球上去，这使得我们的诱导修改都没有对症下药。面对这种困境，Dr. Watts 和 Dr. Rosalence 想尽了办法终于回溯到了 John 的童年时期。</p><p>接下来就是游戏的高潮部分，所有的感动和泪水也都献给了这个部分。当我们回到 John 童年的时候，发现John 还有一个弟弟 Joey，这一点前面的故事也有很多伏笔。可不幸的是弟弟在院子里玩的时候，妈妈开车时没注意撞到了他。随着弟弟去世，妈妈精神有点失常，并且她给 John 服用了一些消除记忆的药物，好让他忘了这个悲惨的事情。这一切导致 John 的童年记忆难以到达。 在药物的作用下，John 忘记了许多东西，可那个重要的相遇和约定一直存在于 John 脑海的深层之处，只是受药物阻碍无法完整浮现出来。</p><p>随着记忆继续向前回溯，一次邂逅在我们面前展开。这是一个游乐园的场景，John 不想和妈妈、弟弟一起玩，就自己一个人在游乐园寻找有趣的地方，他找到了游乐园山上的一个平常没人去的角落。John在这里欣赏天上的星星。就在这时候，有一个小女孩也来到了这个地方。他们在非常惊讶，同时也很开心有一个人像自己一样来到了这个角落。小女孩的名字叫 River。也就是这一天，John 和 River 第一次相遇。John 是普通的 John，世界上有一大票人都叫 John；River 是特殊的 River，世界上没有几个人叫 River。普通的 John 和 特殊的 River 都喜欢这个角落，在这里他们一起看天上的星星，星星闪闪发光，星星就如同灯塔，向远方的灯塔传达交流的信息。无论是普通的 John 和 特殊的 River，他们一直以来只是以不同的形式保持孤独，但是，此时此刻他们都交到了朋友。</p><p><img src="http://i0.hdslb.com/bfs/article/603476bb36c2a11a7231c698a6e85316a9a6f5fc.png" alt="img"></p><p>​    他们发现了自定义的星座，一个兔子星座，而月亮就是这个兔子圆鼓鼓的肚子。</p><p><img src="http://i0.hdslb.com/bfs/article/f007c801ff21259e0d758da96f185d6850aeee17.png" alt="img"></p><p>​    在分别的时候，他们约定之后再见。“但是如果你忘记了或者走丢了呢？”，“那么我们一定会在月亮上相遇的，傻瓜！”</p><p><img src="http://i0.hdslb.com/bfs/article/330e4365f6d6d442905eee612cb189310cf417dc.png" alt="img"></p><p>​    发展至此，我们了解了故事的始末。一次简单的相遇，却是一生的缘分。再回首过往，所有的谜题都被解开。</p><p>​    John 之所以想去月球，是因为他们曾做了这样的约定，可惜 John 因为药物的影响，对这件事印象模糊了，可他记住了这个约定。“但是如果你忘记了或者走丢了呢？”，“那么我们一定会在月亮上相遇的，傻瓜！”。年老之后，River 因病先他而去。River不在身边之后，这个愿望自发地又浮现在脑海。</p><p>​    John 到中学之后，遇到了一个女孩，他爱上了这个女孩，却不知道为什么。然而他只是忘记了，就好像年老之时他想要去月球，却不知道为什么，现在他爱上了一个女孩，也不知道为什么。潜藏在记忆深处的东西自发地推动着他，让他爱上了这个女孩。“我不知道为什么爱上你，但是我一直有一种感觉：我爱你。” 看似狗血的表白理由，只是因为外人无法了解其内心的感受。</p><p>​    River 也不是若有若无的配角存在，她也不是稀里糊涂地接受了 John 的邀请，她一直在等，在等，等了许多年。感情之事，当局者清，旁观者迷。</p><p>​    John 之后一直深爱着 River，尝试所有办法来治好 River。可惜这个可怜的失忆的人不曾意识到自己的问题。“我倾尽一生想要治好你，却没有发现病人原来是自己”。</p><p>​    特殊的 River 更是一直深爱着 John，以她独特的方式。当 John 向她抛白中学时代的愧疚之后。River 剪了头发，因为他们第一次相见的时候，她是短发；她不断的折着纸兔子，因为那是他们的兔子星座；她宁愿放弃自己的治疗，也要在那个废弃的灯塔旁边盖起他们自己的房子，因为那灯塔就是他们相遇时的天上的星星。她所作的一切怪异的事情只是为了唤起 John 的记忆。</p><p>​    命运为幸福的人带来不幸，不幸的人却幸福地活着。</p><p>​    浅薄的言语永远无法表达游戏带来的感受，我推荐这款游戏，希望将这份感动送给你。<a href="https://www.bilibili.com/video/av31555731/?p=1" target="_blank" rel="noopener">To the Moon 游玩记录</a></p><p>​    游戏至此依然没有结束，之后的故事之后再说吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    这篇文章是在之前的一篇文章的基础上续写的。没有读过的读者可以看一下这篇之前的介绍文。&lt;a href=&quot;https://www.bilibili.com/read/cv1128664&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;To the Moon 游戏设定介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;​    To the Moon 这款游戏的剧情设计非常精彩。在游戏中，我们会一点点回溯主人公 John 的记忆。随着对故事的一点一点地深入了解，我们会困惑，会误解，会略带嘲弄，也会略有所感。可是当我们最后看清故事始末，只剩下感动与泪水。&lt;/p&gt;
&lt;p&gt;​    如果想要完整的体验游戏所展现的故事，可以观看下面的视频。&lt;/p&gt;
&lt;iframe src=&quot;//player.bilibili.com/player.html?aid=31555731&amp;cid=55173221&amp;page=1&quot; scrolling=&quot;no&quot; border=&quot;0&quot; frameborder=&quot;no&quot; framespacing=&quot;0&quot; allowfullscreen=&quot;true&quot; width=&quot;100%&quot; height=&quot;500&quot;&gt; &lt;/iframe&gt;
    
    </summary>
    
    
      <category term="个人" scheme="https://n4a.github.io/tags/%E4%B8%AA%E4%BA%BA/"/>
    
      <category term="生活" scheme="https://n4a.github.io/tags/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="To the Moon" scheme="https://n4a.github.io/tags/To-the-Moon/"/>
    
  </entry>
  
  <entry>
    <title>Meta Learning</title>
    <link href="https://n4a.github.io/2018/07/29/Meta-Learning/"/>
    <id>https://n4a.github.io/2018/07/29/Meta-Learning/</id>
    <published>2018-07-29T12:38:31.000Z</published>
    <updated>2018-10-27T12:49:40.229Z</updated>
    
    <content type="html"><![CDATA[<p>​    Meta Learning 最早源于上世纪八九十年代 [6], 最近成为研究的热点，这是一个很好的 可以用来解决 Learn to learn 问题的框架。 17 年 NIPS 有一个 Workshop on Meta Learning 。与迁移学习相比， Meta Learning 可以视为一个更泛化的概念。 </p><p>​    传统的机器学习方法为解决某一个特定的任务总是需要大量的训练数据，有一个很直 观的原因是因为传统的机器学习方法在训练一个模型时，总是从零开始学习。但是人类 的学习过程并不是这样，显然人的学习是一个连续的过程，当一个人想要解决某一个问题 时，他会使用之前跟这个任务相关的知识。以图像分类任务为例，传统的机器学习方法， 例如普通的 CNN 模型，或者 AlexNet， VGG， ResNet 这些模型都需要大量的训练数据。 为了解决数据依赖的问题，现在的一个研究热点就是“one shot learning” (few shot learning)[4]。许多解决这个问题的方法 [9, 8] 就是基于 Meta Learning。 </p><a id="more"></a><div class="row">    <embed src="/pdf/meta-learning.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    Meta Learning 最早源于上世纪八九十年代 [6], 最近成为研究的热点，这是一个很好的 可以用来解决 Learn to learn 问题的框架。 17 年 NIPS 有一个 Workshop on Meta Learning 。与迁移学习相比， Meta Learning 可以视为一个更泛化的概念。 &lt;/p&gt;
&lt;p&gt;​    传统的机器学习方法为解决某一个特定的任务总是需要大量的训练数据，有一个很直 观的原因是因为传统的机器学习方法在训练一个模型时，总是从零开始学习。但是人类 的学习过程并不是这样，显然人的学习是一个连续的过程，当一个人想要解决某一个问题 时，他会使用之前跟这个任务相关的知识。以图像分类任务为例，传统的机器学习方法， 例如普通的 CNN 模型，或者 AlexNet， VGG， ResNet 这些模型都需要大量的训练数据。 为了解决数据依赖的问题，现在的一个研究热点就是“one shot learning” (few shot learning)[4]。许多解决这个问题的方法 [9, 8] 就是基于 Meta Learning。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="Meta Learning" scheme="https://n4a.github.io/tags/Meta-Learning/"/>
    
      <category term="AI" scheme="https://n4a.github.io/tags/AI/"/>
    
      <category term="Machine Learning" scheme="https://n4a.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>ICML&#39;18 GAN 理论文章总结</title>
    <link href="https://n4a.github.io/2018/07/16/ICML-18-GAN-%E7%90%86%E8%AE%BA%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93/"/>
    <id>https://n4a.github.io/2018/07/16/ICML-18-GAN-理论文章总结/</id>
    <published>2018-07-16T15:17:07.000Z</published>
    <updated>2018-07-17T14:44:58.856Z</updated>
    
    <content type="html"><![CDATA[<p>这个部分有五篇文章，其中：</p><ol><li>两篇文是通过改变GAN的结构以解决GAN训练困难和模式消失（Mode collapse）的问题。</li><li>一篇文章从新的数学角度推导GAN的更新过程，该更新过程更一般化，原有的GAN参数更新过程可视为其某种条件下的特例。文中也简要说明了该更新过程是 stable 的。</li><li>一篇文章探究了GAN中生成器的 Jacobian 矩阵的奇异值分布和 GAN 性能的关系。这篇文章很有趣，它根据生成器的 Jacobian 矩阵定义了一个<strong>condition number</strong>，然后在训练过程中发现该值与常用的GAN评估方法 <strong>Inception Score</strong> 和 <strong>Frechet Inception Distance</strong> 的评估值十分相关，最后文中提出一种方法通过控制 <strong>condition number</strong> 来改进 GAN 的训练过程。</li><li>一篇文章提出一种新的方法计算WGAN中 Wasserstein distance，同时做了许多相关的理论推导。 这篇文章理论知识很多，我看起来很费劲，也很困惑。文章中虽然做了很多的工作，但是相比于WGAN没有太大的创新。</li></ol><a id="more"></a> <div class="row">    <embed src="/pdf/memo-icml18-gan.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这个部分有五篇文章，其中：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;两篇文是通过改变GAN的结构以解决GAN训练困难和模式消失（Mode collapse）的问题。&lt;/li&gt;
&lt;li&gt;一篇文章从新的数学角度推导GAN的更新过程，该更新过程更一般化，原有的GAN参数更新过程可视为其某种条件下的特例。文中也简要说明了该更新过程是 stable 的。&lt;/li&gt;
&lt;li&gt;一篇文章探究了GAN中生成器的 Jacobian 矩阵的奇异值分布和 GAN 性能的关系。这篇文章很有趣，它根据生成器的 Jacobian 矩阵定义了一个&lt;strong&gt;condition number&lt;/strong&gt;，然后在训练过程中发现该值与常用的GAN评估方法 &lt;strong&gt;Inception Score&lt;/strong&gt; 和 &lt;strong&gt;Frechet Inception Distance&lt;/strong&gt; 的评估值十分相关，最后文中提出一种方法通过控制 &lt;strong&gt;condition number&lt;/strong&gt; 来改进 GAN 的训练过程。&lt;/li&gt;
&lt;li&gt;一篇文章提出一种新的方法计算WGAN中 Wasserstein distance，同时做了许多相关的理论推导。 这篇文章理论知识很多，我看起来很费劲，也很困惑。文章中虽然做了很多的工作，但是相比于WGAN没有太大的创新。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="GAN" scheme="https://n4a.github.io/tags/GAN/"/>
    
      <category term="ICML" scheme="https://n4a.github.io/tags/ICML/"/>
    
  </entry>
  
  <entry>
    <title>GAN and it&#39;s applications on image translation</title>
    <link href="https://n4a.github.io/2018/03/30/GAN-and-it-s-applications/"/>
    <id>https://n4a.github.io/2018/03/30/GAN-and-it-s-applications/</id>
    <published>2018-03-30T08:12:14.000Z</published>
    <updated>2018-07-17T08:31:05.797Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-GAN"><a href="#1-GAN" class="headerlink" title="1 GAN"></a>1 GAN</h2><h3 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h3><p>To learn the generator’s distribution $p_g$ over data x, we define a prior on input noise variables $p_z(z)$, then represent a mapping to data space as $G(z; θ_g)$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $θ_g$. We also define a second multilayer perceptron $D(x; θ_d)$ that outputs a single scalar. $D(x)$ represents the probability that $x$ came from the data rather than $p_g$. We train $D$ to maximize the probability of assigning the correct label to both training examples and samples from $G$. We simultaneously train $G$ to minimize $log(1 - D(G(z)))$. In other words, $D$ and $G$ play the following two-player mini-max game with value function $V (G; D):$</p><p>$$min_G max_DV (D; G) = E_x∼p_{data(x)}[log D(x)] + E_{z∼p_z(z)}[log(1 - D(G(z)))]$$</p><a id="more"></a><h3 id="1-2-Theory-Analysis"><a href="#1-2-Theory-Analysis" class="headerlink" title="1.2 Theory Analysis"></a>1.2 Theory Analysis</h3><p><img src="/img/gan_and_applications/gan.PNG" alt="gan"></p><h2 id="2-Cycle-GAN-ICCV-2017"><a href="#2-Cycle-GAN-ICCV-2017" class="headerlink" title="2 Cycle GAN(ICCV 2017)"></a>2 Cycle GAN(ICCV 2017)</h2><h2 id="2-1-Task-Cross-Domain-Image-Translation"><a href="#2-1-Task-Cross-Domain-Image-Translation" class="headerlink" title="2.1 Task: Cross Domain Image Translation"></a>2.1 Task: Cross Domain Image Translation</h2><p>Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image.</p><p>In this paper, we present a method that can learn to do the same: capturing special characteristics of one image collection and figuring out how these characteristics could be translated into the other image collection, all in the absence of any paired training examples. </p><h3 id="2-2-Model-Cycle-consistent"><a href="#2-2-Model-Cycle-consistent" class="headerlink" title="2.2 Model: Cycle consistent"></a>2.2 Model: Cycle consistent</h3><p><img src="/img/gan_and_applications/cycleGAN.PNG" alt="Cycle GAN"></p><p>Loss:</p><p><img src="/img/gan_and_applications/cycleGAN-loss.png" alt="cycle gan loss"></p><h3 id="2-3-Experiment"><a href="#2-3-Experiment" class="headerlink" title="2.3 Experiment"></a>2.3 Experiment</h3><p><img src="/img/gan_and_applications/cycleGAN-experiments.PNG" alt="Experiments"></p><ol><li><p>Dataset: Cityscapes dataset , map and aerial photo on data scraped from Google Maps </p></li><li><p>Metrics: AMT perceptual studies, FCN score, Semantic segmentation metrics</p></li><li><p>Result:</p><p><img src="/img/gan_and_applications/cycleGAN-experiments2.PNG" alt="Experiments"></p><p><img src="/img/gan_and_applications/cycleGAN-experiments3.PNG" alt="Experiments"></p><p><img src="/img/gan_and_applications/cycleGAN-experiments4.PNG" alt="Experiments"></p></li></ol><h3 id="2-4-Limitations"><a href="#2-4-Limitations" class="headerlink" title="2.4 Limitations"></a>2.4 Limitations</h3><ol><li>On translation tasks that involve color and texture changes, like many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. </li><li>Some failure cases are caused by the distribution characteristics of the training datasets.</li><li>We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method.  </li></ol><p><img src="/img/gan_and_applications/cycleGAN-limit.PNG" alt="limitations"></p><h2 id="3-DIAT-Deep-Identity-aware-Transfer-of-Facial-Attributes"><a href="#3-DIAT-Deep-Identity-aware-Transfer-of-Facial-Attributes" class="headerlink" title="3 DIAT: Deep Identity-aware Transfer of Facial Attributes"></a>3 DIAT: Deep Identity-aware Transfer of Facial Attributes</h2><h3 id="3-1-Task-Identity-aware-Transfer-of-Facial-Attributes"><a href="#3-1-Task-Identity-aware-Transfer-of-Facial-Attributes" class="headerlink" title="3.1 Task: Identity-aware Transfer of Facial Attributes"></a>3.1 Task: Identity-aware Transfer of Facial Attributes</h3><p>Our DIAT and DIAT-A models can provide a unified solution for several representative facial attribute transfer tasks such as <strong>expression transfer</strong>, <strong>accessory removal</strong>, <strong>age progression</strong>, and <strong>gender</strong> transfer </p><h3 id="3-2-Model"><a href="#3-2-Model" class="headerlink" title="3.2 Model"></a>3.2 Model</h3><p>In this section, a two-stage scheme is developed to tackle the identity-aware attribute transfer task. </p><ol><li><p>Face transform network</p><p><img src="/img/gan_and_applications/diat-transform.PNG" alt="diat-transform"></p><p>Loss: </p><p><img src="/img/gan_and_applications/diat-transform-loss.png" alt="loss"></p></li><li><p>Face Enhancement Network</p><p><img src="/img/gan_and_applications/diat-enhance.PNG" alt="enhance"></p><p>Loss:</p><p><img src="/img/gan_and_applications/diat-enhance-loss.png" alt="loss"></p></li></ol><h3 id="3-3-DIAT-A"><a href="#3-3-DIAT-A" class="headerlink" title="3.3 DIAT-A"></a>3.3 DIAT-A</h3><p>In DIAT, the perceptual identity loss is defined on the pre-trained VGG-Face. Actually, it may be more effective to define this loss on some CNN trained to attribute transfer. Here we treat identity-preserving and attribute transfer as two related tasks, and define the perceptual identity loss based on the convolutional features of the discriminator. By this way, the network parameters for identity loss will be changed along with the updating of discriminator, and thus we named it as adaptive perceptual identity loss. </p><p><img src="/img/gan_and_applications/diat-a-transform-loss.png" alt="loss"></p><h3 id="3-4-Experiments"><a href="#3-4-Experiments" class="headerlink" title="3.4 Experiments"></a>3.4 Experiments</h3><p>Dataset: a subset of the aligned CelebA dataset  </p><p><img src="/img/gan_and_applications/diat-experiment1.PNG" alt="experiment"></p><p><img src="/img/gan_and_applications/diat-experiment2.PNG" alt="experiment"></p><p><img src="/img/gan_and_applications/diat-experiment3.PNG" alt="experiment"></p><h2 id="4-Unsupervised-Cross-Domain-Image-Generation-ICLR-2017"><a href="#4-Unsupervised-Cross-Domain-Image-Generation-ICLR-2017" class="headerlink" title="4 Unsupervised Cross-Domain Image Generation(ICLR 2017 )"></a>4 Unsupervised Cross-Domain Image Generation(ICLR 2017 )</h2><h3 id="4-1-Task"><a href="#4-1-Task" class="headerlink" title="4.1 Task"></a>4.1 Task</h3><p>Recent achievements replicate some of these capabilities to some degree: Generative Adversarial Networks (GANs) are able to convincingly generate novel samples that match that of a given training set; style transfer methods are able to alter the visual style of images; domain adaptation methods are able to generalize learned functions to new domains even without labeled samples in the target domain and transfer learning is now commonly used to import existing knowledge and to make learning much more efficient.</p><p>These capabilities, however, do not address the general analogy synthesis problem that we tackle in this work. Namely, <strong>given separated but otherwise unlabeled samples from domains $S$ and $T$ and a perceptual function $f$, learn a mapping $G : S \to T$ such that $f(x) ∼ f(G(x)$ </strong></p><p>As a main application challenge, we tackle the problem of <strong>emoji generation for a given facial image</strong>. Despite a growing interest in emoji and the hurdle of creating such personal emoji manually, no system has been proposed, to our knowledge, that can solve this problem. Our method is able to produce face emoji that are visually appealing and capture much more of the facial characteristics than the emoji created by well-trained human annotators who use the conventional tools.</p><h3 id="4-2-Model"><a href="#4-2-Model" class="headerlink" title="4.2 Model"></a>4.2 Model</h3><p><img src="/img/gan_and_applications/dtn-model.PNG" alt="dtn model"></p><p>Loss: </p><p><img src="/img/gan_and_applications/dtn-loss.png" alt="dtn-loss"></p><ol><li>$D$ is a ternary classification function from the domain $T$ to 1,2,3, and $D_i(x)$ is the<br>probability it assigns to class $i = 1,2,3$ for an input sample $x$</li><li>During optimization, $L_G$ is minimized over $g$ and $L_D$ is minimized over $D$ </li><li>$L_{CONST}$ enforces f-constancy for $x \in S$, while $L_{TID}$ enforces that for samples $x \in T$  </li><li>$L_{TV}$ is an anisotropic total variation loss, which is added in order to slightly smooth the resulting image</li><li>$f$ is trained use other datasets before training this model</li></ol><h3 id="4-3-Experiments"><a href="#4-3-Experiments" class="headerlink" title="4.3 Experiments"></a>4.3 Experiments</h3><p><img src="/img/gan_and_applications/dtn-e1.PNG" alt="dtn experiment"></p><p>Dataset: </p><ol><li>Street View House Number (SVHN) dataset to the domain of the MNIST dataset</li><li>FROM PHOTOS TO EMOJI</li></ol><p>Metrics: MNIST Accuracy</p><p><img src="/img/gan_and_applications/dtn-e2.PNG" alt="e"></p><p><img src="/img/gan_and_applications/dtn-e3.PNG" alt="e3"></p><h2 id="5-StarGAN-Multi-Domain-Image-to-Image-Translation"><a href="#5-StarGAN-Multi-Domain-Image-to-Image-Translation" class="headerlink" title="5 StarGAN: Multi-Domain Image-to-Image Translation"></a>5 StarGAN: Multi-Domain Image-to-Image Translation</h2><h3 id="5-1-Introduction"><a href="#5-1-Introduction" class="headerlink" title="5.1 Introduction"></a>5.1 Introduction</h3><p>Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. </p><p>To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model</p><p>We can further extend to training multiple domains from different datasets.</p><h3 id="5-2-Model"><a href="#5-2-Model" class="headerlink" title="5.2 Model"></a>5.2 Model</h3><p><img src="/img/gan_and_applications/stargan-model.PNG" alt="model"></p><p>Loss:</p><p><img src="/img/gan_and_applications/stargan-loss.png" alt="loss"></p><ol><li>a domain classification loss of real images($L_{cls}^r$) used to optimize D, and a domain classification loss of fake images($L_{cls}^f$) used to optimize G </li><li>Use $L_{rec}$ to guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs.</li></ol><h3 id="5-3-Training-with-Multiple-Datasets"><a href="#5-3-Training-with-Multiple-Datasets" class="headerlink" title="5.3 Training with Multiple Datasets"></a>5.3 Training with Multiple Datasets</h3><h4 id="5-3-1-Mask-Vector"><a href="#5-3-1-Mask-Vector" class="headerlink" title="5.3.1 Mask Vector"></a>5.3.1 Mask Vector</h4><p><img src="/img/gan_and_applications/stargan-maskv.PNG" alt="mask"></p><p>In StarGAN, we use an n-dimensional one-hot vector to represent m, with n being the number of datasets.  and $c_i$ represents a vector for the labels of the $i$-th dataset. The vector of the known label $c_i$ can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes</p><h4 id="5-3-2-Training-Strategy"><a href="#5-3-2-Training-Strategy" class="headerlink" title="5.3.2 Training Strategy"></a>5.3.2 Training Strategy</h4><p> When training StarGAN with multiple datasets, we use the domain label $\overset{\sim}{c}$ defined at above as input to the generator. By doing so, the generator learns to ignore the unspecified labels, which are zero vectors, and<br>focus on the explicitly given label. The structure of the generator is exactly the same as in training with a single dataset, except for the dimension of the input label $\overset{\sim}{c}$. </p><h3 id="5-3-3-CelebA-and-RaFD-dataset-demo"><a href="#5-3-3-CelebA-and-RaFD-dataset-demo" class="headerlink" title="5.3.3 CelebA and RaFD dataset demo"></a>5.3.3 CelebA and RaFD dataset demo</h3><p><img src="/img/gan_and_applications/stargan-model2.PNG" alt="model"></p><p><img src="/img/gan_and_applications/stargan-model3.PNG" alt="model"></p><h3 id="5-4-Experiments"><a href="#5-4-Experiments" class="headerlink" title="5.4 Experiments"></a>5.4 Experiments</h3><p>Dataset: CelebA, RaFD</p><p><img src="/img/gan_and_applications/stargan-e1.PNG" alt="e"></p><p><img src="/img/gan_and_applications/stargan-e3.PNG" alt="e"></p><p><img src="/img/gan_and_applications/stargan-e4.PNG" alt="e"></p><p>Metrics: AMT(human evaluation)</p><p><img src="/img/gan_and_applications/stargan-e2.PNG" alt="e"></p><p>Dataset: RaFD dataset (90%/10% splitting for training and test sets) </p><p>Metrics: compute the classification error of a facial expression on synthesized images</p><p><img src="/img/gan_and_applications/stargan-e5.PNG" alt="e"></p><h2 id="6-Pix2Pix-Image-to-Image-Translation-with-Conditional-Adversarial-Networks-use-paired-data-CVPR-2017"><a href="#6-Pix2Pix-Image-to-Image-Translation-with-Conditional-Adversarial-Networks-use-paired-data-CVPR-2017" class="headerlink" title="6 Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks(use paired data)(CVPR 2017)"></a>6 Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks(use paired data)(CVPR 2017)</h2><h3 id="6-1-Introduction"><a href="#6-1-Introduction" class="headerlink" title="6.1 Introduction"></a>6.1 Introduction</h3><p>We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. </p><p>we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either </p><p>(One architecture to different works)</p><h3 id="6-2-Model"><a href="#6-2-Model" class="headerlink" title="6.2 Model"></a>6.2 Model</h3><h4 id="6-2-1-Generator-with-skips"><a href="#6-2-1-Generator-with-skips" class="headerlink" title="6.2.1 Generator with skips"></a>6.2.1 Generator with skips</h4><p><img src="/img/gan_and_applications/pix2pix-model1.PNG" alt="model"></p><h4 id="6-2-2-Conditional-GANs"><a href="#6-2-2-Conditional-GANs" class="headerlink" title="6.2.2 Conditional GANs"></a>6.2.2 Conditional GANs</h4><p><img src="/img/gan_and_applications/pix2pix-model2.PNG" alt="model"></p><h4 id="6-2-3-PatchGAN"><a href="#6-2-3-PatchGAN" class="headerlink" title="6.2.3 PatchGAN"></a>6.2.3 PatchGAN</h4><p>It is well known that the L2 loss and L1produce blurry results on image generation problems . Although these losses fail to encourage high-frequency crispness, in many cases they nonetheless accurately capture the low frequencies .</p><p>In order to model high-frequencies, it is sufficient to restrict our attention to the structure in local image patches. Therefore, we design a discriminator architecture – which we term a PatchGAN – that only penalizes structure at the scale of patches. This discriminator tries to classify if each N × N patch in an image is real or fake. We run this discriminator convolutionally across the image, averaging all responses to provide the ultimate output of D </p><h3 id="6-2-4-Loss"><a href="#6-2-4-Loss" class="headerlink" title="6.2.4 Loss"></a>6.2.4 Loss</h3><p><img src="/img/gan_and_applications/pix2pix-loss.png" alt="loss"></p><h3 id="6-3-Experiments"><a href="#6-3-Experiments" class="headerlink" title="6.3 Experiments"></a>6.3 Experiments</h3><p>Dataset:</p><ol><li>Semantic labels$photo, trained on the Cityscapes dataset.</li><li>Architectural labels!photo, trained on CMP Facades</li><li>Map to aerial photo, trained on data scraped from Google Maps.</li><li>BW to color photos, trained on [50 Imagenet large scale visual recognition challenge].</li><li>Edges to photo, trained on data from [64 Generative visual manipulation on the natural image manifold] and [59 Fine-Grained Visual Comparisons with Local Learning ]; binary edges generated using the HED edge detector [57 Holistically-nested edge detection ]  plus post processing.</li><li>Sketch to photo: tests edges to photo models on human drawn sketches from [18 How do humans sketch objects].</li><li>Day to night, trained on [32 Transient attributes for high-level understanding and editing of outdoor<br>scenes ].</li><li>Thermal to color photos, trained on data from [26 Multispectral pedestrian detection: Benchmark dataset and baseline].</li><li>Photo with missing pixels to inpainted photo, trained on Paris StreetView from [13 What makes paris look like paris] </li></ol><p>Metrics: AMT, FCN-scores</p><p><img src="/img/gan_and_applications/pix2pix-e1.PNG" alt="loss"></p><p><img src="/img/gan_and_applications/pix2pix-e2.PNG" alt="loss"></p><p><img src="/img/gan_and_applications/pix2pix-e3.PNG" alt="loss"></p><h2 id="7-Photo-Realistic-Single-Image-Super-Resolution-Using-a-GAN-use-paired-data-to-train-CVPR-2017"><a href="#7-Photo-Realistic-Single-Image-Super-Resolution-Using-a-GAN-use-paired-data-to-train-CVPR-2017" class="headerlink" title="7 Photo-Realistic Single Image Super-Resolution Using a GAN(use paired data to train)(CVPR 2017)"></a>7 Photo-Realistic Single Image Super-Resolution Using a GAN(use paired data to train)(CVPR 2017)</h2><h3 id="7-1-Task"><a href="#7-1-Task" class="headerlink" title="7.1 Task"></a>7.1 Task</h3><p>Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? </p><p>Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution.</p><p>To our knowledge, it is the first framework capable of inferring photo-realistic natural images for <strong>4× upscaling</strong> factors. To achieve this, we propose a perceptual loss function which consists of an <strong>adversarial loss</strong> and a <strong>content loss</strong></p><h3 id="7-2-Model"><a href="#7-2-Model" class="headerlink" title="7.2 Model"></a>7.2 Model</h3><p><img src="/img/gan_and_applications/sr-model.PNG" alt="model"></p><p>Loss:</p><p><img src="/img/gan_and_applications/sr-loss.png" alt="model"></p><ol><li>$φ<em>{i,j}$ in $l</em>{VGG/i,j}^{SR}$ , we indicate the feature map obtained by the j-th convolution (after activation) before the i-th max pooling layer within the VGG19 network </li><li>D network is optimized by the min-max game</li><li>G network is optimized by the loss $l^{SR}$</li></ol><h3 id="7-3-Experiments"><a href="#7-3-Experiments" class="headerlink" title="7.3 Experiments"></a>7.3 Experiments</h3><p>Dataset: </p><ol><li>Set5 [Low-complexity single-image super-resolution based on nonnegative neighbor embedding ],</li><li>Set14 [On single image scale-up using sparse-representations ]</li><li>BSD100</li><li>the testing set of BSD300  </li></ol><p>Metrics: Mean opinion score (MOS) testing(human evaluation)</p><p><img src="/img/gan_and_applications/sr-e1.PNG" alt="experiment"></p><p><img src="/img/gan_and_applications/sr-e2.PNG" alt="e"></p><h2 id="8-Conclusion"><a href="#8-Conclusion" class="headerlink" title="8 Conclusion"></a>8 Conclusion</h2><h3 id="8-1-Reason-for-using-GAN"><a href="#8-1-Reason-for-using-GAN" class="headerlink" title="8.1 Reason for using GAN"></a>8.1 Reason for using GAN</h3><ul><li><p>Difficulties of traditional methods</p><ol><li>How to design effective loss</li><li>How to use unpaired data</li></ol></li><li><p>GAN’s advantages</p><ol><li>No need of the specific loss, but a high level goal</li><li>Able to handle unpaired data</li></ol></li><li><p>GAN’s disadvantages</p><ol><li>The Generator network often produce insensitive results</li><li>Mode collapse: all inputs are mapped to the same output</li></ol><p><img src="/img/gan_and_applications/mse-problem.PNG" alt="mse problem"></p><p>​</p></li></ul><h3 id="8-2-Good-ideas"><a href="#8-2-Good-ideas" class="headerlink" title="8.2 Good ideas"></a>8.2 Good ideas</h3><ul><li>GAN Loss: keep high level domain feature</li><li>Keep specific entity feature<ul><li>Given separated but otherwise unlabeled samples from domains $S$ and $T$ and a perceptual function $f$, learn a mapping $G : S \to T$ such that $f(x) ∼ f(G(x))$ <ol><li>Perceptual Loss</li><li>pre-trained f</li></ol></li><li>Cycle consistency</li><li>Enhancement network</li></ul></li><li>Translations for multiple domains using only a single model</li></ul><h3 id="8-3-Metrics"><a href="#8-3-Metrics" class="headerlink" title="8.3 Metrics"></a>8.3 Metrics</h3><ul><li>Human evaluation: AMT, MOS</li><li>Visualizing the generated  results</li><li>Use a model in the target domain to evaluate: FCN Scores(MNIST classifiers, VGG face classifier)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-GAN&quot;&gt;&lt;a href=&quot;#1-GAN&quot; class=&quot;headerlink&quot; title=&quot;1 GAN&quot;&gt;&lt;/a&gt;1 GAN&lt;/h2&gt;&lt;h3 id=&quot;1-1-Introduction&quot;&gt;&lt;a href=&quot;#1-1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1.1 Introduction&quot;&gt;&lt;/a&gt;1.1 Introduction&lt;/h3&gt;&lt;p&gt;To learn the generator’s distribution $p_g$ over data x, we define a prior on input noise variables $p_z(z)$, then represent a mapping to data space as $G(z; θ_g)$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $θ_g$. We also define a second multilayer perceptron $D(x; θ_d)$ that outputs a single scalar. $D(x)$ represents the probability that $x$ came from the data rather than $p_g$. We train $D$ to maximize the probability of assigning the correct label to both training examples and samples from $G$. We simultaneously train $G$ to minimize $log(1 - D(G(z)))$. In other words, $D$ and $G$ play the following two-player mini-max game with value function $V (G; D):$&lt;/p&gt;
&lt;p&gt;$$min_G max_DV (D; G) = E_x∼p_{data(x)}[log D(x)] + E_{z∼p_z(z)}[log(1 - D(G(z)))]$$&lt;/p&gt;
    
    </summary>
    
    
      <category term="GAN" scheme="https://n4a.github.io/tags/GAN/"/>
    
      <category term="Image Translation" scheme="https://n4a.github.io/tags/Image-Translation/"/>
    
      <category term="Cross Domain Learning" scheme="https://n4a.github.io/tags/Cross-Domain-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Capsule net</title>
    <link href="https://n4a.github.io/2017/11/09/Capsule-net/"/>
    <id>https://n4a.github.io/2017/11/09/Capsule-net/</id>
    <published>2017-11-09T08:08:07.000Z</published>
    <updated>2018-07-17T12:05:22.950Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Transforming-Auto-encoder-ICANN-2011"><a href="#1-Transforming-Auto-encoder-ICANN-2011" class="headerlink" title="1 Transforming Auto-encoder(ICANN 2011 )"></a>1 Transforming Auto-encoder(ICANN 2011 )</h2><h3 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h3><blockquote><p>Artificial neural networks should use local “capsules” that perform some quite complicated internal computations on their inputs and then encapsulate the results of these computations into a small vector of highly informative outputs. Each capsule learns to recognize an implicitly defined visual entity over a limited domain of viewing conditions and deformations and it outputs both the probability that the entity is present within its limited domain and a set of “instantiation parameters” that may include the precise pose, lighting and deformation of the visual entity relative to an implicitly defined canonical version of that entity. When the capsule is working properly, the probability of the visual entity being present is locally invariant – it does not change as the entity moves over the manifold of possible appearances within the limited domain covered by the capsule. The instantiation parameters, however, are “equivariant” – as the viewing conditions change and the entity moves over the appearance manifold, the instantiation parameters change by a corresponding amount because they are representing the intrinsic coordinates of the entity on the appearance manifold. </p></blockquote><a id="more"></a><h3 id="1-2-Details-about-a-image-shift-example"><a href="#1-2-Details-about-a-image-shift-example" class="headerlink" title="1.2 Details about a image shift example"></a>1.2 Details about a image shift example</h3><ol><li><p>Model graph</p><p><img src="/img/tae.PNG" alt="tae"></p></li><li><p>Core codes (use tensorflow)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, X_in, extra_in)</span>:</span></span><br><span class="line">  rec = tf.sigmoid(self.fc_layer(X_in, self.in_dim, self.r_dim,</span><br><span class="line">                                 <span class="string">'recog_layer_pre_act'</span>), <span class="string">'recog_layer'</span>)</span><br><span class="line">   </span><br><span class="line">  xy_vec = self.fc_layer(rec, self.r_dim, <span class="number">2</span>, <span class="string">'xy_prediction'</span>)</span><br><span class="line">  pro = tf.sigmoid(self.fc_layer(rec, self.r_dim, <span class="number">1</span>,</span><br><span class="line">                                 <span class="string">'probability_lin'</span>),<span class="string">'probability_prediction'</span>)</span><br><span class="line">  probability_vec = tf.tile(pro, (<span class="number">1</span>, self.in_dim))</span><br><span class="line">   </span><br><span class="line">  xy_extend = tf.add(xy_vec, extra_in)</span><br><span class="line">  gen = tf.sigmoid(self.fc_layer(xy_extend, <span class="number">2</span>, self.g_dim, <span class="string">'gen_pre_act'</span>), </span><br><span class="line">                   <span class="string">'gen_layer'</span>)</span><br><span class="line">   </span><br><span class="line">  out = self.fc_layer(gen, self.g_dim, self.in_dim, <span class="string">'out_prediction'</span>)</span><br><span class="line">   </span><br><span class="line">  <span class="keyword">return</span> tf.multiply(out, probability_vec)</span><br></pre></td></tr></table></figure></li><li><p>Experiment Result</p><p><img src="/img/experiment.PNG" alt="experiment"></p></li></ol><h3 id="1-3-Work-to-do"><a href="#1-3-Work-to-do" class="headerlink" title="1.3 Work to do"></a>1.3 Work to do</h3><p>​    Once pixel intensities have been converted into the outputs of a set of active, first-level capsules each of which produces an explicit representation of the pose of its visual entity, it is relatively easy to see how larger and more complex visual entities can be recognized by using <strong>agreements of the poses</strong> predicted by active, lower-level capsules. </p><p>​    <strong>Agreements of the poses example</strong>： If a capsule can learn to output the pose of its visual entity in a vector that is linearly related to the “natural” representations of pose used in computer graphics, there is a simple and highly selective test for whether the visual entities represented by two active capsules, A and B, have the right spatial relationship to activate a higher-level capsule, C. Suppose that the pose outputs of capsule A are represented by a matrix, TA, that specifies the coordinate transform between the canonical visual entity of A and the actual instantiation of that entity found by capsule A. If we multiply TA by the part-whole coordinate transform TAC that relates the canonical visual entity of A to the canonical visual entity of C, we get a prediction for TC. Similarly, we can use TB and TBC to get another prediction. If these predictions are a good match, the instantiations found by capsules A and B are in the right spatial relationship to activate capsule C and the average of the predictions tells us how the larger visual entity represented by C is transformed relative to the canonical visual entity of C. If, for example, A represents a mouth and B represents a nose, they can each make a prediction for the pose of the face. If these predictions agree, the mouth and nose must be in the right spatial relationship to form a face. An interesting property of this way of performing shape recognition is that the knowledge of part-whole relationships is viewpoint-invariant and is represented by weight matrices whereas the knowledge of the instantiation parameters of currently observed objects and their parts is viewpoint-equivariant and is represented by neural activities  </p><h2 id="2-Dynamic-Routing-Between-Capsules-NIPS-2017"><a href="#2-Dynamic-Routing-Between-Capsules-NIPS-2017" class="headerlink" title="2 Dynamic Routing Between Capsules(NIPS 2017)"></a>2 Dynamic Routing Between Capsules(NIPS 2017)</h2><h3 id="2-1-Introduction"><a href="#2-1-Introduction" class="headerlink" title="2.1 Introduction"></a>2.1 Introduction</h3><p>​    The previous paper said that: “Once pixel intensities have been converted into the outputs of a set of active, first-level capsules each of which produces an explicit representation of the pose of its visual entity, it is relatively easy to see how larger and more complex visual entities can be recognized by using <strong>agreements of the poses</strong> predicted by active, lower-level capsules. “ But it didn’t do the work to build a multilayer capsules network.</p><p>​    This paper give a implementation of the multilayer capsules network that do classification work on <strong>MNIST</strong> dataset. And it give a routing algorithm comparing to just the word <strong>agreements</strong>. It give a detail formulation to measure <strong>agreements</strong>. Then it use <strong>Margin Loss</strong> and <strong>Reconstruction</strong> as a regularization method </p><p>​    Besides, instead of  using common full connection layer to initialize the first-level capsules, it use convolutional neural network to do this work. The paper says: “Convolutional neural networks (CNNs) use translated replicas of learned feature detectors and this allows them to translate knowledge about good weight values acquired at one position in an image to other positions. This has proven extremely helpful in image interpretation. Even though we are replacing the scalar-output feature detectors of CNNs with vector-output capsules and max-pooling with routing-by-agreement, we would still like to replicate learned knowledge across space, so we make all but the last layer of capsules be convolutional. “ </p><h3 id="2-2-Model-detail"><a href="#2-2-Model-detail" class="headerlink" title="2.2 Model detail"></a>2.2 Model detail</h3><ol><li><p>Model graph</p><p><img src="/img/capsnet.PNG" alt="capsule net"></p></li><li><p>Core codes and algorithms</p><ol><li><p>First layer is a common convolutional layer</p></li><li><p>Second layer is PrimaryCaps layer.  But One can see ist as a Convolution layer with $$v_j = \frac{||s_j||^2}{1 +||s_j||^2 }\frac{s_j}{||s_j||}$$  as its block non-linearity.  Just as the following codes.(In the following codes, squash is a function to implement the former eq. )</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PrimaryCap</span><span class="params">(inputs, dim_vector, n_channels, kernel_size, strides, padding)</span>:</span></span><br><span class="line">    outputs = []</span><br><span class="line">    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size,</span><br><span class="line">                           strides=strides, padding=padding)(inputs)</span><br><span class="line">    outputs = layers.Reshape(target_shape=[<span class="number">-1</span>, dim_vector])(output)</span><br><span class="line">    <span class="keyword">return</span> layers.Lambda(squash)(outputs)</span><br></pre></td></tr></table></figure></li><li><p>In total PrimaryCapsules has [32; 6; 6] capsule outputs (each output is an 8D vector) . (shape is [32*6*6, 8]). The third layer is Digit layer. Between these two layers, the author use the routing algorithm as bellow.</p><p><img src="/img/routing.PNG" alt="routing"></p></li><li><p>Margin loss.</p><p><img src="/img/margin_loss.PNG" alt="margin losss"></p></li><li><p>Reconstruction as a regularization method.</p><p><img src="/img/reg.PNG" alt="regularization"></p><p>Fig. 3:</p><p> <img src="/img/reconstruction.PNG" alt="reconstruction"></p></li></ol></li><li><p>Experiment Result</p><ol><li>error rate on MNIST</li></ol><p><img src="/img/capsnet_exp.PNG" alt="experiment"></p><ol start="2"><li><p>What the individual dimensions of a capsule represent.</p><blockquote><p>Since we are passing the encoding of only one digit and zeroing out other digits, the dimensions of a digit capsule should learn to span the space of variations in the way digits of that class are instantiated. These variations include stroke thickness, skew and width. They also include digit-specific variations such as the length of the tail of a 2. We can see what the individual dimensions represent by making use of the decoder network. After computing the activity vector for the correct digit capsule, we can feed a perturbed version of this activity vector to the decoder network and see how the perturbation affects the reconstruction. Examples of these perturbations are shown in Fig. 4. We found that one dimension (out of 16) of the capsule almost always represents the width of the digit. While some dimensions represent combinations of global variations, there are other dimensions that represent variation in a localized part of the digit. For example, different dimensions are used for the length of the ascender of a 6 and the size of the loop </p></blockquote><p>Fig 4:</p><p><img src="/img/digit_d.PNG" alt="What the individual dimensions of a capsule represent"></p></li><li><p>Robustness to Affine Transformations </p><blockquote><p>To test the robustness of CapsNet to affine transformations we trained a CapsNet and a traditional convolutional network (with MaxPooling and DropOut) on a padded and translated MNIST training set, in which each example is an MNIST digit placed randomly on a black background of 40 × 40 pixels. We then tested this network on the affNIST4 data set, in which each example is an MNIST digit with a random small affine transformation.</p></blockquote><p>| Net\Accuracy | Expanded MNIST Test Set | Affined Test Set |<br>| :———-: | :———————: | :————–: |<br>|   CapsNet    |         99.23%          |        79        |<br>|     CNN      |          99.22          |        66        |</p></li><li><p>Segmenting highly overlapping digits</p><p><img src="/img/overlap.PNG" alt="overlap"></p><blockquote><p>Figure 5: Sample reconstructions of a CapsNet with 3 routing iterations on MultiMNIST test dataset. <strong>The two reconstructed digits are overlayed in green and red as the lower image</strong>. The upper image shows the input image. L:(l1; l2) represents the label for the two digits in the image and R:(r1; r2) represents the two digits used for reconstruction. The two right most columns show two examples with wrong classification reconstructed from the label and from the prediction (P). In (2; 8) example the model confuses 8 with a 7 and in (4; 9) it confuses 9 with 0. The other columns have correct classifications and show that model accounts for all the pixels while being able to assign one pixel to two digits in extremely difficult scenarios (column 1 - 4). Note that in dataset generation the pixel values are clipped at 1. The two columns with <strong>the (*) mark</strong> show reconstructions from a digit that is neither the label nor the prediction. </p></blockquote></li></ol></li></ol><h3 id="2-3-Drawbacks"><a href="#2-3-Drawbacks" class="headerlink" title="2.3 Drawbacks"></a>2.3 Drawbacks</h3><blockquote><p>One drawback of Capsules which it shares with generative models is that it likes to account for<br>everything in the image so it does better when it can model the clutter than when it just uses an<br>additional “orphan” category in the dynamic routing. In CIFAR-10, the backgrounds are much too<br>varied to model in a reasonable sized net which helps to account for the poorer performance. </p></blockquote><h2 id="3-Matrix-capsules-with-EM-routing-ICLR-2018-under-review"><a href="#3-Matrix-capsules-with-EM-routing-ICLR-2018-under-review" class="headerlink" title="3 Matrix capsules with EM routing  (ICLR 2018, under review)"></a>3 Matrix capsules with EM routing  (ICLR 2018, under review)</h2><h3 id="3-1-Introduction"><a href="#3-1-Introduction" class="headerlink" title="3.1 Introduction"></a>3.1 Introduction</h3><p>The system in previous paper has several deficiencies  as arguing in this paper:</p><ol><li>It uses the length of the pose vector to represent the probability that the entity represented by<br>a capsule is present. To keep the length less than 1 requires an unprincipled non-linearity<br>that prevents there from being any sensible objective function that is minimized by the<br>iterative routing procedure. </li><li>It uses the cosine of the angle between two pose vectors to measure their agreement. Unlike<br>the log variance of a Gaussian cluster, the cosine is not good at distinguishing between quite<br>good agreement and very good agreement </li><li>It uses a vector of length n rather than a matrix with n elements to represent a pose, so its<br>transformation matrices have n2 parameters rather than just n. </li></ol><h3 id="3-2-Model-detail"><a href="#3-2-Model-detail" class="headerlink" title="3.2 Model detail"></a>3.2 Model detail</h3><ol><li><p>Model Graph</p><p><img src="/img/em_capsnet.PNG" alt="model"></p></li><li><p>Gaussian mixture</p><p><img src="/img/EM_old_faithful.gif" alt="gaussian mixture"></p></li><li><p>EM routing</p><p><img src="/img/em_routing.PNG" alt="em routing"></p></li><li><p>Experiment Result: ADVERSARIAL ROBUSTNESS </p><p><img src="/img/ad_re.PNG" alt="adversarial"></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Transforming-Auto-encoder-ICANN-2011&quot;&gt;&lt;a href=&quot;#1-Transforming-Auto-encoder-ICANN-2011&quot; class=&quot;headerlink&quot; title=&quot;1 Transforming Auto-encoder(ICANN 2011 )&quot;&gt;&lt;/a&gt;1 Transforming Auto-encoder(ICANN 2011 )&lt;/h2&gt;&lt;h3 id=&quot;1-1-Introduction&quot;&gt;&lt;a href=&quot;#1-1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1.1 Introduction&quot;&gt;&lt;/a&gt;1.1 Introduction&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;Artificial neural networks should use local “capsules” that perform some quite complicated internal computations on their inputs and then encapsulate the results of these computations into a small vector of highly informative outputs. Each capsule learns to recognize an implicitly defined visual entity over a limited domain of viewing conditions and deformations and it outputs both the probability that the entity is present within its limited domain and a set of “instantiation parameters” that may include the precise pose, lighting and deformation of the visual entity relative to an implicitly defined canonical version of that entity. When the capsule is working properly, the probability of the visual entity being present is locally invariant – it does not change as the entity moves over the manifold of possible appearances within the limited domain covered by the capsule. The instantiation parameters, however, are “equivariant” – as the viewing conditions change and the entity moves over the appearance manifold, the instantiation parameters change by a corresponding amount because they are representing the intrinsic coordinates of the entity on the appearance manifold. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="CNN" scheme="https://n4a.github.io/tags/CNN/"/>
    
      <category term="Capsule Net" scheme="https://n4a.github.io/tags/Capsule-Net/"/>
    
      <category term="Deep Learning" scheme="https://n4a.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>李航_统计学习方法</title>
    <link href="https://n4a.github.io/2017/10/21/%E6%9D%8E%E8%88%AA-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    <id>https://n4a.github.io/2017/10/21/李航-统计学习方法/</id>
    <published>2017-10-21T08:21:54.000Z</published>
    <updated>2018-07-17T12:11:32.996Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-统计学习概述"><a href="#1-统计学习概述" class="headerlink" title="1  统计学习概述"></a>1  统计学习概述</h3><p>​    统计学习三要素：模型、策略、算法。</p><h4 id="1-1-模型"><a href="#1-1-模型" class="headerlink" title="1.1 模型"></a>1.1 模型</h4><p>​    模型就是所要学习的条件概率分布或者决策函数</p><h4 id="1-2-策略"><a href="#1-2-策略" class="headerlink" title="1.2 策略"></a>1.2 策略</h4><p>​    策略即是决定用什么样的准则学习或选择最优的模型。 </p><ol><li><p>损失函数（loss function）</p><p><img src="/img/lihang/loss%20function.PNG" alt="loss function"></p></li><li><p>经验风险最小化和结构风险最小化</p><ol><li>empirical risk minimization，ERM：其理论依据是大数定理。但是通常情况下训练数据较少并不满足大数定理的要求，容易发生过拟合现象。</li><li>structural risk minimization，SRM：为了防止过拟合现象，SRM增加正则化项，对模型的复杂度进行约束，要求模型复杂度较小。</li></ol></li></ol><h4 id="1-3-算法"><a href="#1-3-算法" class="headerlink" title="1.3 算法"></a>1.3 算法</h4><p>​    算法是指学习模型的具体算法，例如BP算法、EM算法等。</p><h4 id="1-4-模型评估与模型选择"><a href="#1-4-模型评估与模型选择" class="headerlink" title="1.4 模型评估与模型选择"></a>1.4 模型评估与模型选择</h4><p>​    训练误差、测试误差、交叉验证。</p><p>​    生成模型、判别模型。</p><a id="more"></a><h3 id="2-感知机模型"><a href="#2-感知机模型" class="headerlink" title="2  感知机模型"></a>2  感知机模型</h3><h4 id="2-1-模型"><a href="#2-1-模型" class="headerlink" title="2.1 模型"></a>2.1 模型</h4><p>$$<br>f(x) = sign(wx + b) \tag{1.1}<br>$$</p><p>$$<br>sign(x) = \begin{cases}<br>        1,  &amp; \text{if x &gt;= 0} \<br>        -1, &amp; \text{if x &lt; 0}<br>        \end{cases} \tag{1.2}<br>$$</p><p>​    其中所要训练的参数为w和b，感知机模型是一种简单的线性分类模型，属于判别模型。</p><h4 id="2-2-策略"><a href="#2-2-策略" class="headerlink" title="2.2 策略"></a>2.2 策略</h4><p>定义 loss function：</p><p><img src="/img/lihang/perceptron.PNG" alt="感知机模型亏损函数"></p><h4 id="2-3-算法"><a href="#2-3-算法" class="headerlink" title="2.3 算法"></a>2.3 算法</h4><p>SGD算法：</p><p><img src="/img/lihang/perceptron%20sgd.PNG" alt="随机梯度下降算法"></p><p>3  KNN模型</p><h4 id="3-1-模型"><a href="#3-1-模型" class="headerlink" title="3.1 模型"></a>3.1 模型</h4><p><img src="/img/lihang/knn.PNG" alt="KNN模型"></p><h4 id="3-2-策略"><a href="#3-2-策略" class="headerlink" title="3.2 策略"></a>3.2 策略</h4><p>KNN模型是一个只需正向统计的过程，没有待训练参数，也不需要定义 loss function 。但是在统计前要决定策略三要素：距离度量方法、k值选择和分类决策方法。</p><ol><li>距离度量方法： 欧式距离、$L_p$距离、曼哈顿距离等。</li><li>k值选择：k值越小对临近数据点越敏感，模型越复杂，越容易发生过拟合；k值越大，模型越简单，不易发生过拟合，但是模型能力若，预测能力差。</li><li>分类决策方法：多数表决，平均值方法等。</li></ol><h4 id="3-3-算法"><a href="#3-3-算法" class="headerlink" title="3.3 算法"></a>3.3 算法</h4><p>最简单的算法就是线性搜索所有数据集，找出K个最近邻。其搜索复杂度为O(n)</p><p>优化的算法如<strong>kd树算法</strong>，搜索复杂度为 O(log n)</p><ol><li><p>构造kd树</p><p><img src="/img/lihang/kdt1.PNG" alt="构造kd树"></p><p><img src="/img/lihang/kdt2.PNG" alt="构造kd树"></p></li><li><p>利用kd树搜索最近邻</p><p><img src="/img/lihang/kdt3.png" alt="kd树搜索"></p><p><img src="/img/lihang/kdt4.png" alt="利用kd树搜索"></p></li></ol><h3 id="4-朴素贝叶斯方法"><a href="#4-朴素贝叶斯方法" class="headerlink" title="4 朴素贝叶斯方法"></a>4 朴素贝叶斯方法</h3><h4 id="4-1-模型"><a href="#4-1-模型" class="headerlink" title="4.1 模型"></a>4.1 模型</h4><p>朴素贝叶斯法通过训练数据来学习联合概率分布P(X,Y)。根据$P(X,Y) = P(X|Y)P(Y)$，所以如下图所示，我们可以通过学习Y的先验概率分布和X的条件概率分布来确定X与Y的联合分布。</p><p><img src="/img/lihang/b1.PNG" alt="Bayesian"></p><p>之后根据前面所说的大数定理，训练数据满足这种分布，之后的新数据也当满足这种分布。至于训练数据不足的问题，也可套用结构风险最小的理论。</p><p>但是求解P(X|Y)的发杂度非常高，设X为n维向量，$x_j$有$S_j$个不同的取值，Y有K个不同的取值。那么统计P(X|Y)的复杂度为$K\Pi_{j=1}^{n}Sj$.</p><p>为降低复杂度，朴素贝叶斯法做了X的每个维度相互独立的假设，那么条件概率分布变为：<br>$$<br>P(X=x|Y=y) = P(X^1=x^1,…,X^n = x^n | Y=c_k)  \<br>= \Pi_{j=1}^nP(X^j = x^j|Y=c_k) \<br>k = 1,2,…,K \tag{4.3}<br>$$<br>这样使得模型变得简单，可计算。但是其效果就要差一点。</p><h4 id="4-2-策略"><a href="#4-2-策略" class="headerlink" title="4.2 策略"></a>4.2 策略</h4><p>朴素贝叶斯的策略被称为后验概率最大化策略，后面会说明这一策略和经验风险最小化策略是等价的。</p><ol><li><p>后验概率最大化策略</p><p><img src="/img/lihang/posterior.PNG" alt="posterior"></p><p>​</p></li><li><p>后验概率最大等价于经验风险最小</p><p>为了证明这个问题，首先定义0-1损失函数如下：<br>$$<br>L(Y,F(X)) = \begin{cases}<br>1, &amp; Y \neq f(X) \<br>-1, &amp; Y = f(X)<br>\end{cases}<br>$$<br>式中f(X)式分类决策函数. 这时，期望风险函数为<br>$$<br>R_{exp} = E[L(Y,f(X))]<br>$$<br>然后利用条件期望全期望公式得：<br>$$<br>R_{exp}(f) = E_x\sum_{k=1}^K[L(c_k, f(X))]P(c_k|X)<br>$$<br>因为P(X=x)是确定的，所以只需对X=x的情况下逐个取最小化，如下：<br>$$<br>\begin{align}<br>f(x) &amp; = arg {min}<em>{y \in Y}\sum</em>{k=1}^KL(c_k,y)P(c_kX=x) \\<br> &amp; = arg {min}<em>{y \in Y} \sum</em>{k=1}^KP(y \neq c_k|X=x) \\<br> &amp; = arg{min}(1-P(y=c_k|X=x)) \\<br> &amp; = arg {max}_{y \in Y}P(y=c_k|X=x)<br>\end{align}<br>$$<br>这样经验风险最小化就和前面的后验概率最大化策略目标相同了</p></li></ol><h4 id="4-3-算法"><a href="#4-3-算法" class="headerlink" title="4.3 算法"></a>4.3 算法</h4><p><img src="/img/lihang/bayesian%20algorithm.PNG" alt="bayes algorithm"></p><h3 id="5-决策树模型"><a href="#5-决策树模型" class="headerlink" title="5 决策树模型"></a>5 决策树模型</h3><h4 id="5-1-模型"><a href="#5-1-模型" class="headerlink" title="5.1 模型"></a>5.1 模型</h4><p><img src="/img/lihang/decision_tree_model.PNG" alt="decision_tree"></p><h4 id="5-2-策略"><a href="#5-2-策略" class="headerlink" title="5.2 策略"></a>5.2 策略</h4><p>决策树的学习策略是损失函数最小的策略，但是这一策略在学习算法中不会明显的体现出来。具体的学习算法只会要求对于训练数据，分类尽可能正确。这也就蕴含了损失函数最小的思想。</p><p>即是损失函数已经最小了，决策树学习还要求树的结构最优，即树的层数要尽量少。</p><h4 id="5-3-算法"><a href="#5-3-算法" class="headerlink" title="5.3 算法"></a>5.3 算法</h4><p>从所有的二叉树中找出结构最优的的树是NP难度的问题。所以具体的算法都是启发式的算法，从根节点开始先找到分类最优的分类特征，然后一次递归地执行下去。</p><p>常用的算法有ID3，C4.5 与 CART。这些算法基本都基于信息熵和信息增益的理论。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-统计学习概述&quot;&gt;&lt;a href=&quot;#1-统计学习概述&quot; class=&quot;headerlink&quot; title=&quot;1  统计学习概述&quot;&gt;&lt;/a&gt;1  统计学习概述&lt;/h3&gt;&lt;p&gt;​    统计学习三要素：模型、策略、算法。&lt;/p&gt;
&lt;h4 id=&quot;1-1-模型&quot;&gt;&lt;a href=&quot;#1-1-模型&quot; class=&quot;headerlink&quot; title=&quot;1.1 模型&quot;&gt;&lt;/a&gt;1.1 模型&lt;/h4&gt;&lt;p&gt;​    模型就是所要学习的条件概率分布或者决策函数&lt;/p&gt;
&lt;h4 id=&quot;1-2-策略&quot;&gt;&lt;a href=&quot;#1-2-策略&quot; class=&quot;headerlink&quot; title=&quot;1.2 策略&quot;&gt;&lt;/a&gt;1.2 策略&lt;/h4&gt;&lt;p&gt;​    策略即是决定用什么样的准则学习或选择最优的模型。 &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;损失函数（loss function）&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/lihang/loss%20function.PNG&quot; alt=&quot;loss function&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;经验风险最小化和结构风险最小化&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;empirical risk minimization，ERM：其理论依据是大数定理。但是通常情况下训练数据较少并不满足大数定理的要求，容易发生过拟合现象。&lt;/li&gt;
&lt;li&gt;structural risk minimization，SRM：为了防止过拟合现象，SRM增加正则化项，对模型的复杂度进行约束，要求模型复杂度较小。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&quot;1-3-算法&quot;&gt;&lt;a href=&quot;#1-3-算法&quot; class=&quot;headerlink&quot; title=&quot;1.3 算法&quot;&gt;&lt;/a&gt;1.3 算法&lt;/h4&gt;&lt;p&gt;​    算法是指学习模型的具体算法，例如BP算法、EM算法等。&lt;/p&gt;
&lt;h4 id=&quot;1-4-模型评估与模型选择&quot;&gt;&lt;a href=&quot;#1-4-模型评估与模型选择&quot; class=&quot;headerlink&quot; title=&quot;1.4 模型评估与模型选择&quot;&gt;&lt;/a&gt;1.4 模型评估与模型选择&lt;/h4&gt;&lt;p&gt;​    训练误差、测试误差、交叉验证。&lt;/p&gt;
&lt;p&gt;​    生成模型、判别模型。&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学习" scheme="https://n4a.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec</title>
    <link href="https://n4a.github.io/2017/10/21/Memo-word2vec/"/>
    <id>https://n4a.github.io/2017/10/21/Memo-word2vec/</id>
    <published>2017-10-21T07:42:58.000Z</published>
    <updated>2018-07-17T08:10:37.360Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>最近看了一些Word2Vec的一些相关文章，主要分为两类。一是有关于Word2Vec的发展，主要是以下4篇文章起到奠基性的作用</p><p>我看了一些Word2Vec的一些相关文章，主要分为两类。一是有关于Word2Vec的发展，主要是以下4篇文章起到奠基性的作用</p><ol><li><p>A Neural Probabilistic Language Model.2003 (NNLM)</p></li><li><p>Recurrent neural network based language model.2010 (RNNLM)</p></li><li><p>Distributed Representations of Words and Phrases and their Compositionality.2013 (Skip Gram Model and CBOW)</p></li><li><p>Efficient Estimation of Word Representations in Vector Space.2013 (Skip Gram Model and CBOW)</p></li></ol><p>二是关于Word2Vec应用的一些文章，一些有趣的idea像是如下的文章</p><ol><li>Linguistic Regularities in Continuous Space Word Representations.2013</li><li>Exploiting Similarities among Languages for Machine Translation.2013</li><li>Distributed Representations of Sentences and Documents.2014</li></ol><a id="more"></a> <h3 id="Development"><a href="#Development" class="headerlink" title="Development"></a>Development</h3><h4 id="NPLM-A-Neural-Probabilistic-Language-Model-2003"><a href="#NPLM-A-Neural-Probabilistic-Language-Model-2003" class="headerlink" title="NPLM: A Neural Probabilistic Language Model.2003"></a>NPLM: A Neural Probabilistic Language Model.2003</h4><p>这篇文章主要解决在此之前的自然语言模型是统计语言模型和基于统计语言模型n-gram模型的维度灾难问题。</p><p><img src="/img/SLM.PNG" alt="statistical language model "></p><p>统计语言模型的基本想法就是对于一句话，在给定前几个词的情况下，统计出现下一个词的概率。这样一句话的出现概率就是第一个词出现的概率$P(W_1)$乘上在第一个词给定的情况下出现第二个词的概率$P(W_2|W_1)$, 依此类推，一句话的概率就是上图第一行的联合条件概率乘积。</p><p>N-gram模型就是假设一个词出现的概率只考察前后该词前后n个词，以此来降低复杂度。</p><p>这些模型的问题就是复杂度非常高，例如：</p><p><img src="/img/CoD.PNG" alt="curse of dimension"></p><p>上图的<strong>free parameters</strong>就是指前面语言模型的各个概率P. 统计语言模型就是要统计所有文本将所有概率P确定下来。</p><p>文章的作者要解决这一问题，采用distributed vectors来表示每一个词，一句话的上下文语境用训练好的vector的值和网络参数来表达。如下：</p><p><img src="/img/feature vector.PNG" alt="feature vector"></p><p>作者给出的网络结构如下：</p><p><img src="/img/NNLM.PNG" alt="NNLM"></p><p>如图，首先有一个全局的矩阵C,通过C将一个词w转换为向量C(w)。然后用这些向量作为一个三层神经网络的输入，最终训练出参数和所有词向量的矩阵C。</p><p>作者首次使用神经网络来表示自然语言模型，大大降低了计算复杂度（复杂度在后面统一比较）。作者实际上已经使用了词向量但是没有发现这些向量蕴含的语义和语法内容，只是用它来作为自然语言模型的工具。</p><h4 id="RNNLM-Recurrent-neural-network-based-language-model-2010"><a href="#RNNLM-Recurrent-neural-network-based-language-model-2010" class="headerlink" title="RNNLM:Recurrent neural network based language model.2010"></a>RNNLM:Recurrent neural network based language model.2010</h4><p>NNLM有一个问题就是对于一个词，训练的时候它的上下文只取该词前面的n个词，如果n过大，训练复杂度增大；n过小，覆盖的上下文不够。本文作者就利用RNN的特性来解决这一问题。</p><p><img src="/img/RNNLM.PNG" alt="RNN LM"></p><p>如上图，RNNLM中将隐藏层视为context层，每一次迭代中，输入由新的输入和前一次的context层共同组成。这样上下文的宽度n不用很大，也可以利用到距离当前词足够远的词带来的影响。</p><p>但是本文作者同样是没有注意到词向量蕴含的语义和语法内容</p><h4 id="Skip-Gram-Model-and-CBOW"><a href="#Skip-Gram-Model-and-CBOW" class="headerlink" title="Skip Gram Model and CBOW"></a>Skip Gram Model and CBOW</h4><p>“Distributed Representations of Words and Phrases and their Compositionality”和“Efficient Estimation of Word Representations in Vector Space”两篇文章提出了Skip Gram Model and CBOW两个模型。</p><p>CBOW模型：</p><p><img src="/img/CBOW.PNG" alt="CBOW"></p><p>CBOW模型相比于前两个模型，去掉了隐藏层。输出不是线性的softmax，而是基于huffman树的softmax，这样首先使得输出层复杂度由N降到log(N)，其次很好的应用了huffman树的聚类效果。</p><p>Skip Gram Model与CBOW类似，只是CBOW是由当前词的上下文来推导出当前词汇，而Skip Gram Model则是反过来。</p><p>这两个模型首先是降低了计算复杂度，如下图比较：</p><p><img src="/img/complexity.PNG" alt="complexity"></p><p>按照文章的说法</p><blockquote><p>This makes the training extremely efficient: an optimized single-machine implementation can train on more than 100 billion words in one day </p></blockquote><p><strong>此外最重要的是，作者首次察觉到词向量蕴含了丰富的语法和语义的含义。比如</strong></p><blockquote><p> <strong>vec(“Madrid”) - vec(“Spain”) + vec(“France”) is closer to vec(“Paris”) . </strong></p></blockquote><p><strong>文章中用了大量篇幅和实现表现词向量的良好特性</strong></p><h3 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h3><p>然后还有些应用词向量的文章。</p><h4 id="Exploiting-Similarities-among-Languages-for-Machine-Translation"><a href="#Exploiting-Similarities-among-Languages-for-Machine-Translation" class="headerlink" title="Exploiting Similarities among Languages for Machine Translation"></a>Exploiting Similarities among Languages for Machine Translation</h4><p>文章的想法非常简单。在一种语言中一个词的词向量为$X_i$，另一种语言中其对应的词的词向量为$Z_i$,作者做了一些实验发现这写词在各自的语言空间中的相对位置关系有一定的相似性，如下图：</p><p><img src="/img/translation2.PNG" alt="translation"></p><p>所以作者认为，只要训练一个转移矩阵W，使得$X_i$经过变换$WX_i$转换到与$Z_i$接近的位置，再转换成对应的词，这样就完成了翻译，训练目标如下图：</p><p><img src="/img/translation.PNG" alt="translation"></p><p>作者想法很新颖，但是模型结构非常简单，训练的过程中只考虑到词，没有融入整句话的语义概念。翻译的效果比不上之前的翻译模型，只是开阔了一种新的思路。</p><h4 id="Distributed-Representations-of-Sentences-and-Documents"><a href="#Distributed-Representations-of-Sentences-and-Documents" class="headerlink" title="Distributed Representations of Sentences and Documents"></a>Distributed Representations of Sentences and Documents</h4><p>这一篇文章进一步扩展词向量，提出一种方法来训练句子和文章的向量。</p><p><img src="/img/pvdm.PNG" alt="pv dm"></p><p>如图，作者的想法也非常简单，在原有的CBOW模型上，作者在输入层额外添加了一个paragraph vector。这个向量是由段落决定的，文章的每一段对应一个向量，在训练过程中，同一段落内部使用同一个向量。这一个向量就代表着该段落的context and semantic.</p><p>这是一个非常简单但是巧妙的想法，效果也相当不错。作者做了一个实验来来评估模型，实验是分析一段评论是positive or negative，数据集是Stanford Sentiment Treebank Dataset。实验结果如下，错误率要低于之前的模型。</p><p><img src="/img/pvdm_exam.PNG" alt="pvdm experiment"></p><p>这一篇训练sentence vector还可以应用到前一篇翻译的工作中，可以弥补其没有融入整句话的语义概念的不足之处。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;最近看了一些Word2Vec的一些相关文章，主要分为两类。一是有关于Word2Vec的发展，主要是以下4篇文章起到奠基性的作用&lt;/p&gt;
&lt;p&gt;我看了一些Word2Vec的一些相关文章，主要分为两类。一是有关于Word2Vec的发展，主要是以下4篇文章起到奠基性的作用&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A Neural Probabilistic Language Model.2003 (NNLM)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Recurrent neural network based language model.2010 (RNNLM)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distributed Representations of Words and Phrases and their Compositionality.2013 (Skip Gram Model and CBOW)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Efficient Estimation of Word Representations in Vector Space.2013 (Skip Gram Model and CBOW)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;二是关于Word2Vec应用的一些文章，一些有趣的idea像是如下的文章&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Linguistic Regularities in Continuous Space Word Representations.2013&lt;/li&gt;
&lt;li&gt;Exploiting Similarities among Languages for Machine Translation.2013&lt;/li&gt;
&lt;li&gt;Distributed Representations of Sentences and Documents.2014&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="World2vec" scheme="https://n4a.github.io/tags/World2vec/"/>
    
      <category term="NPLM" scheme="https://n4a.github.io/tags/NPLM/"/>
    
      <category term="词向量" scheme="https://n4a.github.io/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>Flask实践_Flask+jinja2+bootstrap+sqlite3+sqlalchemy构建web基础</title>
    <link href="https://n4a.github.io/2017/05/28/Flask%E5%AE%9E%E8%B7%B5-Flask-jinja2-bootstrap-sqlite3-sqlalchemy%E6%9E%84%E5%BB%BAweb%E5%9F%BA%E7%A1%80/"/>
    <id>https://n4a.github.io/2017/05/28/Flask实践-Flask-jinja2-bootstrap-sqlite3-sqlalchemy构建web基础/</id>
    <published>2017-05-28T13:52:40.000Z</published>
    <updated>2018-07-17T14:14:48.849Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/N4A/FlaskWebFramework" target="_blank" rel="noopener">source codes</a></p><a id="more"></a><h2 id="1-route-路由"><a href="#1-route-路由" class="headerlink" title="1 route 路由"></a>1 route 路由</h2><ol><li><p>语法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通路由</span></span><br><span class="line"><span class="meta">@app.route('/')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'Hello World!!!'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 路由传参</span></span><br><span class="line"><span class="meta">@app.route('/users/&lt;name&gt;')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">user</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'user: '</span> + name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定参数类型， 参考下面2</span></span><br><span class="line"><span class="meta">@app.route('/articles/&lt;string:article_name&gt;')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">article</span><span class="params">(article_name)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'article: '</span> + article_name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定 HTTP Methods  参考下面3</span></span><br><span class="line"><span class="meta">@app.route('/login', methods=['GET', 'POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> request.method == <span class="string">'POST'</span>:</span><br><span class="line">        do_the_login()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        show_the_login_form()</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 获取表单数据</span></span><br><span class="line"><span class="meta">@app.route('/search', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'key word: '</span> + request.form.get(<span class="string">'search_key'</span>) + <span class="string">'&lt;br&gt; Find nothing.'</span></span><br></pre></td></tr></table></figure></li><li><p>The following converters exist:</p><p>| string | accepts any text without a slash (the default) |<br>| —— | ———————————————- |<br>| int    | accepts integers                               |<br>| float  | like <code>int</code> but for floating point values       |<br>| path   | like the default but also accepts slashes      |<br>| any    | matches one of the items provided              |<br>| uuid   | accepts UUID strings                           |</p></li><li><p>Http Methods</p><p><em>GET</em></p><p>The browser tells the server to just <em>get</em> the information stored on that page and send it. This is probably the most common method.</p><p><em>HEAD</em></p><p>The browser tells the server to get the information, but it is only interested in the <em>headers</em>, not the content of the page. An application is supposed to handle that as if a <em>GET</em> request was received but to not deliver the actual content. In Flask you don’t have to deal with that at all, the underlying Werkzeug library handles that for you.</p><p><em>POST</em></p><p>The browser tells the server that it wants to <em>post</em> some new information to that URL and that the server must ensure the data is stored and only stored once. This is how HTML forms usually transmit data to the server.</p><p><em>PUT</em></p><p>Similar to <em>POST</em> but the server might trigger the store procedure multiple times by overwriting the old values more than once. Now you might be asking why this is useful, but there are some good reasons to do it this way. Consider that the connection is lost during transmission: in this situation a system between the browser and the server might receive the request safely a second time without breaking things. With <em>POST</em> that would not be possible because it must only be triggered once.</p><p><em>DELETE</em></p><p>Remove the information at the given location.</p><p><em>OPTIONS</em></p><p>Provides a quick way for a client to figure out which methods are supported by this URL. Starting with Flask 0.6, this is implemented for you automatically.</p></li></ol><h2 id="2-static-通过url直接访问静态文件"><a href="#2-static-通过url直接访问静态文件" class="headerlink" title="2 static: 通过url直接访问静态文件"></a>2 static: 通过url直接访问静态文件</h2><p>静态文件直接放在static目录下，通过url: <strong>static/\&lt;filename></strong>即可访问</p><h2 id="3-模板继承-使用模板，模板语法参考jinja2"><a href="#3-模板继承-使用模板，模板语法参考jinja2" class="headerlink" title="3 模板继承: 使用模板，模板语法参考jinja2"></a>3 模板继承: 使用模板，模板语法参考jinja2</h2><ol><li><p>base template: 申明block，子模版可以通过block复写父模板对应block下的内容，如下</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!doctype html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    &#123;% block head %&#125;</span><br><span class="line">        <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">href</span>=<span class="string">"&#123;&#123; url_for('static', filename='style.css') &#125;&#125;"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">href</span>=<span class="string">"http://cdn.static.runoob.com/libs/bootstrap/3.3.7/css/bootstrap.min.css"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"http://cdn.static.runoob.com/libs/bootstrap/3.3.7/js/bootstrap.min.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">&#123;#        title block#&#125;</span><br><span class="line">        <span class="tag">&lt;<span class="name">title</span>&gt;</span>&#123;% block title %&#125;&#123;% endblock %&#125;<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    &#123;% endblock %&#125;</span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line">&#123;#    navigation bar#&#125;</span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"nav"</span>&gt;</span>&#123;% block nav %&#125; &#123;% endblock %&#125;<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">&#123;#    main content block#&#125;</span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"content"</span>&gt;</span>&#123;% block content %&#125;&#123;% endblock %&#125;<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">&#123;#    footer block#&#125;</span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"footer"</span> <span class="attr">class</span>=<span class="string">"bottom footer text-center"</span>&gt;</span></span><br><span class="line">      &#123;% block footer %&#125;</span><br><span class="line">        <span class="tag">&lt;<span class="name">br</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">h3</span>&gt;</span>&amp;copy; Copyright 2017 by <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.github.com/N4A/"</span>&gt;</span>N4A<span class="tag">&lt;/<span class="name">a</span>&gt;</span>.<span class="tag">&lt;/<span class="name">h3</span>&gt;</span></span><br><span class="line">      &#123;% endblock %&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>该模板将页面大致分为几个区域，确定了页面的整体框架。</p></li><li><p>子模版：static_nav_bar.html</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&#123;% extends "layout/base.html" %&#125;</span><br><span class="line">&#123;% block title %&#125;static nav bar&#123;% endblock %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% block nav %&#125;</span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">nav</span> <span class="attr">class</span>=<span class="string">"navbar navbar-default navbar-static-top"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"container-fluid"</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Brand and toggle get grouped for better mobile display --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"navbar-header"</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">button</span> <span class="attr">type</span>=<span class="string">"button"</span> <span class="attr">class</span>=<span class="string">"navbar-toggle collapsed"</span> <span class="attr">data-toggle</span>=<span class="string">"collapse"</span> <span class="attr">data-target</span>=<span class="string">"#bs-example-navbar-collapse-1"</span> <span class="attr">aria-expanded</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"sr-only"</span>&gt;</span>Toggle navigation<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"icon-bar"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"icon-bar"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"icon-bar"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">"navbar-brand"</span> <span class="attr">href</span>=<span class="string">"/"</span>&gt;</span>Home<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- Collect the nav links, forms, and other content for toggling --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"collapse navbar-collapse"</span> <span class="attr">id</span>=<span class="string">"bs-example-navbar-collapse-1"</span>&gt;</span></span><br><span class="line">            &#123;% block nav_content %&#125;</span><br><span class="line"></span><br><span class="line">            &#123;% endblock %&#125;</span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span><span class="comment">&lt;!-- /.navbar-collapse --&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">div</span>&gt;</span><span class="comment">&lt;!-- /.container-fluid --&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">nav</span>&gt;</span></span><br><span class="line"></span><br><span class="line">&#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><p>注意该模板继承上面的模板，复写了 block nav 块，确定 navigation bar 的样式，并且留出 block nav_content 供使用者决定内容。这样我们就可以确定一个基本的带响应式导航栏的页面样式。而他们就可以不断被复用。下面是一个列子。</p></li><li><p>my_nav_bar.html</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#123;% extends 'layout/static_nav_bar.html' %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% block nav_content %&#125;</span><br><span class="line">     <span class="tag">&lt;<span class="name">ul</span> <span class="attr">class</span>=<span class="string">"nav navbar-nav"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"https://github.com/N4A/FlaskWebFramework/tree/master/docs"</span>&gt;</span>Documentation<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"https://github.com/N4A/FlaskWebFramework"</span>&gt;</span>Download<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"dropdown"</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"#"</span> <span class="attr">class</span>=<span class="string">"dropdown-toggle"</span> <span class="attr">data-toggle</span>=<span class="string">"dropdown"</span> <span class="attr">role</span>=<span class="string">"button"</span> <span class="attr">aria-haspopup</span>=<span class="string">"true"</span> <span class="attr">aria-expanded</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">              Bootstrap Examples</span><br><span class="line">          <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"caret"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">ul</span> <span class="attr">class</span>=<span class="string">"dropdown-menu"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/examples/grid"</span>&gt;</span>Grid Example<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/examples/theme"</span>&gt;</span>Theme Example<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">"/search"</span> <span class="attr">method</span>=<span class="string">"post"</span> <span class="attr">class</span>=<span class="string">"navbar-form navbar-right"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"form-group"</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"search_key"</span> <span class="attr">class</span>=<span class="string">"form-control"</span> <span class="attr">placeholder</span>=<span class="string">"Search"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">button</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">class</span>=<span class="string">"btn btn-default"</span>&gt;</span>Submit<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line">&#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><p>该模板继承上面的模板，确定了一个具体的导航栏。</p></li></ol><h2 id="4-使用sqllite3"><a href="#4-使用sqllite3" class="headerlink" title="4 使用sqllite3"></a>4 使用sqllite3</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> g</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">DATABASE = <span class="string">'/path/to/database.db'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">connect_db</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment"># 这里使用相对路径可能会失效，虽然是官网样例，我试验的时候就失败了</span></span><br><span class="line">    <span class="comment"># return sqlite3.connect(DATABASE)</span></span><br><span class="line">    <span class="comment"># 使用绝对路径</span></span><br><span class="line">PROJECT_ROOT = os.path.dirname(os.path.realpath(__file__))<span class="comment">#当前文件目录</span></span><br><span class="line">AB_DATABASE = os.path.join(PROJECT_ROOT, DATABASE)</span><br><span class="line">    <span class="keyword">return</span> sqlite3.connect(DATABASE)</span><br><span class="line">  </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 该函数会在一个请求开始前执行，</span></span><br><span class="line"><span class="comment"># 这样在任何一个请求里都可以直接使用g.db访问数据库</span></span><br><span class="line"><span class="meta">@app.before_request</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">before_request</span><span class="params">()</span>:</span></span><br><span class="line">    g.db = connect_db()</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.teardown_request</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">teardown_request</span><span class="params">(exception)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> hasattr(g, <span class="string">'db'</span>):</span><br><span class="line">        g.db.close()</span><br></pre></td></tr></table></figure><h2 id="5-使用ORM框架：-SQLAlchemy"><a href="#5-使用ORM框架：-SQLAlchemy" class="headerlink" title="5 使用ORM框架： SQLAlchemy"></a>5 使用ORM框架： SQLAlchemy</h2><p><a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/0014021031294178f993c85204e4d1b81ab032070641ce5000" target="_blank" rel="noopener">参考连接</a></p><ol><li><p>声明model</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2017/5/28 13:05</span></span><br><span class="line"><span class="comment"># @Author  : duocai</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入:</span></span><br><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> Column, String</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.ext.declarative <span class="keyword">import</span> declarative_base</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建对象的基类:</span></span><br><span class="line">Base = declarative_base()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义User对象:</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span><span class="params">(Base)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 表的名字:</span></span><br><span class="line"></span><br><span class="line">    tablename = <span class="string">'users'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 表的结构:</span></span><br><span class="line">    id = Column(String(<span class="number">20</span>), primary_key=<span class="keyword">True</span>)</span><br><span class="line">    name = Column(String(<span class="number">20</span>))</span><br><span class="line">    passwd = Column(String(<span class="number">20</span>))</span><br></pre></td></tr></table></figure></li><li><p>使用SQLAlchemy连接数据库和使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## dbu.py</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">get_sesion</span><span class="params">(url=sqlalchemy_db)</span>:</span></span><br><span class="line">       <span class="comment"># 初始化数据库连接:</span></span><br><span class="line">       engine = create_engine(url)</span><br><span class="line">       <span class="comment"># 创建DBSession类型:</span></span><br><span class="line">       <span class="keyword">return</span> sessionmaker(bind=engine)()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="comment"># test db</span></span><br><span class="line">   <span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">       <span class="comment"># init_db,!!!!! delete the old one and create the new one</span></span><br><span class="line">       init_db() <span class="comment">##这里就创建一个数据库，详见下面源码</span></span><br><span class="line">       print(<span class="string">'create db: ok'</span>)</span><br><span class="line">       <span class="comment"># test sqlalchemy</span></span><br><span class="line">       sess = get_sesion()</span><br><span class="line">       sess.add(User(id=<span class="number">1</span>, name=<span class="string">'Test'</span>, passwd=<span class="string">'Test'</span>))</span><br><span class="line">       sess.commit()</span><br><span class="line">       print(<span class="string">'test insert user(name=Test, passwd=Test): ok'</span>)</span><br><span class="line"></span><br><span class="line">       user = sess.query(User).one()</span><br><span class="line">       print(<span class="string">'test reading user from the database: '</span>)</span><br><span class="line">       print(<span class="string">'user: '</span>, user)</span><br><span class="line">       print(<span class="string">'print message of the user: '</span>)</span><br><span class="line">       print(<span class="string">'name: '</span>, user.name, <span class="string">', passwd'</span>, user.passwd)</span><br><span class="line">       sess.close()</span><br><span class="line">       print(<span class="string">'The database works well.'</span>)</span><br></pre></td></tr></table></figure></li><li><p>在flask里使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## improt dbu</span></span><br><span class="line"><span class="meta">@app.before_request</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">before_request</span><span class="params">()</span>:</span></span><br><span class="line">    g.db = dbu.get_sesion()</span><br><span class="line">    </span><br><span class="line"><span class="meta">@app.teardown_request</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">teardown_request</span><span class="params">(exception)</span>:</span></span><br><span class="line"><span class="keyword">if</span> hasattr(g, <span class="string">'db'</span>):</span><br><span class="line">g.db.close()</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/N4A/FlaskWebFramework&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;source codes&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://n4a.github.io/tags/python/"/>
    
      <category term="Flask" scheme="https://n4a.github.io/tags/Flask/"/>
    
      <category term="jinjia2" scheme="https://n4a.github.io/tags/jinjia2/"/>
    
      <category term="bootstrap" scheme="https://n4a.github.io/tags/bootstrap/"/>
    
      <category term="sqlite3" scheme="https://n4a.github.io/tags/sqlite3/"/>
    
      <category term="sqlalchemy" scheme="https://n4a.github.io/tags/sqlalchemy/"/>
    
  </entry>
  
</feed>
